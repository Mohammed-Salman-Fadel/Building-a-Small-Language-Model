{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee66e88",
   "metadata": {},
   "source": [
    "# **Building a Small Language Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5fe9d",
   "metadata": {},
   "source": [
    "# **Stage One - Building the LLM**\n",
    "This Stage Consists of 3 Parts:\n",
    "1. Data Preparation and Sampling\n",
    "2. Attention Mechanisms\n",
    "3. LLM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f70983",
   "metadata": {},
   "source": [
    "## Part 1 - *Data Preparation & Sampling*\n",
    "Formatting and preparing the data into the necessary form to be processed and understood by the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578b9cc",
   "metadata": {},
   "source": [
    "### All Imports Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0bf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee58bf5",
   "metadata": {},
   "source": [
    "### Importing the Dataset Example\n",
    "In our case, we will be using the pdf form of the book \"The Verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006a75eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x154a4c3dfd0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3146f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b5ceb",
   "metadata": {},
   "source": [
    "### Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a534b0",
   "metadata": {},
   "source": [
    "#### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43373940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ']\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to create tokens.\n",
    "# We want to filter out whiite spaces and special characters.\n",
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(preprocessed[:50])\n",
    "\n",
    "# Will filter out any whitespaces and only return the characters.\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b258f4",
   "metadata": {},
   "source": [
    "#### Step 2: Determining the Vocabulary and mapping them to their token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad18907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Creating our Vocabulary of unique tokens (Alphabetically Arranged)\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# Print first 50 vocab elements\n",
    "for i, item in enumerate(vocab.items()):\n",
    "  print(item)\n",
    "  if i >= 50:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ff2239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6af922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleTokenizerV1 - A simple tokenizer that can perform encoding and decoding.\n",
    "\n",
    "SimpleTokenizerV2 - Replaces Uknown words with the special character <|unk|> and\n",
    "unrelated pieces of texts with <|endoftext|>\n",
    "\n",
    "\"\"\"\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "      self.str_to_int = vocab\n",
    "      self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "      preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "      preprocessed = [\n",
    "      item.strip() for item in preprocessed if item.strip()\n",
    "      ]\n",
    "      ids = [self.str_to_int[s] for s in preprocessed]\n",
    "      return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "      text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5986e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [\n",
    "    item.strip() for item in preprocessed if item.strip()\n",
    "    ]\n",
    "\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                else \"<|unk|>\" for item in preprocessed]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc647e92",
   "metadata": {},
   "source": [
    "##### Create Byte-Pair Encoder\n",
    "- Is a subword tokenization algorithm. The most common pair of consecutive bytes of data is replaced with a byte that does not occur in data.\n",
    "\n",
    "Advantages:\n",
    "- Byte-pair encoding can reduce the size of the vocabulary significantly.\n",
    "- The BPE tokenizer can handle any unknown word without needing the `<|unk|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab7a058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a498ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text)) # Prints the new number of tokens using the GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63be29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "z:            [2241, 287, 257, 4489]\n",
      "------------------------------------\n",
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To better visualize what's being done\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "z = enc_sample[2:context_size+2]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "print(f\"z:            {z}\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Representation: left side = input, right side = target\n",
    "for i in range(1, context_size+1):\n",
    "  context = enc_sample[:i]\n",
    "  desired = enc_sample[i]\n",
    "  print(context, \"---->\", desired)\n",
    "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired])) # Text equivalent\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f21822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset Implementation\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # Tokenizes the entire text\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "\n",
    "    # Uses a sliding window approach to chunk the book into overlapping sequences.\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Dataloader Implementation\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                 stride=128, shuffle=True, drop_last=True,\n",
    "                 num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiates the gpt2 tokenizer\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  # Initialize the Dataset class created earlier\n",
    "\n",
    "      # Intantiates and provides parameters for the DataLoader python class provided by PyTorch.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "549478f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619],\n",
      "        [  402,   271, 10899,  2138,   257,  7026]]), tensor([[  367,  2885,  1464,  1807,  3619,   402],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632]])]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    dataloader = create_dataloader_v1(\n",
    "      raw_text, batch_size=2, max_length=6, stride=6, shuffle=False)\n",
    "    data_iter = iter(dataloader)  # Creates an Iterator\n",
    "    first_batch = next(data_iter) # Gets the next batch from the data Iterator, the first_batch will be assigned a tuple of tensors.\n",
    "    print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d30748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[15632,   438,  2016,   257,   922,  5891],\n",
      "        [ 1576,   438,   568,   340,   373,   645]]), tensor([[ 438, 2016,  257,  922, 5891, 1576],\n",
      "        [ 438,  568,  340,  373,  645, 1049]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a32d20",
   "metadata": {},
   "source": [
    "#### Step 3: Mapping Token Embeddings --- COME BACK TO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993f557",
   "metadata": {},
   "source": [
    "##### Import Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "094c5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install gensim\n",
    "# import gensim.downloader as api\n",
    "# model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fe982",
   "metadata": {},
   "source": [
    "##### Positional Embedding Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a43a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the inputs that we'll be using, with their embeddings, given that each word has 3 dimensions. \n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c93ee5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 256])\n",
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Embedding function only requires the vocab_size\n",
    "as it'll provide random embeddings originally\n",
    "and those embeddings will be adjusted over training.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs.long())\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Instantiating the data loader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "   stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "836b290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5dc735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! [[TODO]] SIZE INCOMPATABILITY BETWEEN TOKEN AND POSITIONAL EMBEDDINGS\n",
    "# input_embeddings = token_embeddings + pos_embeddings\n",
    "# print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24bc33",
   "metadata": {},
   "source": [
    "## Part 2 - *ATTENTION MECHANISM*\n",
    "There are 4 kinds of attention mechanisms and we will be implementing each one until we arrive to the original </br>\n",
    "transformer's **\"Multi-Head Self-Attention\"** mechanism, building on top of the previous implementation and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fabb8",
   "metadata": {},
   "source": [
    "### First Implementation: **Simplified Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a22ef6",
   "metadata": {},
   "source": [
    "#### Compute Attention Scores - 2nd input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee95f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2956967., 16597455., 29439276., 30712224., 23737832.,  2581577.,\n",
      "        23769278., 12535636.])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # Takes 'journey' as the query.\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "  attention_scores_2[i] = torch.dot(x_i, query) \n",
    "\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002ab0",
   "metadata": {},
   "source": [
    "#### Compute Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a70b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.0208, 0.1166, 0.2068, 0.2158, 0.1668, 0.0181, 0.1670, 0.0881])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attention_weights_2_tmp)\n",
    "print(\"Sum: \", attention_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccb429bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Sum: tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following is a naive Softmax Implementation. However, this implementation has weaknesses when dealing with very small or large values.\n",
    "Therefore it is recommended to simply use PyTorch's implmentation of Softmax.\n",
    "''' \n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attention_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a347d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Softmax Function Implementation, results in same value of previous softmax function.\n",
    "attn_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1446cd",
   "metadata": {},
   "source": [
    "#### Compute Context Vectors\n",
    "The context vector z is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "becf5848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6203.2407, 3242.1980,  940.7390, 1740.2675])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attention_weights_2_tmp[i]*x_i #x_i is simply the input vector, multiplying with its corresponding attention weights.\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3122ce",
   "metadata": {},
   "source": [
    "#### A Generalised Approach\n",
    "So far, for the attention mechanism built above, we have implemented attention with respect to the embeddings vector of the second input. Now we will create a generalised method that can be applied to all embeddings vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa276d",
   "metadata": {},
   "source": [
    "#### Generalized Process\n",
    "We perform the same 3 steps performed prior, with a few modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3f5b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddcacc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n!\\nThe issue with the above implementation is that cause of the nested for loop\\nit is slow and computationally expensive, a better approach can be done with linear algebra.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, j_i in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, j_i)\n",
    "\n",
    "print(attention_scores) # Unnormalized output\n",
    "\n",
    "\"\"\" \n",
    "!\n",
    "The issue with the above implementation is that cause of the nested for loop\n",
    "it is slow and computationally expensive, a better approach can be done with linear algebra.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9700d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following method is more efficient, not because the time complexity is any less,\n",
    "but the implementation time is far less, the time it takes to perform each multiplication, the '@' symbol represents matrix multiplication.\n",
    "You get the exact same answer as the previous method, but much faster.\n",
    "\"\"\"\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83715b4",
   "metadata": {},
   "source": [
    "#### Calculating the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d530f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "----------------------------------------------------------------\n",
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Declaring the dimensions to be -1 will normally apply the normalization to the last dimension, however, for a 2 dimensional tensor,\n",
    "the dim = -1 will apply the Softmax into all columns.\n",
    "'''\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) #\n",
    "print(attention_weights)\n",
    "\n",
    "# To verify that all rows sum up to 1\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attention_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641172",
   "metadata": {},
   "source": [
    "#### Computing the Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "200027bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Simply a matrix multiplication is performed. Concluding the generalised method of calculating context vectors.\n",
    "all_context_vec = attention_weights @ inputs\n",
    "print(all_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5497f",
   "metadata": {},
   "source": [
    "### Second Implementation: **Self-Attention**\n",
    "This self-attention mechanism is also called *scaled dot-product attention*. \n",
    "<br>We will be building on top of our last model, this time introducing **Trainable Weight Matrices**. This can help the model to produce better context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a933d6",
   "metadata": {},
   "source": [
    "For illustration purposes, we will be computing for only one context vector, *z(2)*.\n",
    "- Note that in GPT-like models, the input and output dimensions are usually the same,\n",
    " but to better follow the computation, we’ll use different input (d_in=3) and output\n",
    " (d_out=2) dimensions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9e377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables dealing with the second input\n",
    "x_2 = inputs[1]    \n",
    "d_in = inputs.shape[1]     \n",
    "d_out = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abb67c",
   "metadata": {},
   "source": [
    " Initialize the query, key, and value matrices of the second input \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef594936",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e655dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b100eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac2bf",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Input 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa4d33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]            \n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d2d81",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26b54dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2) # Notice the attention score matches the previously calculated attention score for the second input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e1344",
   "metadata": {},
   "source": [
    "#### Calculate the Attention Weights of the entire Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdbcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247b93b",
   "metadata": {},
   "source": [
    "#### Calculate the Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d624f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0915d",
   "metadata": {},
   "source": [
    "#### Implementing a compact self-attention Python class\n",
    "* In practice, with the LLM implementation that'll be performed later in mind, it is helpful to organize this code into a Python class, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4abb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attention_scores = queries @ keys.T # omega\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3819522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11b141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention Class version 2\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c55b9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\\n they use different initial weights for the weight matrices since nn.Linear uses a more\\n sophisticated weight initialization scheme.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "'''\n",
    " Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    " they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    " sophisticated weight initialization scheme.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de148abf",
   "metadata": {},
   "source": [
    "### Third Implementation: **Causal Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ddc795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)    \n",
    "keys = sa_v2.W_key(inputs) \n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56313a",
   "metadata": {},
   "source": [
    "#### Step 1: Initialise the masked matrix with the \"*tril*\" function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7526eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c81c",
   "metadata": {},
   "source": [
    "#### Step 2: Multiply the attention weights matrix with the masked matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55398fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attention_weights * mask_simple\n",
    "print(masked_simple) # prints non-normalised values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586bc4",
   "metadata": {},
   "source": [
    "#### Step 3: Renormalize each row to sum up to 1 using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e128baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd130fef",
   "metadata": {},
   "source": [
    "#### Second More Efficient Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "458ff6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffa990d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84c6f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995c95",
   "metadata": {},
   "source": [
    "#### Using *Dropout*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8f89d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)   \n",
    "example = torch.ones(6, 6)     \n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e7b47d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying dropout to the attention weights\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f172bc",
   "metadata": {},
   "source": [
    "#### Implementing a Causal Attention Class\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented <br>\n",
    "earlier, except that we added the dropout and causal mask components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6696b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)           \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                  \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2)   \n",
    "        attention_scores.masked_fill_(                   \n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "015435da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f197cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa70423",
   "metadata": {},
   "source": [
    "### Final Implementation: **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a206a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "        [CausalAttention(\n",
    "         d_in, d_out, context_length, dropout, qkv_bias\n",
    "     ) \n",
    "     for _ in range(num_heads)]\n",
    " )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39937465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    "    )\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35d32",
   "metadata": {},
   "source": [
    "#### Efficient Multi-Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1d5e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "         context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                        diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)   \n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim    \n",
    "        )\n",
    "\n",
    "        keys = keys.transpose(1, 2)   \n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] \n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec)   \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688d0e3",
   "metadata": {},
   "source": [
    "## PART 3 - *LLM ARCHITECTURE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16afc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e852edb",
   "metadata": {},
   "source": [
    "### Placeholder GPT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f88db552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])      # Creates the 768 dimensional word embedding.\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])  # Creates the positional embeddings.\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])                        # Creates a random dropout embedding of 10% of the total embeddings. \n",
    "        self.trf_blocks = nn.Sequential(                                    # Creates an \"n_layers\" amount of transformer blocks.\n",
    "            *[DummyTransformerBlock(cfg)              \n",
    "            for _ in range(cfg[\"n_layers\"])]        \n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                    # Final normalization layer\n",
    "        self.out_head = nn.Linear(                                          # Projects the final hidden states to vocabulary size to get the logits for predicting the next token.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):                                              # in_idx param is a tensor of shape (batch_size, seq_len) containing token indices.\n",
    "        batch_size, seq_len = in_idx.shape                                  # Splitting the in_idx tensor into 2 seperate vectors.\n",
    "        tok_embeds = self.tok_emb(in_idx)                                   # Converts input tokens into vectors.\n",
    "        pos_embeds = self.pos_emb(                                          # Creates the (absolute) position vectors from the sequence length of each input.\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds                                         # Adds the token embeddings with the (absolute) positional embeddings, resulting into our prepared input.\n",
    "        x = self.drop_emb(x)                                                # We drop the randomly chosen matrix values (dropout regularization).\n",
    "        x = self.trf_blocks(x)                                              # Runs the input through each transformer block in sequence.\n",
    "        x = self.final_norm(x)                                              # Final normalization\n",
    "        logits = self.out_head(x)                                           # The output logits will contain the probability values needed for prediction.\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):                                     # Simple placeholder class that'll be replaced by a real Transformer Block later.\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):    \n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):                                            # Simple placeholder class that'll be replaced by a real LayerNorm later.\n",
    "    def __init__(self, normalized_shape, eps=1e-5):   \n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccb7f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")           # Instantiate the GPT-2 tokenizer.\n",
    "batch = []                                          # Initialize an empty list\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))  # First encodes the text using GPT-2 tokenizer -> converted into a tensor -> is appended into the batch list.\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)                   # Takes in a list of tensors and \"stacks\" (concatenates) them into a 2D tensor.\n",
    "print(batch)                                        # Print the resulting 2D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26e03364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe output tensor has two rows corresponding to the two text samples. \\nEach text sample consists of four tokens.\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)                      # Passes the previously defined batch into our DummyGPTModel.\n",
    "print(\"Output shape:\", logits.shape)       # The standard shape distribution: (batch_size, seq_len, vocab_size).\n",
    "print(logits)                              # Outputs the 2 batches, each containing 4 sequences(inputs) and their embeddings relative to a vocab of 50257.\n",
    "\n",
    "\"\"\"\n",
    "The output tensor has two rows corresponding to the two text samples. \n",
    "Each text sample consists of four tokens.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abdd6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)                   # Creates 2 batches of random numbers, each containing 5 values. \n",
    "layer = nn.Sequential(                              # Constructs a sequential neural network module.\n",
    "    nn.Linear(5, 6),                                # Creates a fully connected linear layer, taking in 5 input vectors and outputing 6.\n",
    "    nn.ReLU()                                       # Applies nonlinear activation function.\n",
    ")       \n",
    "out = layer(batch_example)                          # Feeds the nn layer with the batch example\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5adaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)   # Keepdim ensures the dimensions stay the same, and doesn't combine the dimensions of the first and second input together.\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "347ca1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [5.9605e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)          # Disables Scientific Notation \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78df5c",
   "metadata": {},
   "source": [
    "#### GELU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "efc64996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x  * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0cbdf",
   "metadata": {},
   "source": [
    "#### Layer Normalization Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "88b6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8adcd",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74bc04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(                            # Simply an implemtation of a 3 layered forward neural network.\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),      # This linear portion \"expands\" the input embedding dimension into the number of nodes of the following hidden layer, in our case it'll 4 times the amount of the emb dim.\n",
    "            GELU(),                                             # Calls the GELU function on the nodes.\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])       # Does the opposite of the first linear layer, \"compressing\" from 4 times the size back to the original size of the input/\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8ec4a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)  # Intance\n",
    "x = torch.rand(2, 3, 768)           # 2 batches, each batch has 3 tokens, and each token will have an embedding size of 768.\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037276cc",
   "metadata": {},
   "source": [
    "### Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9bd77bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([                                   # Implement 5 layers.\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \n",
    "                  GELU())\n",
    " ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)        \n",
    "            if self.use_shortcut and x.shape == layer_output.shape:   \n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4f39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)                           \n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37e56d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)               # Forward Pass.\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)     # Calculates loss based on how close the target and outputs are.\n",
    "    \n",
    "    loss.backward()                 # Backward pass to calculate the gradients.\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "08db27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8432b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    " )\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a08917f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  \n",
    "   \n",
    "        shortcut = x        \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5812e",
   "metadata": {},
   "source": [
    "#### Instantiating a Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d3e4e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                  \n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be38b3",
   "metadata": {},
   "source": [
    "### GPT FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "550129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):        #initializes the token and positional embedding layers using the configurations passed in via cfg.\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(                # Creates transformer blocks equal to that in specified in cfg.\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]    \n",
    "            )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])     # Layer normalization is applied.\n",
    "        self.out_head = nn.Linear(  # linear output head without bias is defined, which projects the transformer’s output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f76141",
   "metadata": {},
   "source": [
    "#### Instantiating our GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90a92ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)     # Shape: [2, 4, 50257]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cba46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5e328bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f434dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    " )\n",
    "\n",
    "print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e422a",
   "metadata": {},
   "source": [
    "#### Calculate the total size needed for the 163 million parameters in our GPTModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "673d4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4      \n",
    "total_size_mb = total_size_bytes / (1024 * 1024)    \n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "208052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,                \n",
    "                 max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]   \n",
    "        with torch.no_grad():\n",
    "           logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]                   \n",
    "        probas = torch.softmax(logits, dim=-1)          \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f44fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "baa9627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()                 \n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b5236fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d9c1e",
   "metadata": {},
   "source": [
    "# **Stage Two - Foundational Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b901e7",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42bce8",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e399602",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,   #! Context length dropped from 1024 -> 256.\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa5be60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)                 # Unsqueeze adds a batch dimension. Shape becomes [1, seq_length]\n",
    "    return encoded_tensor\n",
    " \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)                 # Removes batch dimension, resulting in a 1D tensor.\n",
    "    return tokenizer.decode(flat.tolist())      # First the tensor is converted to a list of integers and then decoded to human readable text.\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(               # Calls and passes parameter values into the generate_text_simple function.\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),    # Is the starting prompt.\n",
    "    max_new_tokens=10,                                  # Specifies that 10 new tokens should be generated after the first prompt\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]      # Determines the maximum token size to be considered at once.\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))       # Calls token ids to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "16937771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs.long())\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7533de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "28125199",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "               [        40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e9b34",
   "metadata": {},
   "source": [
    "#### Calculating Logits and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3eea9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "937701dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)  # Since we have 2 batches, each containing 3 tokens, we received the highest probability value for each token in each batch.\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75bd1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957afd64",
   "metadata": {},
   "source": [
    "### Text Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f199b",
   "metadata": {},
   "source": [
    "#### Calculating Target Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57543b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0000,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Printing the initial Softmax probability scores\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409c271",
   "metadata": {},
   "source": [
    "#### Calculating the Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aadc810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20d5f",
   "metadata": {},
   "source": [
    "#### Average Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cc451d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8765)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed6e58",
   "metadata": {},
   "source": [
    "#### Negative Average Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99a093be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1   # Simply multiply the average by -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbfa57",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57be226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# It's important to keep track of the dimensions in order to perform cross entropy.\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e83bc9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "For the cross_entropy loss function in PyTorch, we want to flatten these tensors\n",
    "by combining them over the batch dimension:\n",
    "'''\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c86fce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b42c4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52918.7773)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Perplexity\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22782",
   "metadata": {},
   "source": [
    "### Training & Validation Losses\n",
    "We will use the text file of \"the verdict\" once more to demonstrate training our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4ccaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "31df5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)  # We will only be working with 5145 tokens for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56f834",
   "metadata": {},
   "source": [
    "#### Train and Testing Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7901e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bca0de",
   "metadata": {},
   "source": [
    "#### Calling the Dataloader from Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ad18c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader is called for both training and validation sets.\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    " )\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a1006d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Iterating through both data loaders\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")   \n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e11a",
   "metadata": {},
   "source": [
    "#### Cross entropy lost among Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6ef8e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)        \n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d37b8",
   "metadata": {},
   "source": [
    "#### Cross entropy lost over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b45c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))  #  Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()   # Sums the loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches     # Averages the loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9352e13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Training loss: 10.988501760694716\n",
      "Validation loss: 10.99034309387207\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)  \n",
    "with torch.no_grad():                                       \n",
    "    train_loss = calc_loss_loader(train_loader, model, device)   \n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ccee86b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.7.0\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "GPU: No GPU detected\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cd656",
   "metadata": {},
   "source": [
    "### The main function for pretraining LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "51f47fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model and generate_and_print_sample functions are not defined yet. \n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "               optimizer, device, num_epochs,\n",
    "               eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []   \n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                    \n",
    "            optimizer.step()                   \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:   \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(                     \n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b2de5",
   "metadata": {},
   "source": [
    "#### Evaluate model function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2389b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()            # Dropout is disabled \n",
    "    with torch.no_grad():   # Disables gradient tracking\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3693a8",
   "metadata": {},
   "source": [
    "#### Generate and print sample function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d946563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1508a",
   "metadata": {},
   "source": [
    "#### Training a GPTModel for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f48aca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),          \n",
    "    lr=0.0004, weight_decay=0.1\n",
    " )\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448655fb",
   "metadata": {},
   "source": [
    "#### Visualizing the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "185b08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVk1JREFUeJzt3Xd4FNXXwPHv7ibZ9E4akAKEhNBJqKEKAoooIoIICKIi0kX9oSJVBVFBVBTFV8ECgkgRaVKUGhAEAqGFHgIkkEI6qTvvHwubLKEkkLCbcD7PM092Z+7cOXsJOXtn7txRKYqiIIQQQgizpDZ1AEIIIYS4PUnUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQlQSKpWKlStXmjoMIUQZk0QthJlQqVR3XAYNGmTqEIUQJmBh6gCEEHpxcXGG10uWLGHixIlER0cb1tnY2JgiLCGEiUmPWggz4eXlZVicnJxQqVRG6xYtWkTNmjWxsrIiKCiIn3/++Y71TZ06FU9PTyIjIwGIiIigbdu22NjYUL16dUaNGkVmZqahvL+/P9OmTWPw4ME4ODjg6+vLvHnzDNtzc3MZMWIE3t7eWFtb4+/vz/Tp0297/C1bttCsWTPs7OxwdnYmPDycmJgYw/Y///yT0NBQrK2tqVGjBlOmTCE/P9+wPTU1lSFDhuDh4YGjoyOPPPIIBw8eNGyfPHkyjRo14ueff8bf3x8nJyeee+450tPTS9zmQlQEkqiFqABWrFjB6NGjeeONNzh8+DCvvvoqL774Iv/880+xsoqiMHr0aL7//nt27NhBo0aNiIqKokuXLvTs2ZNDhw6xZMkSduzYwYgRI4z2nTlzJmFhYRw4cIBhw4bx2muvcfz4cQC++OILVq1axW+//UZ0dDS//PIL/v7+t4w3Pz+fHj160K5dOw4dOsSuXbsYMmQIKpUKgL/++ov+/fszatQojh49yrfffsuCBQv48MMPDZ+hW7duxMfHs3btWvbt20eTJk3o2LEjycnJhuOcPn2alStXsnr1alavXs3WrVv56KOPyqLJhTAfihDC7MyfP19xcnIyvG/VqpXyyiuvGJV59tlnlccff9zwHlCWLl2q9O/fXwkODlZiY2MN2wYMGKAMGTLEaP/t27crarVauXbtmqIoiuLn56f079/fsF2n0ykeHh7K3LlzFUVRlJEjRyqPPPKIotPp7hp/UlKSAihbtmy55fY2bdoo06ZNM1r3888/K97e3oqiKMrmzZsVR0dHJTs726hMzZo1lW+//VZRFEWZNGmSYmtrq6SlpRm2v/XWW0rz5s3vGp8QFYlcoxaiAjh27BhDhgwxWhceHs7nn39utO71119Hq9Wye/du3N3dDev37dvHqVOnWLhwoWGdoijodDrOnj1LnTp1AGjQoIFh+41T71euXAFg0KBBPProowQFBdG1a1eeeOIJOnfufMt4XV1dGTRoEF26dOHRRx+lU6dO9O7dG29vb0M8e/fuNfSgAQoKCsjOziYrK4t9+/aRkZGBm5ubUb3Xrl3j9OnThvf+/v44ODgY3nt7exviFaKykEQtRAVx47TxDYqiFFv36KOP8uuvv/LXX3/Rr18/w3qdTserr77KqFGjitXr6+treG1paVnsmDqdDoAmTZpw9uxZ1q1bx6ZNm+jduzedOnXi999/v2W88+fPZ9SoUaxfv54lS5bw3nvvsXHjRlq0aIFOp2PKlCn07Nmz2H7W1tbodDq8vb3ZsmVLse3Ozs4lileIykIStRAVQJ06ddixYwcvvPCCYV1ERIShJ3zDk08+Sffu3Xn++efRaDQ899xzgD7JHjlyhFq1at1XHI6OjvTp04c+ffrQq1cvunbtSnJyMq6urrcs37hxYxo3bsw777xDy5YtWbRoES1atKBJkyZER0ffNp4mTZoQHx+PhYXFba+DC/GwkEQtRAXw1ltv0bt3b8OAqj///JPly5ezadOmYmWffvppfv75ZwYMGICFhQW9evVi3LhxtGjRguHDh/PKK69gZ2fHsWPH2LhxI19++WWJYvjss8/w9vamUaNGqNVqli5dipeXl1EP94azZ88yb948nnzySXx8fIiOjubEiROGLxoTJ07kiSeeoHr16jz77LOo1WoOHTpEVFQUH3zwAZ06daJly5b06NGDGTNmEBQUxKVLl1i7di09evQgLCzsvtpTiIpEErUQFUCPHj34/PPP+eSTTxg1ahQBAQHMnz+f9u3b37J8r1690Ol0DBgwALVaTc+ePdm6dSvjx4+nTZs2KIpCzZo16dOnT4ljsLe3Z8aMGZw8eRKNRkPTpk1Zu3YtanXxm0dsbW05fvw4P/74I0lJSXh7ezNixAheffVVALp06cLq1auZOnUqH3/8MZaWlgQHB/Pyyy8D+lPYa9euZfz48QwePJiEhAS8vLxo27Ytnp6epW9AISowlaIoiqmDEEIIIcStyX3UQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUt/H1118TEBCAtbU1oaGhbN++3dQhmdy2bdvo3r07Pj4+qFQqVq5cabRdURQmT56Mj48PNjY2tG/fniNHjhiVycnJYeTIkbi7u2NnZ8eTTz7JhQsXjMpcvXqVAQMG4OTkhJOTEwMGDCAlJcWozPnz5+nevTt2dna4u7szatQocnNzy+NjPzDTp0+nadOmODg44OHhQY8ePYyeRw3Sxvdr7ty5NGjQAEdHRxwdHWnZsiXr1q0zbJf2LVvTp09HpVIxZswYwzpp43tgsseBmLHFixcrlpaWynfffaccPXpUGT16tGJnZ6fExMSYOjSTWrt2rTJ+/Hhl2bJlCqCsWLHCaPtHH32kODg4KMuWLVOioqKUPn36KN7e3kZPNxo6dKhStWpVZePGjcr+/fuVDh06KA0bNlTy8/MNZbp27arUq1dPiYiIUCIiIpR69eopTzzxhGF7fn6+Uq9ePaVDhw7K/v37lY0bNyo+Pj7KiBEjyr0NylOXLl2U+fPnK4cPH1YiIyOVbt26Kb6+vkpGRoahjLTx/Vm1apWyZs0aJTo6WomOjlbeffddxdLSUjl8+LCiKNK+ZWnPnj2Kv7+/0qBBA2X06NGG9dLGpSeJ+haaNWumDB061GhdcHCw8vbbb5soIvNzc6LW6XSKl5eX8tFHHxnWZWdnK05OTso333yjKIqipKSkKJaWlsrixYsNZS5evKio1Wpl/fr1iqIoytGjRxVA2b17t6HMrl27FEA5fvy4oij6LwxqtVq5ePGiocyvv/6qaLVaJTU1tVw+rylcuXJFAZStW7cqiiJtXF5cXFyU//u//5P2LUPp6elKYGCgsnHjRqVdu3aGRC1tfG/k1PdNcnNz2bdvX7HH93Xu3JmIiAgTRWX+zp49S3x8vFG7abVa2rVrZ2i3ffv2kZeXZ1TGx8eHevXqGcrs2rULJycnmjdvbijTokULnJycjMrUq1cPHx8fQ5kuXbqQk5PDvn37yvVzPkipqakAhgdeSBuXrYKCAhYvXkxmZiYtW7aU9i1Dw4cPp1u3bnTq1MlovbTxvZG5vm+SmJhIQUFBsfmEPT09iY+PN1FU5u9G29yq3WJiYgxlrKyscHFxKVbmxv7x8fF4eHgUq9/Dw8OozM3HcXFxwcrKqtL8GymKwtixY2ndujX16tUDpI3LSlRUFC1btiQ7Oxt7e3tWrFhBSEiI4Q+8tO/9Wbx4Mfv372fv3r3Ftsnv8L2RRH0bJXn2ryjuXtrt5jK3Kn8vZSqyESNGcOjQIXbs2FFsm7Tx/QkKCiIyMpKUlBSWLVvGwIED2bp1q2G7tO+9i42NZfTo0WzYsAFra+vblpM2Lh059X0Td3d3NBpNsW9cV65ckaf23IGXlxfAHdvNy8uL3Nxcrl69escyly9fLlZ/QkKCUZmbj3P16lXy8vIqxb/RyJEjWbVqFf/88w/VqlUzrJc2LhtWVlbUqlWLsLAwpk+fTsOGDfn888+lfcvAvn37uHLlCqGhoVhYWGBhYcHWrVv54osvsLCwMHw2aePSkUR9EysrK0JDQ9m4caPR+o0bN9KqVSsTRWX+AgIC8PLyMmq33Nxctm7dami30NBQLC0tjcrExcVx+PBhQ5mWLVuSmprKnj17DGX+/fdfUlNTjcocPnyYuLg4Q5kNGzag1WoJDQ0t189ZnhRFYcSIESxfvpy///6bgIAAo+3SxuVDURRycnKkfctAx44diYqKIjIy0rCEhYXRr18/IiMjqVGjhrTxvXiwY9cqhhu3Z33//ffK0aNHlTFjxih2dnbKuXPnTB2aSaWnpysHDhxQDhw4oADKrFmzlAMHDhhuW/voo48UJycnZfny5UpUVJTSt2/fW952Ua1aNWXTpk3K/v37lUceeeSWt100aNBA2bVrl7Jr1y6lfv36t7ztomPHjsr+/fuVTZs2KdWqVauQt10U9dprrylOTk7Kli1blLi4OMOSlZVlKCNtfH/eeecdZdu2bcrZs2eVQ4cOKe+++66iVquVDRs2KIoi7Vseio76VhRp43shifo2vvrqK8XPz0+xsrJSmjRpYrhF5mH2zz//KECxZeDAgYqi6G+9mDRpkuLl5aVotVqlbdu2SlRUlFEd165dU0aMGKG4uroqNjY2yhNPPKGcP3/eqExSUpLSr18/xcHBQXFwcFD69eunXL161ahMTEyM0q1bN8XGxkZxdXVVRowYoWRnZ5fnxy93t2pbQJk/f76hjLTx/Rk8eLDh/3WVKlWUjh07GpK0okj7loebE7W0cempFEVRTNOXF0IIIcTdyDVqIYQQwoxJohZCCCHMmCRqIYQQwoxJohZCCCHMmCRqIYQQwoxJohZCCCHMmCTqO8jJyWHy5Mnk5OSYOpRKSdq3fEn7lj9p4/Il7asn91HfQVpaGk5OTqSmpuLo6GjqcCodad/yJe1b/qSNy5e0r570qIUQQggzJolaCCGEMGOV/nnU+fn5HDhwAE9PT9Tq0n0vSU9PB+DixYukpaWVR3gPNWnf8iXtW/6kjctXZW5fnU7H5cuXady4MRYWd07Flf4a9d69e2nWrJmpwxBCCCGK2bNnD02bNr1jmUrfo77xgPA9e/bg7e1t4miEEEII/TO2mzVrZshRd1LpE/WN093e3t5Uq1bNxNEIIYQQhUpySdakg8m2bdtG9+7d8fHxQaVSsXLlSqPtiqIwefJkfHx8sLGxoX379hw5csQ0wQohhBAmYNJEnZmZScOGDZkzZ84tt3/88cfMmjWLOXPmsHfvXry8vHj00UcNAwyEEEKIys6kp74fe+wxHnvssVtuUxSF2bNnM378eHr27AnAjz/+iKenJ4sWLeLVV199kKEKIYQQJmG216jPnj1LfHw8nTt3NqzTarW0a9eOiIgISdRCiHJRUFBAXl6eqcMQFZylpSUajaZM6jLbRB0fHw9QbEScp6cnMTExt90vJyfHaF5YOU0uhCgJRVGIj48nJSXF1KGISsLZ2RkvLy9UKtV91WO2ifqGmz+goih3/NDTp09nypQp5ROMosCuOWDtDE0GlM8xhBAmcSNJe3h4YGtre99/XMXDS1EUsrKyuHLlCsB93xpstonay8sL0P/nKfohr1y5csf7zt555x3Gjh1reH/x4kVCQkLKJqjjq2HDe6CxAo86UC2sbOoVQphUQUGBIUm7ubmZOhxRCdjY2AD6nOXh4XFfp8HNdq7vgIAAvLy82Lhxo2Fdbm4uW7dupVWrVrfdT6vV4ujoaFgcHBzKLKbNShh7rMOhIBeWDID0y2VWtxDCdG5ck7a1tTVxJKIyufH7dL9jHkzao87IyODUqVOG92fPniUyMhJXV1d8fX0ZM2YM06ZNIzAwkMDAQKZNm4atrS3PP//8A481MyefccsPcy1jMH87XcAzPQaWDoQXVoGF1QOPRwhR9uR0tyhLZfX7ZNIe9X///Ufjxo1p3LgxAGPHjqVx48ZMnDgRgP/973+MGTOGYcOGERYWxsWLF9mwYUOZ9pJLyk5rwWd9GpGlsuG5tFHkWdjD+V3w17sPPBYhhBAPD5Mm6vbt26MoSrFlwYIFgP7byOTJk4mLiyM7O5utW7dSr149k8XbJrAKox4J5KzizaicYSioYO93cOAXk8UkhBBlrX379owZM6bE5c+dO4dKpSIyMrLcYgLYsmULKpXqoRuZb7bXqM3VqI6BtAl0Z11eI+Zb9dWvXP06XNhn2sCEEA8dlUp1x2XQoEH3VO/y5ct5//33S1y+evXqxMXFmbQjVZlJoi4ljVrF7D6N8HK05v20xzlkf2NwWX/IuGLq8IQQD5G4uDjDMnv2bBwdHY3Wff7550blSzqoydXVtVSXGDUaDV5eXnd9rrK4N5Ko74GbvZY5zzdGrdbwfOKLpNoFQPol+G0g5OeaOjwhxEPCy8vLsDg5OaFSqQzvs7OzcXZ25rfffqN9+/ZYW1vzyy+/kJSURN++falWrRq2trbUr1+fX3/91ajem099+/v7M23aNAYPHoyDgwO+vr7MmzfPsP3mU983TlFv3ryZsLAwbG1tadWqFdHR0UbH+eCDD/Dw8MDBwYGXX36Zt99+m0aNGpWqDZYtW0bdunXRarX4+/szc+ZMo+1ff/01gYGBWFtb4+npSa9evQzbfv/9d+rXr4+NjQ1ubm506tSJzMzMUh3/QZBEfY/C/F15u2swGdjSO2UkBZb2cD4CNow3dWhCiDKgKApZufkmWRRFKbPPMW7cOEaNGsWxY8fo0qUL2dnZhIaGsnr1ag4fPsyQIUMYMGAA//777x3rmTlzJmFhYRw4cIBhw4bx2muvcfz48TvuM378eGbOnMl///2HhYUFgwcPNmxbuHAhH374ITNmzGDfvn34+voyd+7cUn22ffv20bt3b5577jmioqKYPHkyEyZMMIxz+u+//xg1ahRTp04lOjqa9evX07ZtW0B/NqJv374MHjyYY8eOsWXLFnr27FmmbV9W5DzFfXi5TQB7zyWz4Si8qxrJDKbD3u+h6StQpbapwxNC3IdreQWETPzLJMc+OrULtlZl8+d5zJgxhgcb3fDmm28aXo8cOZL169ezdOlSmjdvftt6Hn/8cYYNGwbok/9nn33Gli1bCA4Ovu0+H374Ie3atQPg7bffplu3bmRnZ2Ntbc2XX37JSy+9xIsvvgjAxIkT2bBhAxkZGSX+bLNmzaJjx45MmDABgNq1a3P06FE++eQTBg0axPnz57Gzs+OJJ57AwcEBPz8/w11GcXFx5Ofn07NnT/z8/ACoX79+iY/9IEmP+j6oVCo+ebYhvq62LEmrzxLXoegGrJQkLYQwG2FhxjMoFhQU8OGHH9KgQQPc3Nywt7dnw4YNnD9//o71NGjQwPD6xin2G1NklmSfGzNM3tgnOjqaZs2aGZW/+f3dHDt2jPDwcKN14eHhnDx5koKCAh599FH8/PyoUaMGAwYMYOHChWRlZQHQsGFDOnbsSP369Xn22Wf57rvvuHr1aqmO/6BIj/o+OdlY8nW/JvScG8G4S225GluVoTVMHZUQ4n7ZWGo4OrWLyY5dVuzs7Izez5w5k88++4zZs2dTv3597OzsGDNmDLm5dx5fY2lpafRepVKh0+lKvM+NyT+K7nOrZzmUxq2e/VC0DgcHB/bv38+WLVvYsGEDEydOZPLkyezduxdnZ2c2btxIREQEGzZs4Msvv2T8+PH8+++/BAQElCqO8iY96jJQr6oTk7vXBeCTv6L590wSJETDH8OhQB6XJ0RFpFKpsLWyMMlSnjOkbd++naeeeor+/fvTsGFDatSowcmTJ8vteLcTFBTEnj17jNb9999/paojJCSEHTt2GK2LiIigdu3ahrm1LSws6NSpEx9//DGHDh3i3Llz/P3334D+3zg8PJwpU6Zw4MABrKysWLFixX18qvIhPeoy0rdZdfaeS2bFgYuMXbSHbdavo8mIA8eq0EFmLxNCmIdatWqxbNkyIiIicHFxYdasWcTHx1OnTp0HGsfIkSN55ZVXCAsLo1WrVixZsoRDhw5Ro0bJT0m+8cYbNG3alPfff58+ffqwa9cu5syZw9dffw3A6tWrOXPmDG3btsXFxYW1a9ei0+kICgri33//ZfPmzXTu3BkPDw/+/fdfEhISHng7lIQk6jKiUqn48Ol6HL6YyskrGXzuOITX/TajavqKqUMTQgiDCRMmcPbsWbp06YKtrS1DhgyhR48epKamPtA4+vXrx5kzZ3jzzTfJzs6md+/eDBo0qFgv+06aNGnCb7/9xsSJE3n//ffx9vZm6tSpholenJ2dWb58OZMnTyY7O5vAwEB+/fVX6taty7Fjx9i2bRuzZ88mLS0NPz8/Zs6cyWOPPVZOn/jeqRRzHItehi5cuED16tWJjY2lWrVq5X68U1fSeXLOTrJyCxjZoSZvdLn9iEghhHnIzs7m7NmzBAQEYG1tbepwHlqPPvooXl5e/Pzzz6YOpUzc6feqNLlJrlGXsVoeDkzvqR/i/+U/p9kSfX1UZNTvkJlowsiEEMJ8ZGVlMWvWLI4cOcLx48eZNGkSmzZtYuDAgaYOzexIoi4HTzWqSv8WvgC8viSStI0fw7KXYOkgGVwmhBDoLxeuXbuWNm3aEBoayp9//smyZcvo1KmTqUMzO3KNupxMeCKEg7GpRF1MZfyx6nxhZY/q3HbYOBG6Tjd1eEIIYVI2NjZs2rTJ1GFUCNKjLidaCw1f92uCo7UFf15yZEn161OL7v4aDi42bXBCCCEqDEnU5ai6qy0zezcC4O0jfpwM1k+/x5+j4VKkyeISQghRcUiiLmePhnjyalv9fYE9j7Uly68T5GfrH4spg8uEEELchSTqB+DNLkE083clPUfHgKsvoXOtCamx1weX5Zs6PCGEEGZMEvUDYKlR8+XzjXG3t2LfFYXPXCeBlT3cGFwmhBBC3IYk6gfE09Gaz59rjEoFXx62IKL++/oNu7+CQ7+ZNjghhBBmSxL1AxRey53XO+kfgTl4jzeJTUbqN6waCXEHTRiZEOJh1r59e8aMGWN47+/vz+zZs++4j0qlYuXKlfd97LKq504mT55Mo0aNyvUY5UkS9QM2okMt2tauQnaejueiO5Bfo8jgsrxsU4cnhKhAunfvftsJQnbt2oVKpWL//v2lrnfv3r0MGTLkfsMzcrtkGRcXZ5bza5sTSdQPmFqtYnafRng7WXMqKZu3VaNRvBtCl2lgKXMMCyFK7qWXXuLvv/8mJiam2LYffviBRo0a0aRJk1LXW6VKFWxtbcsixLvy8vJCq9U+kGNVVJKoTcDVzoo5zzfBQq3i9yPp/FRvAdTpbuqwhBAVzBNPPIGHhwcLFiwwWp+VlcWSJUt46aWXSEpKom/fvlSrVg1bW1vq16/Pr7/+esd6bz71ffLkSdq2bYu1tTUhISFs3Lix2D7jxo2jdu3a2NraUqNGDSZMmEBenn7K5AULFjBlyhQOHjyISqVCpVIZYr751HdUVBSPPPIINjY2uLm5MWTIEDIyMgzbBw0aRI8ePfj000/x9vbGzc2N4cOHG45VEjqdjqlTp1KtWjW0Wi2NGjVi/fr1hu25ubmMGDECb29vrK2t8ff3Z/r0whklJ0+ejK+vL1qtFh8fH0aNGlXiY98LmULUREL9XHjn8Tq8v/ooH6w9TkNfVxpVd4bks7B9Jjz+qfSwhTAHuZml30ejBc31P68F+VCQAyo1WNrcvV4ruxIfxsLCghdeeIEFCxYwceJEVCoVAEuXLiU3N5d+/fqRlZVFaGgo48aNw9HRkTVr1jBgwABq1KhB8+bN73oMnU5Hz549cXd3Z/fu3aSlpRldz77BwcGBBQsW4OPjQ1RUFK+88goODg7873//o0+fPhw+fJj169cbpg11cnIqVkdWVhZdu3alRYsW7N27lytXrvDyyy8zYsQIoy8j//zzD97e3vzzzz+cOnWKPn360KhRI155pWSPFf7888+ZOXMm3377LY0bN+aHH37gySef5MiRIwQGBvLFF1+watUqfvvtN3x9fYmNjSU2NhaA33//nc8++4zFixdTt25d4uPjOXiwfMcYmXWizs/PZ/LkySxcuJD4+Hi8vb0ZNGgQ7733Hmp1xT8ZMDjcn//OJbPucDzDF+5n9fAWuCzqDYknwEIL3WaaOkQhxDSf0u/z7AKo+7T+9fE/9XMm+LWGF9cUlpldH7KSiu87uXTPhR48eDCffPIJW7ZsoUOHDoD+tHfPnj1xcXHBxcWFN99801B+5MiRrF+/nqVLl5YoUW/atIljx45x7tw5w+MYp02bVuy68nvvvWd47e/vzxtvvMGSJUv43//+h42NDfb29lhYWODl5XXbYy1cuJBr167x008/YWen/8IyZ84cunfvzowZM/D09ATAxcWFOXPmoNFoCA4Oplu3bmzevLnEifrTTz9l3LhxPPfccwDMmDGDf/75h9mzZ/PVV19x/vx5AgMDad26NSqVCj8/P8O+58+fx8vLi06dOmFpaYmvry/NmjUr0XHvlVlnuxkzZvDNN98wZ84cjh07xscff8wnn3zCl19+aerQyoRKpWJGrwb4u9lyMeUaY5YepuCxmeDdCNq+ZerwhBAVQHBwMK1ateKHH34A4PTp02zfvp3BgwcDUFBQwIcffkiDBg1wc3PD3t6eDRs2cP78+RLVf+zYMXx9fY2emdyyZcti5X7//Xdat26Nl5cX9vb2TJgwocTHKHqshg0bGpI0QHh4ODqdjujoaMO6unXrotFoDO+9vb25cuVKiY6RlpbGpUuXCA8PN1ofHh7OsWPHAP3p9cjISIKCghg1ahQbNmwwlHv22We5du0aNWrU4JVXXmHFihXk55fvxFVm3aPetWsXTz31FN26dQP039J+/fVX/vvvPxNHVnYcrS35ql8Ten4dwdYTCUxw8eXDV/5GpS78JURR4PopLSHEA/bupdLvoykyOCq4u74O1U39ojFR9xdXES+99BIjRozgq6++Yv78+fj5+dGxY0cAZs6cyWeffcbs2bOpX78+dnZ2jBkzhtzc3BLVrShKsXWqm/4e7d69m+eee44pU6bQpUsXnJycWLx4MTNnlu6soKIoxeq+1TEtLS2LbdPpdKU61s3HKXrsJk2acPbsWdatW8emTZvo3bs3nTp14vfff6d69epER0ezceNGNm3axLBhw/jkk0/YunVrsbjKiln3qFu3bs3mzZs5ceIEAAcPHmTHjh08/vjjt90nJyeHtLQ0w5Kenv6gwr1ndX2cDJOhLPr3PHO3nS3cGPmrPMdaCFOysiv9oinSB9JY6NcVvT59p3rvQe/evdFoNCxatIgff/yRF1980ZB0tm/fzlNPPUX//v1p2LAhNWrU4OTJkyWuOyQkhPPnz3PpUuEXll27dhmV2blzJ35+fowfP56wsDACAwOLjUS3srKioKDgrseKjIwkM7Pw+v3OnTtRq9XUrl27xDHfiaOjIz4+PuzYscNofUREBHXq1DEq16dPH7777juWLFnCsmXLSE5OBvSP6HzyySf54osv2LJlC7t27SIqquy+eN3MrHvU48aNIzU1leDgYDQajeEUTt++fW+7z/Tp05kyZcoDjLJsdK3nxaQnQpj851E+Xh9NVWcbnqppAatfh/xr+kLPfG/8B0AIIQB7e3v69OnDu+++S2pqKoMGDTJsq1WrFsuWLSMiIgIXFxdmzZpFfHy8UVK6k06dOhEUFMQLL7zAzJkzSUtLY/z48UZlatWqxfnz51m8eDFNmzZlzZo1rFixwqiMv78/Z8+eJTIykmrVquHg4FDstqx+/foxadIkBg4cyOTJk0lISGDkyJEMGDDAcH26LLz11ltMmjSJmjVr0qhRI+bPn09kZCQLFy4E4LPPPsPb25tGjRqhVqtZunQpXl5eODs7s2DBAgoKCmjevDm2trb8/PPP2NjYGF3HLmtm3aNesmQJv/zyC4sWLWL//v38+OOPfPrpp/z444+33eedd94hNTXVsBw9evQBRnx/BoUH8HLrAADeXHqQiCsa6P0TqC3h6EpYMUQe4iGEuKWXXnqJq1ev0qlTJ3x9fQ3rJ0yYQJMmTejSpQvt27fHy8uLHj16lLhetVrNihUryMnJoVmzZrz88st8+OGHRmWeeuopXn/9dUaMGEGjRo2IiIhgwoQJRmWeeeYZunbtSocOHahSpcotbxGztbXlr7/+Ijk5maZNm9KrVy86duzInDlzStcYdzFq1CjeeOMN3njjDerXr8/69etZtWoVgYGBgP6Lz4wZMwgLC6Np06acO3eOtWvXolarcXZ25rvvviM8PJwGDRqwefNm/vzzT9zc3Mo0xqJUyq0uQJiJ6tWr8/bbbzN8+HDDug8++IBffvmF48ePl6iOCxcuUL16dWJjY40GQ5grnU5h5K8HWBMVh4O1Bb8PbUVQ6g5YMgB0eVD/WXj6Wyh6DVsIcV+ys7M5e/YsAQEBWFvLbZGibNzp96o0ucmse9RZWVnFbsPSaDSlHjRQkajVKmb2bkhTfxfSs/N5cf4eLnt30N/uobaAqKWwchjo7nytRwghROVg1om6e/fufPjhh6xZs4Zz586xYsUKZs2axdNPP23q0MqVtaWG714Io0YVOy6lZjNo/l7SA7pArx9ApYFDi/UP8qjEX1iEEELomXWi/vLLL+nVqxfDhg2jTp06vPnmm7z66qu8//77pg6t3DnbWvHji81wt9dyLC6NYQv3kxfUHXp9r0/WkQth9WhJ1kIIUcmZdaJ2cHBg9uzZxMTEcO3aNU6fPs0HH3yAlZWVqUN7IKq72vLDoDBsLDVsP5nIu8ujUEJ6QM95+nsy9/8Ea8ZKshZCiErMrBO1gAbVnJnzfGPUKli67wKfbz4J9XvpB5Shgn3zYd1b+klRhBBCVDqSqCuAjnU8eb9HPQBmbzrJb//FQoPe0GMuoIK9/wcnN9y5EiHEXVXmgariwSur3yeZPaOC6Nfcj4tXr/H1ltO8uzwKL0dr2jbqC0oBpF6A2l1MHaIQFZaVlRVqtZpLly5RpUoVrKysbjuVpRB3oygKubm5JCQkoFar7/tyrSTqCuStLkFcSrnGyshLvPbLPn4b2pK6jfsbF8rPAY2VzA0uRCmo1WoCAgKIi4szmipTiPtha2uLr6/vfT/tURJ1BaJSqfi4V0Mup+Ww60wSL87fy4rh4VR1vj6HcG4mLOoDVUOh02RJ1kKUgpWVFb6+vuTn5991Tmoh7kaj0WBhYVEmZ2YkUVcwVhZqvhkQyrPfRHDicgYvzt/D0qGtcLKxhFOb4dx2uBQJTV8CZ9+71ieEKKRSqbC0tCy3pyAJcS9kMFkF5GRjyYIXm+HpqOXE5QyG/ryPnPwCCHkSus2EASskSQshRCUhibqC8nG24YdBTbGz0rDrTBLjfj+kf25s05ehetPCgumXTRekEEKI+yaJugKr6+PE3P6hWKhVrIy8xCd/RRsXuHQAvmoG2z41TYBCCCHumyTqCq5t7SpM71kfgK+3nGbhv0Ue1h6zC7JT4O/34fsu8O+3kB5vmkCFEELcE0nUlcCzYdUZ00n/HNUJKw+z+dj1090th+lHf6OC2N2w7n8wMxgWPAF7v4eMBJPFLIQQomQkUVcSozsG0jusGjoFRiw6wMHYFP2G1q/D60egyzSo1hRQ9CPD14yFmUHw01Ow70fISjZl+EIIIW5DpSiVe5Lo0jycu6LLK9Dx0o//se1EAu72VqwYFk51V1vjQinn4cgKOLwc4iIL16stoEYH6Dod3AMfaNxCCPGwKU1ukh51JWKpUfN1vyaEeDuSmJHLwPl7uJqZa1zI2RfCR8OrW2HUAeg4ETzrgy4fTv8NNq6FZRNPQU76g/0QQgghjEiirmTstRbMf7EpPk7WnEnI5JWf/iM77zazLLnWgDZvwGs7YMR/0ONrsHMr3L5qBHxSC46vfTDBCyGEKEYSdSXk6WjNgsHNcLC24L+Yq7zx20F0urtc4XAPhIbPFb7PuwZZSZCfDd4NC9ef3Q5HV+m3CyGEKHcyhWglVdvTgW8HhDLwhz2siYrjclo273arQxNfl5JVYGkDw/dA0mlwqlq4fsdncHozWNlD0OMQ0AY0WlBrQKXW/1RbgEpTuM49sHCmtJwMSIjW1+8ZUlhvSizo8gr3U1uAXRX9ayGEeIhJoq7EWtV057M+jXhz6UH+i7lKz68j6Fbfm/91DcLPze7uFahU4F6r8L2igHcDSDwBqbEQ9Zt+uZvOH0KrEfrXV47C94+Ciz+MPlhYZvHzEH/IeD9LO/CqDz6N9L1670bgXhs08msrhHh4yF+8Su6JBj6E+rkwa8MJft9/gTVRcWw4Gk+/5n6M6hiIq10pnpOqUunvy35kIlz8Tz96PPEE6Ar0z8XWFRi/VgpAp9P3jG9Qa8DJFxyrGtdtaavvpd/YryAP8jL193/H7i4sZ2GjT97eDaFBb6je7L7aRwghzJ3cnvUQORaXxkfrjrP1hH6iEwetBcM61OLFcH+sLc3sFLOuABJPQtxB/W1klyL1Pe7cjMIyT34JTV7Qv758FPb+H/iHQ71nTBGxEKKiUhT9eJzcTP3fmNxMyM0q8vr6els3qNujTA5ZmtwkPeqHSB1vR34c3IwdJxOZtvYYR+PSmLH+OD/vOscbnYN4unFV1GozeYa1WgMewfqlYR/9Op0Okk/rk3ZcJPi2LCwfsxP++x5SYowT9YYJ4FZTf9rcIwQsSnEGQQhh3hRFn0CzkvRL5vWf2amgdYDG/QrLrnkDrsYYzxWx5zvYPFVfh6K7+/GqNy+zRF0akqgfQq0D3Vk9sjUrIy/y6V/RXErN5o2lB/l+x1nefbwOrQPdTR3iramvD0xzD4QGzxpv82kCrUZCleDCdRlXIOKLIvtb6gewedYDGxfQOur/MxstjuBZF6xumihGCPFgpF2CzET9/3NLG/26U5shem1hQs5KLnxdkHvrejxCjBP12W36S3UZY4wndcpJM97Pwgas7K4v9kVe2xn/fXmA5NT3Qy47r4D5O8/x9T+nSM/JB/QP+njnsWDqeDuaOLr7lH4Zdn91vQd+UP+AkpJ4LUKfrAG2fQI7ZkPYYOj8vn5dTjqsfO3Wid7KXn+93fL6f3ZLG/2gOEsb/bV66dGLykJR9LdpWljrv0QDpF7Q38GRl3V9uXaLn9f0p5LzsvSJ1qkadP+8sN5Pa0PGZRi6Qz8eBWD7LNg85faxWNiAnTvYuupPT1s7g4vf9WcdXHd4GeRlQ62O4OClX5eVDNeuFiZiS9sHdqeJnPoWJWZtqeG19jXp07Q6X/59kl92x7DtRALbTybQq0k13ugchJeTtanDvDcOnvDoVP1rRdGfFr8UCUkn9cm22JKm/2ntVFhHdtr16+JFvs9eS4Fjf5Y+nsF/gW8L/eu938PWGVD3aXhshn5dQT78NuB6cre9PsDOtshrO+MvBFp7/U9Hn8KehzCt/Bzj3l7RHuC1ZP0gSQAU8AuH+r30b3PSYf3b+tdPfVVY37/z4OI+fXlFKcFP9AMsW40srOPX5/Xbn/6m8Hd7z3dw4q/CWG6uo+i6gjx9UvVqAE/NKaz3Iz/ISYWR+/WXl0D/e71jVuna7OZeqr1H4ZeAG/xaQdu39EnYtkhCvrGU5AzYrcau2LrqFzNn9on64sWLjBs3jnXr1nHt2jVq167N999/T2hoqKlDq1Rc7ayY1L0ug1r58/H6aNZExbF03wX+PHSJl1oHMLRdTRysLU0d5r1TqfS3hLn4l26/tm9B2Iv6XvENWgd4/NNbJ/rczCK9hyz9gJS8a/oR7JZF/phcu6rvNRQdHJeXqT+9V1p9l0BQV/3rw8tg4ySo1Qm6zy4ss/p1/an/G8ndkOwd9D0ipUA/jaxhKdCPAXCurt8/6TSc2qTviYQ8VVjvjs/0n/nGProi9SgF+vviLaz1ZxIsrEFjBTUf0d9yB/prijE79ZciAtoU1pt8Rv/H2kJbuJ+FNWgs9f+WD4KuQP/vpFIX/jHPTIT9P+q/VLUfV1h2SX84vQVySzPlrqowUefnwIFf9K+fnFP4GWN2wNE/Shf3ze0Tveb6MYqcIk44Dqc2lq5ey5uSoYUWcjBOqA5e4FbL+MvmjTNMhnU2hV9CbVzB0du43le3F/8Mvi0Kv+Q+hMw6UV+9epXw8HA6dOjAunXr8PDw4PTp0zg7O5s6tErLz82Or/o14eXzV5m29hh7z13lq39Os3hPLKM7BdK3mS+WmodoQjtrR/1SlI0zNHul9HUVvcoU+iLU7qJPlDdotND9i1sn+RsjUHPSC3/mZOhfF60jM0l/j/u1Ik9D0+ngv/kYnRUoiWcXFCbquIP6x6T6tzFO1Du/MD5WSWgdChP1laP6swjuQTBiT2GZX/vqk8mtWFjr20qluv4HXaVPpq3HFPYkE07Azz30lxte3Vq47++D4dIBffkb+6lURd6r9KN/s5L0Z05Q9E+gu3EKNTdDP/jIwsY4URfkFSZplVqfgAw9PtfCnxqrwuP4NC7c39IWHplQPEE1eA6qhhX5nDf9hJvWAa4BxnXcOK2stS9Sb5/rx79TXdd/aiz18dndNHZl+L/6z1M0gTd/Vb/cjwf1RawCMetEPWPGDKpXr878+fMN6/z9/U0X0EOksa8Lv73akg1HLzNj3XHOJGYy8Y8jzN95jnFdg+hS1wuV/IcqnaLtZedmPK86gKU1hA68v2PUewaqhep7yzcoOug0SZ/Yiyb6Gz/zs/Uzwd1YbswOZ1vkD7NTdajbs/hpyiYD9Nf91EVmlCtah64ACnL0Pcb8bH2vrmgdVrb6kbQ3Zq4ztIUtWDno9715sFB+tn65WdGeXUEOpF3U9+yLSr2o762XRm5W4Wu7KtCovz7p6goKr2d2maaf2MfWVX99VF3KL7NWttD2zeLrgx8vXT23Ejqo+Lrqze5/DoIKcMq4sjDrwWQhISF06dKFCxcusHXrVqpWrcqwYcN45ZWS92ZkMNn9yyvQsXhvLJ9vOkFihv6PZpifC+88XodQvxJOSSrEvdLp9Mk6P7vwZ34u+uuousJrqbbuYH99cp3cLEiM1n9puDEgCeDyEf24gxvXYBVd8dca7fWBSdcHJclMeKIclCY3mXWitrbWD2IaO3Yszz77LHv27GHMmDF8++23vPDCC7fcJycnh5ycHMP7ixcvEhISIom6DGTk5PPt1tN8t/0M2Xn6ew7bBLrTxNeFOt6OhHg7Ut3VRnraQghxF5UmUVtZWREWFkZERIRh3ahRo9i7dy+7du265T6TJ09mypTiw/glUZedy2nZzNpwgqX7Yrn5oVwOWguCvR2o4+1oSN5BXg7mN/OZEEKYUKW5Pcvb25uQkBCjdXXq1GHZsmW33eedd95h7Nixhvc3etSi7Hg6WjOjVwNeaVuDbScSOBaXxtG4NE5eziA9J5+9566y99xVQ3m1CgLc7QqTt48+gXs4aKX3LYQQd2HWiTo8PJzo6GijdSdOnMDPz++2+2i1WrRareF9WlrabcuK+1PLw55aHoWDlvIKdJxJyDQk7mPXl8SMXE4nZHI6IZPVh+IM5V3trKjj7UAdL33yruPtSM0q9lhZPESjyoUQ4i7uKVHHxsaiUqkM3fU9e/awaNEiQkJCGDJkSJkF9/rrr9OqVSumTZtG79692bNnD/PmzWPevHlldgxRdiw1aoK8HAjycqBH48KnY11Jz+ZYXDpHLxUm7zOJmSRn5rLzVBI7TyUVqUNFLQ8HQrwdeSTYg851PR+u28GEEOIm93SNuk2bNgwZMoQBAwYQHx9PUFAQdevW5cSJE4waNYqJEyeWWYCrV6/mnXfe4eTJkwQEBDB27FgZ9V0JZOcVcOJy+vXEnW7ogadnG99O4+Gg5bmm1enb3BdvJ5l9SwhROZT7YDIXFxd2795NUFAQX3zxBUuWLGHnzp1s2LCBoUOHcuZMKe9TLEeSqCsORVG4cPUax+LS2H8+hWX7L5CQrh/Br1Gr6BjswYCWfoTXdDefp3wJIcQ9KPfBZHl5eYbrwJs2beLJJ58EIDg4mLi4uDvtKsRtqVQqqrvaUt3Vls51vXijc202HLnMz7vPsftMMhuOXmbD0cv4u9nSv4UfvUKr4WwrD7kQQlRu93Txr27dunzzzTds376djRs30rWrfp7hS5cu4ebmdpe9hSgZS42abg28WTykJRtfb8ugVv44aC04l5TFB2uO0XzaZt5cepDI2BTM+C5DIYS4L/d06nvLli08/fTTpKWlMXDgQH744QcA3n33XY4fP87y5cvLPNB7Jae+K5es3Hz+iLzEz7tiOBpXOKK/flUn+rfw5cmGVbGxknu2hRDm7YFMeFJQUEBaWhouLoVTSJ47dw5bW1s8PDzupcpyIYm6clIUhQOxKfyyK4bVUXHk5utnSnO0tuCZ0Gr0b+FHzSr2d6lFCCFMo9wT9bVr11AUBVtb/VNTYmJiWLFiBXXq1KFLly73FnU5kURd+SVn5rL0v1gW/nue88mFD1BoVdONAS386BQit3gJIcxLuSfqzp0707NnT4YOHUpKSgrBwcFYWlqSmJjIrFmzeO211+45+LImifrhodMpbDuZwC+7z/P38cuG6U09HLT0beZL32a+eDlZmzZIIYSgdLnpnroZ+/fvp00b/UPef//9dzw9PYmJieGnn37iiy++uJcqhbhvarWK9kEe/N/AMLaPe4QRHWrhbm/FlfQcPt98kvAZfzP0533sPJUog8+EEBXGPSXqrKwsHBz0D6vfsGEDPXv2RK1W06JFC2JiYso0QCHuRVVnG97sEkTE2x35om9jmgW4UqBTWH8knn7/9y8vLtjL+aSsu1ckhBAmdk+JulatWqxcuZLY2Fj++usvOnfuDMCVK1dwdHQs0wCFuB9WFmqebOjDb6+25K8xbRnQwg8rjZot0Qk8+tlW5vx9kpz8AlOHKYQQt3VPiXrixIm8+eab+Pv706xZM1q2bAnoe9eNGzcu0wCFKCtBXg6836Me68e0IbyWGzn5Oj7dcILHP9/OrtNJd69ACCFM4J5vz4qPjycuLo6GDRuiVuvz/Z49e3B0dCQ4OLhMg7wfMphM3IqiKKw6eIn3Vx8lMSMXgJ6Nq/Jutzq422vvsrcQQtyfB3IfddGDqVQqqlatevfCJiCJWtxJ6rU8Pv0rml/+jUFR9Pdhj3ssmL5NfWU+cSFEuSn3Ud86nY6pU6fi5OSEn58fvr6+ODs78/7776PT6e4paCFMwcnGkvd71GPFsHDq+jiSlp3P+BWHeeabCI5cSjV1eEIIcW+Jevz48cyZM4ePPvqIAwcOsH//fqZNm8aXX37JhAkTyjpGIcpdo+rO/DE8nEndQ7DXWnDgfArdv9zB+6uPkpGTf/cKhBCinNzTqW8fHx+++eYbw1Ozbvjjjz8YNmwYFy9eLLMA75ec+haldTktm6mrj7LmkP5JcF6O1kzqHkLXel6oVHI6XAhx/8r91HdycvItB4wFBweTnJx8L1UKYTY8Ha356vkm/Di4Gb6utsSnZfPawv1y77UQwiTuKVE3bNiQOXPmFFs/Z84cGjRocN9BCWEO2tWuwobX2zLqkVpYalRy77UQwiTu6dT31q1b6datG76+vrRs2RKVSkVERASxsbGsXbvWML2oOZBT36IsnE7IYMLKw0Rcv9+6ZhU7PuhRn5Y15fnrQojSK/dT3+3atePEiRM8/fTTpKSkkJycTM+ePTly5Ajz58+/p6CFMGc1q9iz8OXmzO7TCHd7K04nZNL3u92MXRJJYkaOqcMTQlRi930fdVEHDx6kSZMmFBSYz2lB6VGLspaalccnG46z8N/zcu+1EOKelHuPWoiHmZOtJR/0qM/y11oR4m187/Xec8kU6OTJXEKIsmNh6gCEqKga+7qwakQ4P+2KYeaGaA6cT+HZb3bhbGtJeC132ga60yawCj7ONqYOVQhRgUmiFuI+WGjUDG4dwOP1vfn4r+NsPHKZlKw81hyKM9yHXbOKHW0Cq9C2tjvNA9yw08p/OyFEyZXqL0bPnj3vuD0lJeV+YhGiwvJysmZW70bkF+g4eCGFrScS2X4ygYOxKZxOyOR0QiYLIs5hqVER6udC29pVaBtYhRBvR7muLYS4o1Ilaicnp7tuf+GFF+4rICEqMguNmlA/V0L9XBn7aG1Ss/KIOJ3ItpOJbDuRwMWUa+w+k8zuM8l8vD4aVzsrWtdyp8310+ReTtam/ghCCDNTpqO+y9v06dN59913GT16NLNnzy7RPjLqW5gLRVE4l5TF9pMJbDuRyK7TiWTmGt8hUdvTnjaBVWgTqD9NbmOlMVG0QojyVJrcVGEulu3du5d58+bJzGeiwlKpVAS42xHgbscLLf3JK9Bx4HyKPnGfTOTQhRROXM7gxOUMvt9xFiuNmqYBLrQNrELrQHfqeMlpciEeRhUiUWdkZNCvXz++++47PvjgA1OHI0SZsNSoaRbgSrMAV97oHMTVzFwiTidd73EncCk1m52nkth5KgnWgZudFa1quRNe043wWu5Ud7U19UcQQjwAFSJRDx8+nG7dutGpU6e7JuqcnBxycgpnikpPTy/v8IQoEy52VnRr4E23Bt4oisKZxEy2n0hg+8lEdp1JIikzlz8PXuLPg5cA8HOzpVVNd1rXcqdlTTdc7axM/AmEEOXB7BP14sWL2b9/P3v37i1R+enTpzNlypRyjkqI8qVSqahZxZ6aVewZFB5Abr6OyNgUdpxKJOJUIgdiU4hJyiIm6Ty/7jmPSgUh3o60ruVOeC13mvq7yvVtISoJsx5MFhsbS1hYGBs2bKBhw4YAtG/fnkaNGt12MNnNPeqLFy8SEhIig8lEpZKencees8nXT40nEn3Z+MyRlUZNEz9nWtdyp1UtdxpUdcJCIxMRCmEuSjOYzKwT9cqVK3n66afRaAp7BgUFBahUKtRqNTk5OUbbbkVGfYuHwZX0bHadTmLHyUR2nkrkUmq20XYHrQUtaroRXtON1oHu1Kxij0olA9OEMJVKk6jT09OJiYkxWvfiiy8SHBzMuHHjqFev3l3rkEQtHjY3bgPbcSqRndevb6deyzMq4+moJbym/jR5m9rueDjI/dtCPEiV5vYsBweHYsnYzs4ONze3EiVpIR5GRW8DG9DCjwKdwpFLqdevbyex51wyl9NyWH7gIssPXMRSo2JMp9oMbVcTjdz+JYTZMetELYS4fxq1igbVnGlQzZlh7WuRnVfAvpir7DyVyNYTCRy5lMYnf0Xz9/ErzOrdED83O1OHLIQowqxPfZcFOfUtxO0pisKy/ReZvOoIGTn52FppmPBECM81rS7XsIUoR/I8aiFEiahUKnqFVmPd6DY0D3AlK7eAd5ZH8fKP/5GQnnP3CoQQ5U4StRCC6q62/PpKC8Y/XgcrjZrNx6/QZfY21h+ON3VoQjz0JFELIQBQq1W80rYGq0aGU8fbkeTMXIb+so83fjtIWnbe3SsQQpQLSdRCCCPBXo6sHN6K19rXRK2CZfsv8Njs7ew+k2Tq0IR4KEmiFkIUo7XQMK5rML+92hJfV1suplyj73e7mbb2GNl5BXevQAhRZiRRCyFuK8zflbWj2/Bc0+ooCszbdoan5uzk6KU0U4cmxENDErUQ4o7stRZ89EwD/u+FMNztrYi+nM5TX+3g6y2nKNBV6rs7hTALkqiFECXSKcSTv8a0pXOIJ3kFCh+vj6bPt7s4n5Rl6tCEqNQkUQshSszNXsu3A0L5pFcD7LUW/Bdzlcc+38biPeep5HMnCWEykqiFEKWiUql4Nqw660a3oVmAK5m5Bby9PIpXfpJJUoQoD5KohRD35MYkKe8+HoyVRs2mY1foOnsbfx2RSVKEKEuSqIUQ90yjVjGkbU1WjQwn2MuBpMxcXv15H28uPUi6TJIiRJmQRC2EuG/BXo78MSKcoe1qolLB7/su0HX2dtZFxcl910LcJ3nMpRCiTGgtNLz9WDAd63gw9rdIYpOv8drC/dhrLehYx4PH63vTrnYVrC01pg5ViApFErUQokw19Xdl3ei2zPn7FKsiL3IpNZs/Ii/xR+Ql7Kw0dKzjyeP1vWkfJElbiJKQ51ELIcqNTqcQeSGFtYfiWBsVx6XUbMM2OysNj9TxpFt9L9oHeUjSFg+V0uQmSdRCiAdCURQiY1NYGxXH2qh4LqZcM2yztdLwSLD+9HiHIA9srCRpi8pNEnURkqiFMD+KonDwQipro+JYcyjOKGnbWBZJ2sFVsLWSK3Si8pFEXYQkaiHMm6IoHLqRtKPiuHDVOGl3CK7C4/W9eSTYQ5K2qDQkURchiVqIikNRFKIuprImSn9NOza5MGlbW6rpEORhSNp2WknaouKSRF2EJGohKiZFUTh8Mc2QtM8nFz78Q2uhJszfhZY13GhZ040G1Zyx1Mi0EKLiKE1ukq+kQgizpFKpqF/NifrVnBjXNYgjlwqTdkxSFjtPJbHzVBKgH4wW5u9KyxputKjhSv2qTlhI4haVhCRqIYTZU6lU1KvqRL2qTvyvSxCnrmSw60wSu04nsftMElez8th2IoFtJxIA/TO0m/q70LKmGy1ruBPi44hGrTLxpxDi3kiiFkJUKCqVikBPBwI9HXihpT86nUL05XR2nU5i15kk/j2TRFp2Pv9EJ/BPtD5xO1pb0CzA7XridiPYywG1JG5RQZh1op4+fTrLly/n+PHj2NjY0KpVK2bMmEFQUJCpQxNCmAm1WkUdb0fqeDsyuHUABTqFY3Fp7L7e495zNpm07Hw2HbvMpmOXAXCxtaT5jcRd041AD3tUKkncwjyZ9WCyrl278txzz9G0aVPy8/MZP348UVFRHD16FDs7uxLVIYPJhHi45RfoOHIpzXCqfO+5ZLJyjR8U4m5vRfMa+t52U39XalSxk8FpolxV2lHfCQkJeHh4sHXrVtq2bVuifSRRCyGKyivQcehCqqHH/V9MMtl5OqMylhoVNavYU9vTgSAvB4K99D+rOttIz1uUiUo76js1NRUAV1dXE0cihKioLDVqQv1cCPVzYXiHWuTkF3AwNvX6Ne5EDl9MIyMnn+Px6RyPT4eDhfvaay2o7WlPkJcjQdd/Bns54GJnZboPJCq9CtOjVhSFp556iqtXr7J9+/bblsvJySEnJ8fw/uLFi4SEhEiPWghRIoqicDHlGtHx6URfTtf/jE/ndEIGeQW3/nPp4aAlyMuBIEMP3JFaHvYyZ7m4rUrZox4xYgSHDh1ix44ddyw3ffp0pkyZ8oCiEkJUNiqVimoutlRzsaVjHU/D+rwCHWcTMzken050fBrR8RlEX04jNvkaV9JzuJKew/aTiUXqAX83O4I8Hah9/fR5eE13nGwtTfGxRAVWIXrUI0eOZOXKlWzbto2AgIA7lpUetRDiQcrIyefk9Z738fh0Tlx/nZSZW6ysrZWGXqHVeDE8gAD3kg2IFZVTpelRK4rCyJEjWbFiBVu2bLlrkgbQarVotVrD+7S0tPIMUQjxkLPXWtDY14XGvi5G6xPSczhxOd3QA98Xc5XTCZn8tCuGn3fH0DHYg8GtA2hZw00GqIk7MutEPXz4cBYtWsQff/yBg4MD8fHxADg5OWFjY2Pi6IQQ4vaqOGip4qAlvJY7oO94RJxO4vsdZ/n7+BU2HdMvdbwdeal1AN0beqO1kGvaojizPvV9u2+Z8+fPZ9CgQSWqQ27PEkKYm9MJGczfeZbf910w3Brmbq9lQAs/+rfwxc1ee5caREVXae+jvheSqIUQ5iolK5dFe87zU0QM8WnZAFhZqHm6UVUGtw4gyMvBxBGK8iKJughJ1EIIc5dXoGNtVBzf7zjLoQuphvVtAt0Z3DqAdoFVZG7ySqbSDCYTQoiHgaVGzVONqvJkQx/2xVzl+x1n+etIPNtPJrL9ZCI1q9gxuHUAPRtXk3uzH0LSoxZCCDMUm5zFgohzLNkbS0ZOPgDOtpY838yXF1r64+VkbeIIxf2QU99FSKIWQlRk6dl5/PbfBRZEnCU2+RoAFmoVTzTw5qXWNahfzcnEEYp7IYm6CEnUQojKoECnsPFoPN/vOMvec1cN65v5uzIo3J92tatgp5WrmRWFXKMWQohKRqNW0bWeN13reXPoQgrf7zjLmkNx7DmXzJ5zyVioVTT2daZVTXfCa7nTqLozVhbyqM7KQHrUQghRQcWnZvPTrnP8eeiS4bT4DbZWGpoFuNK6ljutaroT7OUgI8fNiJz6LkIStRDiYRCbnMXOU4nsOJXIrtNJxeYad7OzomVNN8JrudO6ljvVXW1NFKkASdRGJFELIR42Op1C9OV0dp5KZOepRP49m0xWboFRmequNobedquabjIb2gMmiboISdRCiIddbr6OgxdS2HEykYjTiRw4n0K+zvhPfx1vR8JruhEe6E4zf1cZmFbOJFEXIYlaCCGMZeTks/dssuFU+fH4dKPtFmoVTXxdaFXLjeYBbtSv5oS9JO4yJaO+hRBC3Ja91oIOwR50CPYAIDEjh4jTSURcT9wXrl4zjCaHk6hUEOhhT8NqzjSs7kyj6s4EeTlgqZFR5Q+CJGohhHjIudtrebKhD0829AHgfFIWO04lsvN0IpHnU7iYco0TlzM4cTmDpfsuAKC1UFPXx9GQuBtWc8bPzVaerV0OJFELIYQw4utmy/Nuvjzf3BeAK+nZHIpN5eCFFCJjUzgYm0Jadj77z6ew/3yKYT8nG0t94q7mRMPqzjSo5kwVBxmkdr8kUQshhLgjDwdrOoVY0ynEEwBFUTiXlMXB2OuJ+0IKRy6lkXotj20nEth2IsGwb1VnG32Pu7oTDas5U6+qkwxUKyVpLSGEEKWiUqkIcLcjwN2OHo2rAvqR5dHx6URe0Pe4D8amcCohg4sp17iYco01UXEAqFVQ29OBBtWcqFnFHj83W3xd7fBzs5UEfhvSKkIIIe6blYWa+tWcqF/NiQEt/AD9A0WiLqZyMDZVn7wvpBCXms3x+PRiI81Bf63cz80WP1dbfN1s8Xezw/f6e1c7q4f2+rckaiGEEOXCwdry+oQq7oZ1l9OyORibwuGLqZxLyiImOYvzSZlczcojMSOHxIwc9sVcLVaXvdYCX1db/N0Le+B+rrb4udvh5WiNphJPjyqJWgghxAPj6WhN57pedK7rZbQ+9Voe55OyiEnOJCYpi5gk/c/zyVnEpWaTkZPP0bg0jsalFavTSqOmmquNPnG72VHd1Zaqzjb6xcUGF1vLCt0bl0QthBDC5JxsLA2nzm+WnVfAhatZnEss7IHHJGcRk5TFhatZ5BboOJOQyZmETCCh2P7Wlmp8biRuZxt8DIs1VZ1t8HayMesnjUmiFkIIYdasLTXU8nCglodDsW0FOoVLKdc4fz1xxyRnEpucxaWUbC6lXONKeg7ZeUUTeXEqFVSx1xYmcxcbfJysDQm9mosNTjam65VLohZCCFFhadQqqrvaUt3VlvBaxbfn5BcQn5rNxZRrXErJ5uLVa1xKucal1GvX110jO0/HlfQcrqTnEBmbcsvj2Fpp8HG2oa6PI58/17h8P9RNJFELIYSotLQWGvzc7PBzs7vldkVRSM7M1Sfx64n7UkphEr+Ykk1iRg5ZuQWcupKBrZXmAX8CSdRCCCEeYiqVCjd7LW722lteHwf9NfK4VP2pdFOQRC2EEELcgbWlxjDBiymY7zC3Ir7++msCAgKwtrYmNDSU7du3mzokIYQQ4oEw+0S9ZMkSxowZw/jx4zlw4ABt2rThscce4/z586YOTQghhCh3Zp+oZ82axUsvvcTLL79MnTp1mD17NtWrV2fu3LmmDk0IIYQod2adqHNzc9m3bx+dO3c2Wt+5c2ciIiJuuU9OTg5paWmGJT29+HyyQgghREVh1ok6MTGRgoICPD09jdZ7enoSHx9/y32mT5+Ok5OTYQkJCXkQoQohhBDlokKM+r55NhhFUW47Q8w777zD2LFjDe9jY2OpV68ecXFx5RqjEEIIUVI3cpJOp7trWbNO1O7u7mg0mmK95ytXrhTrZd+g1WrRarWG91lZWQA0a9as/AIVQggh7sHly5fx9fW9YxmzTtRWVlaEhoayceNGnn76acP6jRs38tRTT5WojsaNG7Nnzx48PT1Rq+/vTH96ejohISEcPXoUB4fic86K4qTNSk/arPSkzUpP2qz0yrLNdDodly9fpnHju09HqlIURbmvo5WzJUuWMGDAAL755htatmzJvHnz+O677zhy5Ah+fn4PNJa0tDScnJxITU3F0dHxgR67opI2Kz1ps9KTNis9abPSM1WbmXWPGqBPnz4kJSUxdepU4uLiqFevHmvXrn3gSVoIIYQwBbNP1ADDhg1j2LBhpg5DCCGEeODM+vYsc6PVapk0aZLRYDVxZ9JmpSdtVnrSZqUnbVZ6pmozs79GLYQQQjzMpEcthBBCmDFJ1EIIIYQZk0QthBBCmDFJ1KUgz8UuuenTp9O0aVMcHBzw8PCgR48eREdHmzqsCmP69OmoVCrGjBlj6lDM3sWLF+nfvz9ubm7Y2trSqFEj9u3bZ+qwzFJ+fj7vvfceAQEB2NjYUKNGDaZOnVqiaSwfFtu2baN79+74+PigUqlYuXKl0XZFUZg8eTI+Pj7Y2NjQvn17jhw5Uq4xSaIuIXkuduls3bqV4cOHs3v3bjZu3Eh+fj6dO3cmMzPT1KGZvb179zJv3jwaNGhg6lDM3tWrVwkPD8fS0pJ169Zx9OhRZs6cibOzs6lDM0szZszgm2++Yc6cORw7doyPP/6YTz75hC+//NLUoZmNzMxMGjZsyJw5c265/eOPP2bWrFnMmTOHvXv34uXlxaOPPlq+T2pURIk0a9ZMGTp0qNG64OBg5e233zZRRBXLlStXFEDZunWrqUMxa+np6UpgYKCyceNGpV27dsro0aNNHZJZGzdunNK6dWtTh1FhdOvWTRk8eLDRup49eyr9+/c3UUTmDVBWrFhheK/T6RQvLy/lo48+MqzLzs5WnJyclG+++abc4pAedQncy3OxhbHU1FQAXF1dTRyJeRs+fDjdunWjU6dOpg6lQli1ahVhYWE8++yzeHh40LhxY7777jtTh2W2WrduzebNmzlx4gQABw8eZMeOHTz++OMmjqxiOHv2LPHx8Ua5QKvV0q5du3LNBRViZjJTu5fnYotCiqIwduxYWrduTb169UwdjtlavHgx+/fvZ+/evaYOpcI4c+YMc+fOZezYsbz77rvs2bOHUaNGodVqeeGFF0wdntkZN24cqampBAcHo9FoKCgo4MMPP6Rv376mDq1CuPH3/la5ICYmptyOK4m6FErzXGxRaMSIERw6dIgdO3aYOhSzFRsby+jRo9mwYQPW1tamDqfC0Ol0hIWFMW3aNED/tLwjR44wd+5cSdS3sGTJEn755RcWLVpE3bp1iYyMZMyYMfj4+DBw4EBTh1dhPOhcIIm6BO7ludhCb+TIkaxatYpt27ZRrVo1U4djtvbt28eVK1cIDQ01rCsoKGDbtm3MmTOHnJwcNBqNCSM0T97e3oSEhBitq1OnDsuWLTNRRObtrbfe4u233+a5554DoH79+sTExDB9+nRJ1CXg5eUF6HvW3t7ehvXlnQvkGnUJFH0udlEbN26kVatWJorKvCmKwogRI1i+fDl///03AQEBpg7JrHXs2JGoqCgiIyMNS1hYGP369SMyMlKS9G2Eh4cXu+3vxIkT8nS928jKykKtNv6zr9Fo5PasEgoICMDLy8soF+Tm5rJ169ZyzQXSoy6hsWPHMmDAAMLCwgzPxT5//jxDhw41dWhmafjw4SxatIg//vgDBwcHw9kIJycnbGxsTByd+XFwcCh2/d7Ozg43Nze5rn8Hr7/+Oq1atWLatGn07t2bPXv2MG/ePObNm2fq0MxS9+7d+fDDD/H19aVu3bocOHCAWbNmMXjwYFOHZjYyMjI4deqU4f3Zs2eJjIzE1dUVX19fxowZw7Rp0wgMDCQwMJBp06Zha2vL888/X35Bldt48kroq6++Uvz8/BQrKyulSZMmcqvRHQC3XObPn2/q0CoMuT2rZP7880+lXr16ilarVYKDg5V58+aZOiSzlZaWpowePVrx9fVVrK2tlRo1aijjx49XcnJyTB2a2fjnn39u+bdr4MCBiqLob9GaNGmS4uXlpWi1WqVt27ZKVFRUucYkT88SQgghzJhcoxZCCCHMmCRqIYQQwoxJohZCCCHMmCRqIYQQwoxJohZCCCHMmCRqIYQQwoxJohZCCCHMmCRqIYQQwoxJohZClDmVSsXKlStNHYYQlYIkaiEqmUGDBqFSqYotXbt2NXVoQoh7IA/lEKIS6tq1K/Pnzzdap9VqTRSNEOJ+SI9aiEpIq9Xi5eVltLi4uAD609Jz587lsccew8bGhoCAAJYuXWq0f1RUFI888gg2Nja4ubkxZMgQMjIyjMr88MMP1K1bF61Wi7e3NyNGjDDanpiYyNNPP42trS2BgYGsWrXKsO3q1av069ePKlWqYGNjQ2BgYLEvFkIIPUnUQjyEJkyYwDPPPMPBgwfp378/ffv25dixY4D+mcVdu3bFxcWFvXv3snTpUjZt2mSUiOfOncvw4cMZMmQIUVFRrFq1ilq1ahkdY8qUKfTu3ZtDhw7x+OOP069fP5KTkw3HP3r0KOvWrePYsWPMnTsXd3f3B9cAQlQk5fpsLiHEAzdw4EBFo9EodnZ2RsvUqVMVRdE/gnTo0KFG+zRv3lx57bXXFEVRlHnz5ikuLi5KRkaGYfuaNWsUtVqtxMfHK4qiKD4+Psr48eNvGwOgvPfee4b3GRkZikqlUtatW6coiqJ0795defHFF8vmAwtRyck1aiEqoQ4dOjB37lyjda6urobXLVu2NNrWsmVLIiMjATh27BgNGzbEzs7OsD08PBydTkd0dDQqlYpLly7RsWPHO8bQoEEDw2s7OzscHBy4cuUKAK+99hrPPPMM+/fvp3PnzvTo0YNWrVrd02cVorKTRC1EJWRnZ1fsVPTdqFQqABRFMby+VRkbG5sS1WdpaVlsX51OB8Bjjz1GTEwMa9asYdOmTXTs2JHhw4fz6aeflipmIR4Gco1aiIfQ7t27i70PDg4GICQkhMjISDIzMw3bd+7ciVqtpnbt2jg4OODv78/mzZvvK4YqVaowaNAgfvnlF2bPns28efPuqz4hKivpUQtRCeXk5BAfH2+0zsLCwjBga+nSpYSFhdG6dWsWLlzInj17+P777wHo168fkyZNYuDAgUyePJmEhARGjhzJgAED8PT0BGDy5MkMHToUDw8PHnvsMdLT09m5cycjR44sUXwTJ04kNDSUunXrkpOTw+rVq6lTp04ZtoAQlYckaiEqofXr1+Pt7W20LigoiOPHjwP6EdmLFy9m2LBheHl5sXDhQkJCQgCwtbXlr7/+YvTo0TRt2hRbW1ueeeYZZs2aZahr4MCBZGdn89lnn/Hmm2/i7u5Or169ShyflZUV77zzDufOncPGxoY2bdqwePHiMvjkQlQ+KkVRFFMHIYR4cFQqFStWrKBHjx6mDkUIUQJyjVoIIYQwY5KohRBCCDMm16iFeMjI1S4hKhbpUQshhBBmTBK1EEIIYcYkUQshhBBmTBK1EEIIYcYkUQshhBBmTBK1EEIIYcYkUQshhBBmTBK1EEIIYcYkUQshhBBm7P8BwRPujPjMxSgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    \n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                  \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f63439",
   "metadata": {},
   "source": [
    "#### 5. Text Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea21979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5ffc6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed lun\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a69b8",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "67b38a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the next-token generation process, let's use a small vocab.\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a5ba265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    " )\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5a6c4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# To implement a probabilistic sampling process, we can now replace argmax with the multinomial function in PyTorch.\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "\"\"\"\n",
    "As a result, \"forward\" will still be printed as it is still the most likely next word. \n",
    "However, this time, it will not be the case ALL the time. Let's test this out.\n",
    "\"\"\"\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    \n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa329c4",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9b06e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the diversity of outputs depending on the temperature value.\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "        for T in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "           bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92621977",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 1 - CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacbf93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "612b9ac0",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 2 - PERSONAL ASSISTANT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b3746",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
