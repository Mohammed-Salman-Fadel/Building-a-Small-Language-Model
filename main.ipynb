{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c5fe9d",
   "metadata": {},
   "source": [
    "# **Stage One - Building the LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f70983",
   "metadata": {},
   "source": [
    "## Part 1 - *Data Preparation & Sampling*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578b9cc",
   "metadata": {},
   "source": [
    "### All Imports Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "import urllib.request\n",
    "import zipfile \n",
    "import os\n",
    "from pathlib import Path\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import time\n",
    "\n",
    "import json\n",
    "import urllib\n",
    "from functools import partial\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee58bf5",
   "metadata": {},
   "source": [
    "#### Importing the Dataset Example\n",
    "In our case, we will be using the pdf form of the book \"The Verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "006a75eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x1ccdc749df0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3146f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b5ceb",
   "metadata": {},
   "source": [
    "### Creating a simple **tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a534b0",
   "metadata": {},
   "source": [
    "#### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "43373940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ']\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to create tokens.\n",
    "# We want to filter out whiite spaces and special characters.\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(preprocessed[:50])\n",
    "\n",
    "# Will filter out any whitespaces and only return the characters.\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b258f4",
   "metadata": {},
   "source": [
    "#### Step 2: Determining the Vocabulary and mapping them to their token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad18907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The vocabulary is a dictionary of all the unique tokens mapped into some numeric value stating \n",
    "the order of the token in the sequence of alphabetically arranged tokens.\n",
    "'''\n",
    "\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# Print first 20 vocab elements\n",
    "for i, item in enumerate(vocab.items()):\n",
    "  print(item)\n",
    "  if i >= 20:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d6af922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleTokenizerV1 - A simple tokenizer that can perform encoding and decoding.\n",
    "\n",
    "SimpleTokenizerV2 - Replaces Uknown words with the special character <|unk|> and\n",
    "unrelated pieces of texts with <|endoftext|>\n",
    "\n",
    "\"\"\"\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "      self.str_to_int = vocab\n",
    "      self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "      preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "      preprocessed = [\n",
    "      item.strip() for item in preprocessed if item.strip()\n",
    "      ]\n",
    "      ids = [self.str_to_int[s] for s in preprocessed]\n",
    "      return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "      text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3d5986e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [\n",
    "    item.strip() for item in preprocessed if item.strip()\n",
    "    ]\n",
    "\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                else \"<|unk|>\" for item in preprocessed]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc647e92",
   "metadata": {},
   "source": [
    "##### Create Byte-Pair Encoder\n",
    "- Is a subword tokenization algorithm. The most common pair of consecutive bytes of data is replaced with a byte that does not occur in data.\n",
    "\n",
    "Advantages:\n",
    "- Byte-pair encoding can reduce the size of the vocabulary significantly.\n",
    "- The BPE tokenizer can handle any unknown word without needing the `<|unk|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ab7a058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a498ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text)) # Prints the new number of tokens using the GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "63be29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "z:            [2241, 287, 257, 4489]\n",
      "------------------------------------\n",
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To better visualize what's being done\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "z = enc_sample[2:context_size+2]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "print(f\"z:            {z}\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Representation: left side = input, right side = target\n",
    "for i in range(1, context_size+1):\n",
    "  context = enc_sample[:i]\n",
    "  desired = enc_sample[i]\n",
    "  print(context, \"---->\", desired)\n",
    "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired])) # Text equivalent\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "89f21822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Implementation\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # Tokenizes the entire text\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "\n",
    "    # Uses a sliding window approach to chunk the book into overlapping sequences.\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Dataloader Implementation\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                 stride=128, shuffle=True, drop_last=True,\n",
    "                 num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiates the gpt2 tokenizer\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  # Initialize the Dataset class created earlier\n",
    "\n",
    "    # Intantiates and provides parameters for the DataLoader python class provided by PyTorch.\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "549478f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619],\n",
      "        [ 3619,   402,   271, 10899,  2138,   257]]), tensor([[  367,  2885,  1464,  1807,  3619,   402],\n",
      "        [  402,   271, 10899,  2138,   257,  7026]])]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "    dataloader = create_dataloader_v1(\n",
    "      raw_text, batch_size=2, max_length=6, stride=5, shuffle=False\n",
    "    )\n",
    "    \n",
    "    data_iter = iter(dataloader)  # Creates an Iterator\n",
    "    first_batch = next(data_iter) # Gets the next batch from the data Iterator, the first_batch will be assigned a tuple of tensors.\n",
    "    print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b5d30748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  257,  7026, 15632,   438,  2016,   257],\n",
      "        [  257,   922,  5891,  1576,   438,   568]]), tensor([[ 7026, 15632,   438,  2016,   257,   922],\n",
      "        [  922,  5891,  1576,   438,   568,   340]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a32d20",
   "metadata": {},
   "source": [
    "#### Step 3: Mapping Positional Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fe982",
   "metadata": {},
   "source": [
    "##### Positional Embedding Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a43a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the inputs that we'll be using, with their embeddings, given that each word has 3 dimensions. \n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c93ee5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 256])\n",
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Embedding function only requires the vocab_size\n",
    "as it'll provide random embeddings originally\n",
    "and those embeddings will be adjusted over training.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs.long())\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Instantiating the data loader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "   stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "836b290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24bc33",
   "metadata": {},
   "source": [
    "## Part 2 - *ATTENTION MECHANISM*\n",
    "There are 4 kinds of attention mechanisms and we will be implementing each one until we arrive to the original </br>\n",
    "transformer's **\"Multi-Head Self-Attention\"** mechanism, building on top of the previous implementation and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fabb8",
   "metadata": {},
   "source": [
    "### First Implementation: **Simplified Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a22ef6",
   "metadata": {},
   "source": [
    "#### Compute Attention Scores - 2nd input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fee95f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2956967., 16597455., 29439276., 30712224., 23737832.,  2581577.,\n",
      "        23769278., 12535636.])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # Takes 'journey' as the query.\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "  attention_scores_2[i] = torch.dot(x_i, query) \n",
    "\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002ab0",
   "metadata": {},
   "source": [
    "#### Compute Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "88a70b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.0208, 0.1166, 0.2068, 0.2158, 0.1668, 0.0181, 0.1670, 0.0881])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attention_weights_2_tmp)\n",
    "print(\"Sum: \", attention_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb429bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Sum: tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following is a naive Softmax Implementation. However, this implementation \n",
    "has weaknesses when dealing with very small or large values. Therefore it is \n",
    "recommended to simply use PyTorch's implmentation of Softmax.\n",
    "''' \n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attention_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2a347d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Softmax Function Implementation, results in same value of previous softmax function.\n",
    "attn_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1446cd",
   "metadata": {},
   "source": [
    "#### Compute Context Vectors\n",
    "The context vector z is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "becf5848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6203.2407, 3242.1980,  940.7390, 1740.2675])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attention_weights_2_tmp[i]*x_i #x_i is simply the input vector, multiplying with its corresponding attention weights.\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3122ce",
   "metadata": {},
   "source": [
    "#### A Generalised Approach\n",
    "So far, for the attention mechanism built above, we have implemented attention with respect to the embeddings vector of the second input. Now we will create a generalised method that can be applied to all embeddings vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa276d",
   "metadata": {},
   "source": [
    "#### Generalized Process\n",
    "We perform the same 3 steps performed prior, with a few modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c3f5b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ddcacc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nThe issue with the above implementation is that cause of the nested for loop\\nit is slow and computationally expensive, a better approach can be done with linear algebra.\\n'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, j_i in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, j_i)\n",
    "\n",
    "print(attention_scores) # Unnormalized output\n",
    "\n",
    "\"\"\" \n",
    "The issue with the above implementation is that cause of the nested for loop\n",
    "it is slow and computationally expensive, a better approach can be done with linear algebra.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9700d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following method is more efficient, not because the time complexity is any less,\n",
    "but the implementation time is far less, the time it takes to perform each multiplication, the '@' symbol represents matrix multiplication.\n",
    "You get the exact same answer as the previous method, but much faster.\n",
    "\"\"\"\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83715b4",
   "metadata": {},
   "source": [
    "#### Calculating the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d530f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "----------------------------------------------------------------\n",
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Declaring the dimensions to be -1 will normally apply the normalization to the last dimension, however, for a 2 dimensional tensor,\n",
    "the dim = -1 will apply the Softmax into all columns.\n",
    "'''\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights)\n",
    "\n",
    "# To verify that all rows sum up to 1\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attention_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641172",
   "metadata": {},
   "source": [
    "#### Computing the Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "200027bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Simply a matrix multiplication is performed. Concluding the generalised method of calculating context vectors.\n",
    "all_context_vec = attention_weights @ inputs\n",
    "print(all_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5497f",
   "metadata": {},
   "source": [
    "### Second Implementation: **Self-Attention**\n",
    "This self-attention mechanism is also called *scaled dot-product attention*. \n",
    "<br>We will be building on top of our last model, this time introducing **Trainable Weight Matrices**. This can help the model to produce better context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a933d6",
   "metadata": {},
   "source": [
    "For illustration purposes, we will be computing for only one context vector, *z(2)*.\n",
    "- Note that in GPT-like models, the input and output dimensions are usually the same,\n",
    " but to better follow the computation, we’ll use different input (d_in=3) and output\n",
    " (d_out=2) dimensions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a9e377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables dealing with the second input\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abb67c",
   "metadata": {},
   "source": [
    " Initialize the query, key, and value matrices of the second input \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ef594936",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "52e655dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7b100eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac2bf",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Input 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "aa4d33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]            \n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d2d81",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "26b54dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2) # Notice the attention score matches the previously calculated attention score for the second input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e1344",
   "metadata": {},
   "source": [
    "#### Calculate the Attention Weights of the entire Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cdbcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247b93b",
   "metadata": {},
   "source": [
    "#### Calculate the Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d624f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0915d",
   "metadata": {},
   "source": [
    "#### Implementing a compact self-attention Python class\n",
    "* In practice, with the LLM implementation that'll be performed later in mind, it is helpful to organize this code into a Python class, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b4abb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attention_scores = queries @ keys.T # omega\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3819522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "11b141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention Class version 2\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c55b9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\\n they use different initial weights for the weight matrices since nn.Linear uses a more\\n sophisticated weight initialization scheme.\\n'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "'''\n",
    " Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    " they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    " sophisticated weight initialization scheme.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de148abf",
   "metadata": {},
   "source": [
    "### Third Implementation: **Causal Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ddc795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)    \n",
    "keys = sa_v2.W_key(inputs) \n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56313a",
   "metadata": {},
   "source": [
    "#### Step 1: Initialise the masked matrix with the \"*tril*\" function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7526eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c81c",
   "metadata": {},
   "source": [
    "#### Step 2: Multiply the attention weights matrix with the masked matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "55398fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attention_weights * mask_simple\n",
    "print(masked_simple) # prints non-normalised values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586bc4",
   "metadata": {},
   "source": [
    "#### Step 3: Renormalize each row to sum up to 1 using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4e128baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd130fef",
   "metadata": {},
   "source": [
    "#### Second More Efficient Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "458ff6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ffa990d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "84c6f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995c95",
   "metadata": {},
   "source": [
    "#### Using *Dropout*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a8f89d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)   \n",
    "example = torch.ones(6, 6)     \n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e7b47d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying dropout to the attention weights\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f172bc",
   "metadata": {},
   "source": [
    "#### Implementing a Causal Attention Class\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented <br>\n",
    "earlier, except that we added the dropout and causal mask components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d6696b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)           \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                  \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2)   \n",
    "        attention_scores.masked_fill_(                   \n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "015435da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0f197cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa70423",
   "metadata": {},
   "source": [
    "### Final Implementation: **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8a206a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "        [CausalAttention(\n",
    "         d_in, d_out, context_length, dropout, qkv_bias\n",
    "     ) \n",
    "     for _ in range(num_heads)]\n",
    " )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "39937465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    "    )\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35d32",
   "metadata": {},
   "source": [
    "#### Efficient Multi-Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f1d5e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "         context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                        diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)   \n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim    \n",
    "        )\n",
    "\n",
    "        keys = keys.transpose(1, 2)   \n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] \n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec)   \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688d0e3",
   "metadata": {},
   "source": [
    "## PART 3 - *LLM ARCHITECTURE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "16afc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e852edb",
   "metadata": {},
   "source": [
    "### Placeholder GPT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f88db552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])      # Creates the 768 dimensional word embedding.\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])  # Creates the positional embeddings.\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])                        # Creates a random dropout embedding of 10% of the total embeddings. \n",
    "        self.trf_blocks = nn.Sequential(                                    # Creates an \"n_layers\" amount of transformer blocks.\n",
    "            *[DummyTransformerBlock(cfg)              \n",
    "            for _ in range(cfg[\"n_layers\"])]        \n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                    # Final normalization layer\n",
    "        self.out_head = nn.Linear(                                          # Projects the final hidden states to vocabulary size to get the logits for predicting the next token.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):                                              # in_idx param is a tensor of shape (batch_size, seq_len) containing token indices.\n",
    "        batch_size, seq_len = in_idx.shape                                  # Splitting the in_idx tensor into 2 seperate vectors.\n",
    "        tok_embeds = self.tok_emb(in_idx)                                   # Converts input tokens into vectors.\n",
    "        pos_embeds = self.pos_emb(                                          # Creates the (absolute) position vectors from the sequence length of each input.\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds                                         # Adds the token embeddings with the (absolute) positional embeddings, resulting into our prepared input.\n",
    "        x = self.drop_emb(x)                                                # We drop the randomly chosen matrix values (dropout regularization).\n",
    "        x = self.trf_blocks(x)                                              # Runs the input through each transformer block in sequence.\n",
    "        x = self.final_norm(x)                                              # Final normalization\n",
    "        logits = self.out_head(x)                                           # The output logits will contain the probability values needed for prediction.\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):                                     # Simple placeholder class that'll be replaced by a real Transformer Block later.\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):    \n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):                                            # Simple placeholder class that'll be replaced by a real LayerNorm later.\n",
    "    def __init__(self, normalized_shape, eps=1e-5):   \n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ccb7f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")           # Instantiate the GPT-2 tokenizer.\n",
    "batch = []                                          # Initialize an empty list\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))  # First encodes the text using GPT-2 tokenizer -> converted into a tensor -> is appended into the batch list.\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)                   # Takes in a list of tensors and \"stacks\" (concatenates) them into a 2D tensor.\n",
    "print(batch)                                        # Print the resulting 2D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "26e03364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe output tensor has two rows corresponding to the two text samples. \\nEach text sample consists of four tokens.\\n'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)                      # Passes the previously defined batch into our DummyGPTModel.\n",
    "print(\"Output shape:\", logits.shape)       # The standard shape distribution: (batch_size, seq_len, vocab_size).\n",
    "print(logits)                              # Outputs the 2 batches, each containing 4 sequences(inputs) and their embeddings relative to a vocab of 50257.\n",
    "\n",
    "\"\"\"\n",
    "The output tensor has two rows corresponding to the two text samples. \n",
    "Each text sample consists of four tokens.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "abdd6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)                   # Creates 2 batches of random numbers, each containing 5 values. \n",
    "layer = nn.Sequential(                              # Constructs a sequential neural network module.\n",
    "    nn.Linear(5, 6),                                # Creates a fully connected linear layer, taking in 5 input vectors and outputing 6.\n",
    "    nn.ReLU()                                       # Applies nonlinear activation function.\n",
    ")       \n",
    "out = layer(batch_example)                          # Feeds the nn layer with the batch example\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b5adaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)   # Keepdim ensures the dimensions stay the same, and doesn't combine the dimensions of the first and second input together.\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "347ca1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)          # Disables Scientific Notation \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78df5c",
   "metadata": {},
   "source": [
    "#### GELU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "efc64996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x  * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0cbdf",
   "metadata": {},
   "source": [
    "#### Layer Normalization Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "88b6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8adcd",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "74bc04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(                            # Simply an implemtation of a 3 layered forward neural network.\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),      # This linear portion \"expands\" the input embedding dimension into the number of nodes of the following hidden layer, in our case it'll 4 times the amount of the emb dim.\n",
    "            GELU(),                                             # Calls the GELU function on the nodes.\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])       # Does the opposite of the first linear layer, \"compressing\" from 4 times the size back to the original size of the input/\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8ec4a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)  # Intance\n",
    "x = torch.rand(2, 3, 768)           # 2 batches, each batch has 3 tokens, and each token will have an embedding size of 768.\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037276cc",
   "metadata": {},
   "source": [
    "### Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9bd77bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([                                   # Implement 5 layers.\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \n",
    "                  GELU())\n",
    " ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)        \n",
    "            if self.use_shortcut and x.shape == layer_output.shape:   \n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a4f39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)                           \n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "37e56d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)               # Forward Pass.\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)     # Calculates loss based on how close the target and outputs are.\n",
    "    \n",
    "    loss.backward()                 # Backward pass to calculate the gradients.\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "08db27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8432b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    " )\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a08917f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  \n",
    "   \n",
    "        shortcut = x        \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5812e",
   "metadata": {},
   "source": [
    "#### Instantiating a Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d3e4e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                  \n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be38b3",
   "metadata": {},
   "source": [
    "### GPT FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "550129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):        #initializes the token and positional embedding layers using the configurations passed in via cfg.\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(                # Creates transformer blocks equal to that in specified in cfg.\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]    \n",
    "            )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])     # Layer normalization is applied.\n",
    "        self.out_head = nn.Linear(  # linear output head without bias is defined, which projects the transformer’s output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f76141",
   "metadata": {},
   "source": [
    "#### Instantiating our GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "90a92ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)     # Shape: [2, 4, 50257]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2cba46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5e328bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f434dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    " )\n",
    "\n",
    "print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e422a",
   "metadata": {},
   "source": [
    "#### Calculate the total size needed for the 163 million parameters in our GPTModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "673d4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4      \n",
    "total_size_mb = total_size_bytes / (1024 * 1024)    \n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "208052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]   \n",
    "        with torch.no_grad():\n",
    "           logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]                   \n",
    "        probas = torch.softmax(logits, dim=-1)         \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7f44fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "baa9627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()                 \n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2b5236fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d9c1e",
   "metadata": {},
   "source": [
    "# **Stage Two - Foundational Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b901e7",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42bce8",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0e399602",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,   #! Context length dropped from 1024 -> 256.\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fa5be60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)                 # Unsqueeze adds a batch dimension. Shape becomes [1, seq_length]\n",
    "    return encoded_tensor\n",
    " \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)                 # Removes batch dimension, resulting in a 1D tensor.\n",
    "    return tokenizer.decode(flat.tolist())      # First the tensor is converted to a list of integers and then decoded to human readable text.\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(               # Calls and passes parameter values into the generate_text_simple function.\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),    # Is the starting prompt.\n",
    "    max_new_tokens=10,                                  # Specifies that 10 new tokens should be generated after the first prompt\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]      # Determines the maximum token size to be considered at once.\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))       # Calls token ids to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "16937771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs.long())\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7533de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "28125199",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e9b34",
   "metadata": {},
   "source": [
    "#### Calculating Logits and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3eea9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "937701dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)  # Since we have 2 batches, each containing 3 tokens, we received the highest probability value for each token in each batch.\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "75bd1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957afd64",
   "metadata": {},
   "source": [
    "### Text Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f199b",
   "metadata": {},
   "source": [
    "#### Calculating Target Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "57543b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0000,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Printing the initial Softmax probability scores\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409c271",
   "metadata": {},
   "source": [
    "#### Calculating the Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "aadc810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20d5f",
   "metadata": {},
   "source": [
    "#### Average Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2cc451d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8765)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed6e58",
   "metadata": {},
   "source": [
    "#### Negative Average Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "99a093be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1   # Simply multiply the average by -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbfa57",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "57be226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# It's important to keep track of the dimensions in order to perform cross entropy.\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e83bc9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "For the cross_entropy loss function in PyTorch, we want to flatten these tensors\n",
    "by combining them over the batch dimension:\n",
    "\n",
    "This is because the cross_entropy function only understands a 2D prediction matrix\n",
    "and a 1D target vector.\n",
    "'''\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c86fce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b42c4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52918.7773)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Perplexity\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22782",
   "metadata": {},
   "source": [
    "### Training & Validation Losses\n",
    "We will use the text file of \"the verdict\" once more to demonstrate training our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e4ccaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "31df5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)  # We will only be working with 5145 tokens for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56f834",
   "metadata": {},
   "source": [
    "#### Train and Testing Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "7901e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bca0de",
   "metadata": {},
   "source": [
    "#### Calling the Dataloader from Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ad18c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader is called for both training and validation sets.\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    " )\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4a1006d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Iterating through both data loaders\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")   \n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e11a",
   "metadata": {},
   "source": [
    "#### Cross entropy lost among Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6ef8e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)        \n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d37b8",
   "metadata": {},
   "source": [
    "#### Cross entropy lost over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7b45c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))  #  Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()   # Sums the loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches     # Averages the loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9352e13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98850186665853\n",
      "Validation loss: 10.990342140197754\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)  \n",
    "with torch.no_grad():                                       \n",
    "    train_loss = calc_loss_loader(train_loader, model, device)   \n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cd656",
   "metadata": {},
   "source": [
    "### The main function for pretraining LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "51f47fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model and generate_and_print_sample functions are not defined yet. \n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "               optimizer, device, num_epochs,\n",
    "               eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []   \n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                    \n",
    "            optimizer.step()                   \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:   \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(                     \n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b2de5",
   "metadata": {},
   "source": [
    "#### Evaluate model function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2389b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()            # Dropout is disabled \n",
    "    with torch.no_grad():   # Disables gradient tracking\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3693a8",
   "metadata": {},
   "source": [
    "#### Generate and print sample function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4d946563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1508a",
   "metadata": {},
   "source": [
    "#### Training a GPTModel for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f48aca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.823, Val loss 9.932\n",
      "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.336\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.621, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.043, Val loss 6.599\n",
      "Every effort moves you, and,, and,, and,,,, and, and,,,,,,,,, and,,,, the,,,, and,, and,,, the, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.547, Val loss 6.485\n",
      "Ep 3 (Step 000025): Train loss 5.450, Val loss 6.397\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.982, Val loss 6.301\n",
      "Ep 4 (Step 000035): Train loss 4.755, Val loss 6.296\n",
      "Every effort moves you, and I had been the of the picture to the picture.                                     \n",
      "Ep 5 (Step 000040): Train loss 4.162, Val loss 6.182\n",
      "Every effort moves you know the                                                \n",
      "Ep 6 (Step 000045): Train loss 3.742, Val loss 6.175\n",
      "Ep 6 (Step 000050): Train loss 3.213, Val loss 6.189\n",
      "Every effort moves you know the fact, and I felt--I had the fact a little of a little to my work, and in fact, and in the picture.      \"--and it, and, and down, and he was his\n",
      "Ep 7 (Step 000055): Train loss 3.143, Val loss 6.172\n",
      "Ep 7 (Step 000060): Train loss 2.422, Val loss 6.136\n",
      "Every effort moves you know the picture.  I glanced after him, and I was one of the house.\"   \"I didn't you know. I was his pictures.  \"--and I was a little a little the room, I was\n",
      "Ep 8 (Step 000065): Train loss 1.963, Val loss 6.178\n",
      "Ep 8 (Step 000070): Train loss 1.639, Val loss 6.230\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"Once, when I looked up, I had been. Gisburn--as Jack himself, as once one had been the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.286, Val loss 6.227\n",
      "Ep 9 (Step 000080): Train loss 0.987, Val loss 6.249\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs. \"--and I was a year after Jack's resolve had been his eyes; then I looked at the donkey--and I saw that, and down the room, I had\n",
      "Ep 10 (Step 000085): Train loss 0.727, Val loss 6.356\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),          \n",
    "    lr=0.0004, weight_decay=0.1\n",
    " )\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448655fb",
   "metadata": {},
   "source": [
    "#### Visualizing the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "185b08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    \n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                  \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)) \n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f63439",
   "metadata": {},
   "source": [
    "#### 5. Text Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "45921e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5ffc6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a69b8",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "67b38a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the next-token generation process, let's use a small vocab.\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a5ba265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    " )\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5a6c4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# To implement a probabilistic sampling process, we can now replace argmax with the multinomial function in PyTorch.\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "\"\"\"\n",
    "As a result, \"forward\" will still be printed as it is still the most likely next word. \n",
    "However, this time, it will not be the case ALL the time. Let's test this out.\n",
    "\"\"\"\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    \n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa329c4",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9b06e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e180614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the diversity of outputs depending on the temperature value.\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "        for T in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "           bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4d9fe",
   "metadata": {},
   "source": [
    "#### Top-K Sampling\n",
    "Selects the top 'k' probability values of the logits then sets all other values to `-inf` where the softmax function is applied and the remaining probabilities sum up to 1 and the `-inf` values become equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "872276f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Applying in code: Step 1 - Select the top 3 highest probability logits (k being 3 in this example)\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "\n",
    "# Step 2 - Assigning -inf for all non-selected logits.\n",
    "new_logits = torch.where(                               \n",
    "    condition=next_token_logits < top_logits[-1],       # Identifies logits less than the minimum in the top 3\n",
    "    input=torch.tensor(float('-inf')),                  # Assigns –inf to these lower logits\n",
    "    other=next_token_logits                             #  Retains the original logits for all other tokens\n",
    ")\n",
    "print(new_logits)\n",
    "\n",
    "# Step 3 - Applying the Softmax function\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "1a394983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "     temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):           # The for loop is the same as before: gets logits and only focuses on the last time step\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:               # Filters logits with top_k sampling\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        if temperature > 0.0:               #  Applies temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:                               #  Carries out greedy next token selection as before when temperature scaling is disabled\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:             #  Stops generating early if end-of-sequence token is encountered\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "7da3274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you not,\" she down surprise. You. GisitelyI quote by by\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=55,\n",
    "    temperature=1.6\n",
    "    )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0b149",
   "metadata": {},
   "source": [
    "### Loading and Saving Model Weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3058998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "be25a395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()    # switches the model to evaluation mode for inference, disabling the dropout layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "960a616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({    # Using torch.save, we can save both the model and optimizer state_dict contents\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c8a8b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "3477371f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x1cd91f43bc0>)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download GPT2 Weights\n",
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    "    )\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dcb1ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8ee84465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f946afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "91d1bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5c0e0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting which model variant to load\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "# Since we used a 256 token length earlier, meanwhile, the original model uses a token length of 1,024 we must update the model accordingly\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "\n",
    "# Bias vectors are no longer in use, however they we're for creating GPT-2\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4f93bfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating new gpt instance\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9c0bbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                          \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c5b59e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the function that'll load the weights into our gpt instance\n",
    "def load_weights_into_gpt(gpt, params):       #  Sets the model’s positional and token embedding weights to those specified in params\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):    # Iterates over each transformer block\n",
    "        q_w, k_w, v_w = np.split(             #  The np.split function is used to divide the attention and bias weights into three equal parts for the query, key, and value components.\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    " \n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "        \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "eaebf544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the weights\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c8edeb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    " )\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92621977",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 1 - CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacbf93",
   "metadata": {},
   "source": [
    "## Mobile Spam Text Classifier\n",
    "We will use a dataset to help us classify whether a text is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1673c7",
   "metadata": {},
   "source": [
    "### Importing Libraries and Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "507d3831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'sms_spam_collection\\\\SMSSpamCollection' -> 'sms_spam_collection\\\\SMSSpamCollection.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[273]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m         os.rename(original_file_path, data_file_path)              \n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile downloaded and saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mdownload_and_unzip_spam_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[273]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mdownload_and_unzip_spam_data\u001b[39m\u001b[34m(url, zip_path, extracted_path, data_file_path)\u001b[39m\n\u001b[32m     17\u001b[39m     zip_ref.extractall(extracted_path)\n\u001b[32m     19\u001b[39m original_file_path = Path(extracted_path) / \u001b[33m\"\u001b[39m\u001b[33mSMSSpamCollection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m              \n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile downloaded and saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: 'sms_spam_collection\\\\SMSSpamCollection' -> 'sms_spam_collection\\\\SMSSpamCollection.tsv'"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\" \n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(\n",
    "    url, zip_path, extracted_path, data_file_path):\n",
    "        if data_file_path.exists():\n",
    "            print(f\"{data_file_path} already exists. Skipping download \"\n",
    "                                    \"and extraction.\"\n",
    "            )\n",
    "        \n",
    "        with urllib.request.urlopen(url) as response:   \n",
    "            with open(zip_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:   \n",
    "            zip_ref.extractall(extracted_path)\n",
    "\n",
    "        original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "        os.rename(original_file_path, data_file_path)              \n",
    "        print(f\"File downloaded and saved as {data_file_path}\")\n",
    "    \n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b6288536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "4dac1c7c-4360-462e-b7aa-0b7415978ef8",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thats the way u feel. Thats the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "I‘m going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So ü pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged £5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg ü again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
    " )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e6b691b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "55e64fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]    \n",
    "\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
    "        num_spam, random_state=123\n",
    "    )                                        \n",
    "\n",
    "    balanced_df = pd.concat([\n",
    "        ham_subset, df[df[\"Label\"] == \"spam\"]\n",
    "    ])                              \n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5b124a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping ham to 1, and spam to 0\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6d5b912d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "06b8e739-e323-4238-9da7-f98ae85fe7f9",
       "rows": [
        [
         "4307",
         "0",
         "Awww dat is sweet! We can think of something to do he he! Have a nice time tonight ill probably txt u later cos im lonely :( xxx."
        ],
        [
         "4138",
         "0",
         "Just got to  &lt;#&gt;"
        ],
        [
         "4831",
         "0",
         "The word \"Checkmate\" in chess comes from the Persian phrase \"Shah Maat\" which means; \"the king is dead..\" Goodmorning.. Have a good day..:)"
        ],
        [
         "4461",
         "0",
         "This is wishing you a great day. Moji told me about your offer and as always i was speechless. You offer so easily to go to great lengths on my behalf and its stunning. My exam is next friday. After that i will keep in touch more. Sorry."
        ],
        [
         "5440",
         "0",
         "Thank you. do you generally date the brothas?"
        ],
        [
         "4448",
         "0",
         "Please tell me you have some of that special stock you were talking about"
        ],
        [
         "2603",
         "0",
         "So when you gonna get rimac access "
        ],
        [
         "2219",
         "0",
         "Nice talking to you! please dont forget my pix :) i want to see all of you..."
        ],
        [
         "5219",
         "0",
         "Pls she needs to dat slowly or she will vomit more."
        ],
        [
         "1035",
         "0",
         "ZOE IT JUST HIT ME 2 IM FUCKING SHITIN MYSELF IL DEFO TRY MY HARDEST 2 CUM 2MOROW LUV U MILLIONS LEKDOG"
        ],
        [
         "3515",
         "0",
         "I always chat with you. In fact i need money can you raise me?"
        ],
        [
         "3115",
         "0",
         "Yes watching footie but worried we're going to blow it - Phil Neville?"
        ],
        [
         "3553",
         "0",
         "Lol u still feeling sick?"
        ],
        [
         "5264",
         "0",
         "Storming msg: Wen u lift d phne, u say \"HELLO\" Do u knw wt is d real meaning of HELLO?? . . . It's d name of a girl..! . . . Yes.. And u knw who is dat girl?? \"Margaret Hello\" She is d girlfrnd f Grahmbell who invnted telphone... . . . . Moral:One can 4get d name of a person, bt not his girlfrnd... G o o d n i g h t . . .@"
        ],
        [
         "5539",
         "0",
         "Just sleeping..and surfing"
        ],
        [
         "2008",
         "0",
         "Hi here. have birth at on the  to  at 8lb 7oz. Mother and baby doing brilliantly."
        ],
        [
         "291",
         "0",
         "Hey you told your name to gautham ah?"
        ],
        [
         "3305",
         "0",
         "IM GONNAMISSU SO MUCH!!I WOULD SAY IL SEND U A POSTCARD BUTTHERES ABOUTAS MUCH CHANCE OF MEREMEMBERIN ASTHERE IS OFSI NOT BREAKIN HIS CONTRACT!! LUV Yaxx"
        ],
        [
         "3812",
         "0",
         "Excellent! Wish we were together right now!"
        ],
        [
         "4175",
         "0",
         "And pls pls drink plenty plenty water"
        ],
        [
         "4937",
         "0",
         "K..k.:)congratulation .."
        ],
        [
         "3986",
         "0",
         "Whatever, juliana. Do whatever you want."
        ],
        [
         "1859",
         "0",
         "Sir, i am waiting for your call."
        ],
        [
         "4495",
         "0",
         "Man this bus is so so so slow. I think you're gonna get there before me"
        ],
        [
         "5387",
         "0",
         "I will be gentle baby! Soon you will be taking all  &lt;#&gt;  inches deep inside your tight pussy..."
        ],
        [
         "1987",
         "0",
         "S..antha num corrct dane"
        ],
        [
         "4812",
         "0",
         "E admin building there? I might b slightly earlier... I'll call u when i'm reaching..."
        ],
        [
         "1906",
         "0",
         "There're some people by mu, I'm at the table by lambda"
        ],
        [
         "5124",
         "0",
         "He is impossible to argue with and he always treats me like his sub, like he never released me ... Which he did and I will remind him of that if necessary"
        ],
        [
         "1280",
         "0",
         "Waiting 4 my tv show 2 start lor... U leh still busy doing ur report?"
        ],
        [
         "3019",
         "0",
         "I didn't get the second half of that message"
        ],
        [
         "4617",
         "0",
         "Ü called dad oredi..."
        ],
        [
         "3434",
         "0",
         "Christmas is An occasion that is Celebrated as a Reflection of UR... Values..., Desires..., Affections...&amp; Traditions.... Have an ideal Christmas..."
        ],
        [
         "3052",
         "0",
         "Awesome question with a cute answer: Someone asked a boy \"how is ur life?\" . . He smiled &amp; answered: . . \"She is fine!\" Gudnite"
        ],
        [
         "2813",
         "0",
         "Say this slowly.? GOD,I LOVE YOU &amp; I NEED YOU,CLEAN MY HEART WITH YOUR BLOOD.Send this to Ten special people &amp; u c miracle tomorrow, do it,pls,pls do it..."
        ],
        [
         "2645",
         "0",
         "My friends use to call the same."
        ],
        [
         "774",
         "0",
         "I wil be there with in  &lt;#&gt;  minutes. Got any space"
        ],
        [
         "2904",
         "0",
         "Tell me pa. How is pain de."
        ],
        [
         "1102",
         "0",
         "Yeah go on then, bored and depressed sittin waitin for phone to ring... Hope the wind drops though, scary"
        ],
        [
         "3431",
         "0",
         "You've always been the brainy one."
        ],
        [
         "3234",
         "0",
         "Height of recycling: Read twice- People spend time for earning money and the same money is spent for spending time!;-) Good morning.. keep smiling:-)"
        ],
        [
         "5001",
         "0",
         "Well its not like you actually called someone a punto. That woulda been worse."
        ],
        [
         "4893",
         "0",
         "Miserable. They don't tell u that the side effects of birth control are massive gut wrenching cramps for the first 2 months. I didn't sleep at all last night."
        ],
        [
         "4850",
         "0",
         "either way works for me. I am  &lt;#&gt;  years old. Hope that doesnt bother you."
        ],
        [
         "411",
         "0",
         "Come by our room at some point so we can iron out the plan for this weekend"
        ],
        [
         "23",
         "0",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "1825",
         "0",
         "Sent me ur email id soon"
        ],
        [
         "4478",
         "0",
         "Oh :-)only 4 outside players allowed to play know"
        ],
        [
         "1883",
         "0",
         "Sorry, I can't help you on this."
        ],
        [
         "287",
         "0",
         "Ok.."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 1494
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww dat is sweet! We can think of something t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0</td>\n",
       "      <td>Just got to  &amp;lt;#&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>0</td>\n",
       "      <td>The word \"Checkmate\" in chess comes from the P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>0</td>\n",
       "      <td>This is wishing you a great day. Moji told me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank you. do you generally date the brothas?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>1</td>\n",
       "      <td>Want explicit SEX in 30 secs? Ring 02073162414...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>1</td>\n",
       "      <td>ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>1</td>\n",
       "      <td>REMINDER FROM O2: To get 2.50 pounds free call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                               Text\n",
       "4307      0  Awww dat is sweet! We can think of something t...\n",
       "4138      0                             Just got to  &lt;#&gt;\n",
       "4831      0  The word \"Checkmate\" in chess comes from the P...\n",
       "4461      0  This is wishing you a great day. Moji told me ...\n",
       "5440      0      Thank you. do you generally date the brothas?\n",
       "...     ...                                                ...\n",
       "5537      1  Want explicit SEX in 30 secs? Ring 02073162414...\n",
       "5540      1  ASKED 3MOBILE IF 0870 CHATLINES INCLU IN FREE ...\n",
       "5547      1  Had your contract mobile 11 Mnths? Latest Moto...\n",
       "5566      1  REMINDER FROM O2: To get 2.50 pounds free call...\n",
       "5567      1  This is the 2nd time we have tried 2 contact u...\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b468433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "\n",
    "    df = df.sample(\n",
    "        frac=1, random_state=123\n",
    "    ).reset_index(drop=True)                                    # Shuffles entire dataframe\n",
    "\n",
    "    train_end = int(len(df) * train_frac)                       # Calculates split indices\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)  # Test size is implied to be 0.2 as the remainder is 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "27afe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the seperated CSVs for later use. Training, Validation and Test sets.\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9478cdf",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "f8e20fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "02acf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * \n",
    "            (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd55d65",
   "metadata": {},
   "source": [
    "#### Instantiating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1e50733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length) # Prints longest sub-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "001d9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    " )\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f2563",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c710486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0     \n",
    "batch_size = 6\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3f733cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([6, 120])\n",
      "Label batch dimensions torch.Size([6])\n",
      "174 training batches\n",
      "25 validation batches\n",
      "50 test batches\n"
     ]
    }
   ],
   "source": [
    "# To test if the data loaders are working\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "# Dataset Size\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371345cb",
   "metadata": {},
   "source": [
    "### Preparing For Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "524ee7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,         \n",
    "    \"context_length\": 1024,      \n",
    "    \"drop_rate\": 0.0,            \n",
    "    \"qkv_bias\": True             \n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    " }\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "61024c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    " )\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "249436c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "78b690c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAs you'll see, the model struggles with following the instructions as we only pre-trained it \\nwithout instruction fine-tuning the model\\n\""
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our model can generate coherent text, let's see if it is capable of classifying spam already or not.\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "'''\n",
    "As you'll see, the model struggles with following the instructions as we only pre-trained it \n",
    "without instruction fine-tuning the model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e01f8",
   "metadata": {},
   "source": [
    "### Mapping the input layer into 2 final output layers (spam or not spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "daea8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "5171f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we freeze the model, meaning we make all the layers nontrainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6b804406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis new model.out_head layer will have it's 'requires_grad' attribute set to True by default.\\nTherefor, it is the only layer that'll be updated during training.\\n\""
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"], \n",
    "    out_features=num_classes\n",
    ")\n",
    "'''\n",
    "This new model.out_head layer will have it's 'requires_grad' attribute set to True by default.\n",
    "Therefor, it is the only layer that'll be updated during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8a8ed867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the final Normalization layer and transformer block trainable.\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0b3dae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test using the model\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "2c6e0c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f26e4bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "When determining a classification problem, we don't need to analyze every token, in fact, we just need to analyze\n",
    "the very last token since it contains information from all the past tokens. \n",
    "Therefore we look into the last row representing the output token.\n",
    "'''\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319f0fc",
   "metadata": {},
   "source": [
    "### Implement Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "8c982c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n",
      "Class label: 1\n",
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Considering the last token output and determine the class.\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())\n",
    "\n",
    "# Softmax isn't necessary since the higher value simply correlates to its specified meaning and therefore we can further simply our code into:\n",
    "\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30296dde",
   "metadata": {},
   "source": [
    "#### Calculating Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "83c89ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]    \n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += ((predicted_labels == target_batch).sum().item()\n",
    "                                    )\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b4d4dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 43.33%\n",
      "Validation accuracy: 46.67%\n",
      "Test accuracy: 53.33%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "602d056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "\n",
    "    logits = model(input_batch)[:, -1, :]   # Logits of the last output token\n",
    "    # targets_flat = target_batch.flatten()\n",
    "    loss = torch.nn.functional.cross_entropy(logits, targets_flat)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "45dc7ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)\n",
    "print(target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "12ac9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    model.to(device)\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:                                       \n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "171f9e94",
   "metadata": {
    "tags": [
     "function definition"
    ]
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[333]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():                \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m    train_loss = \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m   \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m val_loss = calc_loss_loader(val_loader, model, device, num_batches=\u001b[32m5\u001b[39m)\n\u001b[32m      7\u001b[39m test_loss = calc_loss_loader(test_loader, model, device, num_batches=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[332]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcalc_loss_loader\u001b[39m\u001b[34m(data_loader, model, device, num_batches)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, target_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < num_batches:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m         loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m         total_loss += loss.item()\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[321]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      7\u001b[39m logits = model(input_batch)[:, -\u001b[32m1\u001b[39m, :]   \u001b[38;5;66;03m# Logits of the last output token\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# targets_flat = target_batch.flatten()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m loss = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():                \n",
    "   train_loss = calc_loss_loader(\n",
    "   train_loader, model, device, num_batches=5\n",
    ")\n",
    "\n",
    "val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbad34f",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model to classify spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "b21ecb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  \n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()                     \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                         \n",
    "            optimizer.step()                         \n",
    "            examples_seen += input_batch.shape[0]   \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "            \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    \n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "72985244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "c0e06fbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[337]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m5e-5\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m      4\u001b[39m num_epochs = \u001b[32m5\u001b[39m\n\u001b[32m      5\u001b[39m train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mtrain_classifier_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m end_time = time.time()\n\u001b[32m     13\u001b[39m execution_time_minutes = (end_time - start_time) / \u001b[32m60\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[335]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_classifier_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     11\u001b[39m     optimizer.zero_grad()                     \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     loss.backward()                         \n\u001b[32m     16\u001b[39m     optimizer.step()                         \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[321]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      7\u001b[39m logits = model(input_batch)[:, -\u001b[32m1\u001b[39m, :]   \u001b[38;5;66;03m# Logits of the last output token\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# targets_flat = target_batch.flatten()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m loss = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50,\n",
    "        eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546a3e1",
   "metadata": {},
   "source": [
    "#### Plotting the Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "5086725e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'examples_seen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[338]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     plt.show()\n\u001b[32m     24\u001b[39m epochs_tensor = torch.linspace(\u001b[32m0\u001b[39m, num_epochs, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m examples_seen_tensor = torch.linspace(\u001b[32m0\u001b[39m, \u001b[43mexamples_seen\u001b[49m, \u001b[38;5;28mlen\u001b[39m(train_losses))\n\u001b[32m     26\u001b[39m plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n",
      "\u001b[31mNameError\u001b[39m: name 'examples_seen' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_values(\n",
    " epochs_seen, examples_seen, train_values, val_values,\n",
    " label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    \n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)   \n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    fig.tight_layout()            \n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec587b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Classification Accuracies\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(\n",
    "    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
    "    label=\"accuracy\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdceff",
   "metadata": {},
   "source": [
    "#### Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da695b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.89%\n",
      "Validation accuracy: 97.99%\n",
      "Test accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579035f",
   "metadata": {},
   "source": [
    "### Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    input_ids = input_ids[:min(             \n",
    "        max_length, supported_context_length\n",
    "    )]\n",
    "\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))   \n",
    "    input_tensor = torch.tensor(\n",
    "        input_ids, device=device\n",
    "    ).unsqueeze(0)             \n",
    "    \n",
    "    with torch.no_grad():                               \n",
    "        logits = model(input_tensor)[:, -1, :]    \n",
    "    \n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    " )\n",
    " \n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hi\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9d74c",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddcf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8  # Try reducing this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fced3a3",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message(message):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return classify_review(message, model, tokenizer, device, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    return classify_review(text, model, tokenizer, device, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs= gr.Textbox(lines=5, placeholder=\"Type or paste text here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Spam Classifier\",\n",
    "    description=\"Enter a message and the model will predict whether it's spam or not.\"\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b9ac0",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 2 - PERSONAL ASSISTANT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b3746",
   "metadata": {},
   "source": [
    "## Downloading the dataset needed\n",
    "Contains 1,100 instruction-response pairs, in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc24284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:                                               \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    " \n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    " \n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab8a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# Let's print one entry to see how each entry is structured\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd505ebb",
   "metadata": {},
   "source": [
    "### Prompt Formatting\n",
    "A crucial step in defining the structure of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589109ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes a dictionary entry as input and constructs a formatted string.\n",
    "'''\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2461d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "# Test on the data[50] entry.\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43325b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# Testing for empty input fields\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)                                   # Will completely skip over the input portion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0afca",
   "metadata": {},
   "source": [
    "### **Dividing the Dataset** - *Training, Testing, and Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)                           # 85% of the data is used for training\n",
    "test_portion = int(len(data) * 0.1)                             # 10% of the data is used for testing\n",
    "val_portion = len(data) - train_portion - test_portion          # 5% of the data is used for validating\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4230395",
   "metadata": {},
   "source": [
    "### Batching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        \n",
    "        self.encoded_texts = []\n",
    "        for entry in data:        \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e071036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02ee43",
   "metadata": {},
   "source": [
    "#### Creating a custom collate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc76a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates batches from a list of inputs\n",
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)  \n",
    "    inputs_lst = []\n",
    "    for item in batch:    \n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])   \n",
    "        inputs_lst.append(inputs)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)    \n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e080d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe 50256th token represents the '<|endoftext|>' token and will be used as a padding until the length of the longest element.\\n\""
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))\n",
    "\n",
    "'''\n",
    "The 50256th token represents the '<|endoftext|>' token and will be used as a padding until the length of the longest element.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7971a1",
   "metadata": {},
   "source": [
    "#### Create target token IDs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e802d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "We will implement the custom collate function again but with the addition of the ability\n",
    "to generate the target IDs from the input token IDs.\n",
    "'''\n",
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])    \n",
    "        targets = torch.tensor(padded[1:])   \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print()\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532224d2",
   "metadata": {},
   "source": [
    "#### Replace the padding tokens with placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will replace all but the first instance of the end-of-text (padding) token with -100.\n",
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (                              \n",
    "            new_item + [pad_token_id] *         \n",
    "            (batch_max_length - len(new_item))  \n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])     \n",
    "        targets = torch.tensor(padded[1:])    \n",
    "        \n",
    "        mask = targets == pad_token_id             \n",
    "        indices = torch.nonzero(mask).squeeze()    \n",
    "        if indices.numel() > 1:                    \n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]      \n",
    "            targets = targets[:allowed_max_length]    \n",
    "            \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Testing the Collate Function\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print()\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fe078",
   "metadata": {},
   "source": [
    "#### Calculating cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],    \n",
    "     [-0.5, 1.5]]     \n",
    ")\n",
    "\n",
    "targets_1 = torch.tensor([0, 1]) # Correct token indices to generate\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)\n",
    "\n",
    "# Adding another token ID and it's effect on the loss calculation:\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]     \n",
    ")\n",
    "\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e3a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62efc1b",
   "metadata": {},
   "source": [
    "### Creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e0031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.backends.mps.is_available():  Uncomment these 2 lines if using a MacOS.\n",
    "#     device = torch.device(\"mps\")\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b89086",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fab334",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0     \n",
    "batch_size = 6\n",
    "torch.manual_seed(123)\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b45ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939de5bb",
   "metadata": {},
   "source": [
    "### Loading pre-trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\355M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\355M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\355M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\355M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 16},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e7d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff452563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Model Response\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7892a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dcd78",
   "metadata": {},
   "source": [
    "### Instruction fine-tuning the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ad533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000002635999F170>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1e418",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[223]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     train_loss = \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     val_loss = calc_loss_loader(val_loader, model, device, num_batches=\u001b[32m5\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining loss:\u001b[39m\u001b[33m\"\u001b[39m, train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[221]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcalc_loss_loader\u001b[39m\u001b[34m(data_loader, model, device, num_batches)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, target_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < num_batches:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m         total_loss += loss.item()\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[219]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      5\u001b[39m logits = model(input_batch)[:, -\u001b[32m1\u001b[39m, :]   \u001b[38;5;66;03m# Logits of the last output token\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# targets_flat = target_batch.flatten()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m loss = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "# Calculating the training and validation loss before training\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafa06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d4ae3",
   "metadata": {},
   "source": [
    "### Extracting Responses & Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "for entry in test_data[:3]:     \n",
    "    input_text = format_input(entry)\n",
    "    \n",
    "    token_ids = generate(              \n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59047e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "   input_text = format_input(entry)\n",
    "   token_ids = generate(\n",
    "      model=model,\n",
    "      idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "      max_new_tokens=256,\n",
    "      context_size=BASE_CONFIG[\"context_length\"],\n",
    "      eos_id=50256\n",
    "    )\n",
    "   \n",
    "   generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "   response_text = (\n",
    "      generated_text[len(input_text):]\n",
    "      .replace(\"### Response:\", \"\")\n",
    "      .strip()\n",
    "    )\n",
    "   \n",
    "   test_data[i][\"model_response\"] = response_text\n",
    "   with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "      json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff0001",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[137]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_data\u001b[49m[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fab8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"     \n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
