{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee66e88",
   "metadata": {},
   "source": [
    "# **Building a Small Language Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5fe9d",
   "metadata": {},
   "source": [
    "# **Stage One - Building the LLM**\n",
    "This Stage Consists of 3 Parts:\n",
    "1. Data Preparation and Sampling\n",
    "2. Attention Mechanisms\n",
    "3. LLM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f70983",
   "metadata": {},
   "source": [
    "## Part 1 - *Data Preparation & Sampling*\n",
    "Formatting and preparing the data into the necessary form to be processed and understood by the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578b9cc",
   "metadata": {},
   "source": [
    "### All Imports Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ce0bf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "import urllib.request\n",
    "import zipfile \n",
    "import os\n",
    "from pathlib import Path\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee58bf5",
   "metadata": {},
   "source": [
    "### Importing the Dataset Example\n",
    "In our case, we will be using the pdf form of the book \"The Verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "006a75eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x19867deb110>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3146f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b5ceb",
   "metadata": {},
   "source": [
    "### Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a534b0",
   "metadata": {},
   "source": [
    "#### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "43373940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ']\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to create tokens.\n",
    "# We want to filter out whiite spaces and special characters.\n",
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(preprocessed[:50])\n",
    "\n",
    "# Will filter out any whitespaces and only return the characters.\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b258f4",
   "metadata": {},
   "source": [
    "#### Step 2: Determining the Vocabulary and mapping them to their token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ad18907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Creating our Vocabulary of unique tokens (Alphabetically Arranged)\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# Print first 50 vocab elements\n",
    "for i, item in enumerate(vocab.items()):\n",
    "  print(item)\n",
    "  if i >= 50:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "61ff2239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3d6af922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleTokenizerV1 - A simple tokenizer that can perform encoding and decoding.\n",
    "\n",
    "SimpleTokenizerV2 - Replaces Uknown words with the special character <|unk|> and\n",
    "unrelated pieces of texts with <|endoftext|>\n",
    "\n",
    "\"\"\"\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "      self.str_to_int = vocab\n",
    "      self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "      preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "      preprocessed = [\n",
    "      item.strip() for item in preprocessed if item.strip()\n",
    "      ]\n",
    "      ids = [self.str_to_int[s] for s in preprocessed]\n",
    "      return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "      text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3d5986e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [\n",
    "    item.strip() for item in preprocessed if item.strip()\n",
    "    ]\n",
    "\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                else \"<|unk|>\" for item in preprocessed]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc647e92",
   "metadata": {},
   "source": [
    "##### Create Byte-Pair Encoder\n",
    "- Is a subword tokenization algorithm. The most common pair of consecutive bytes of data is replaced with a byte that does not occur in data.\n",
    "\n",
    "Advantages:\n",
    "- Byte-pair encoding can reduce the size of the vocabulary significantly.\n",
    "- The BPE tokenizer can handle any unknown word without needing the `<|unk|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ab7a058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a498ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text)) # Prints the new number of tokens using the GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "63be29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "z:            [2241, 287, 257, 4489]\n",
      "------------------------------------\n",
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To better visualize what's being done\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "z = enc_sample[2:context_size+2]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "print(f\"z:            {z}\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Representation: left side = input, right side = target\n",
    "for i in range(1, context_size+1):\n",
    "  context = enc_sample[:i]\n",
    "  desired = enc_sample[i]\n",
    "  print(context, \"---->\", desired)\n",
    "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired])) # Text equivalent\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "89f21822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset Implementation\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # Tokenizes the entire text\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "\n",
    "    # Uses a sliding window approach to chunk the book into overlapping sequences.\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Dataloader Implementation\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                 stride=128, shuffle=True, drop_last=True,\n",
    "                 num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiates the gpt2 tokenizer\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  # Initialize the Dataset class created earlier\n",
    "\n",
    "      # Intantiates and provides parameters for the DataLoader python class provided by PyTorch.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "549478f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619],\n",
      "        [  402,   271, 10899,  2138,   257,  7026]]), tensor([[  367,  2885,  1464,  1807,  3619,   402],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632]])]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    dataloader = create_dataloader_v1(\n",
    "      raw_text, batch_size=2, max_length=6, stride=6, shuffle=False)\n",
    "    data_iter = iter(dataloader)  # Creates an Iterator\n",
    "    first_batch = next(data_iter) # Gets the next batch from the data Iterator, the first_batch will be assigned a tuple of tensors.\n",
    "    print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b5d30748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[15632,   438,  2016,   257,   922,  5891],\n",
      "        [ 1576,   438,   568,   340,   373,   645]]), tensor([[ 438, 2016,  257,  922, 5891, 1576],\n",
      "        [ 438,  568,  340,  373,  645, 1049]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a32d20",
   "metadata": {},
   "source": [
    "#### Step 3: Mapping Token Embeddings --- COME BACK TO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993f557",
   "metadata": {},
   "source": [
    "##### Import Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "094c5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install gensim\n",
    "# import gensim.downloader as api\n",
    "# model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fe982",
   "metadata": {},
   "source": [
    "##### Positional Embedding Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7a43a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the inputs that we'll be using, with their embeddings, given that each word has 3 dimensions. \n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c93ee5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 256])\n",
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Embedding function only requires the vocab_size\n",
    "as it'll provide random embeddings originally\n",
    "and those embeddings will be adjusted over training.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs.long())\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Instantiating the data loader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "   stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "836b290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b5dc735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! [[TODO]] SIZE INCOMPATABILITY BETWEEN TOKEN AND POSITIONAL EMBEDDINGS\n",
    "# input_embeddings = token_embeddings + pos_embeddings\n",
    "# print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24bc33",
   "metadata": {},
   "source": [
    "## Part 2 - *ATTENTION MECHANISM*\n",
    "There are 4 kinds of attention mechanisms and we will be implementing each one until we arrive to the original </br>\n",
    "transformer's **\"Multi-Head Self-Attention\"** mechanism, building on top of the previous implementation and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fabb8",
   "metadata": {},
   "source": [
    "### First Implementation: **Simplified Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a22ef6",
   "metadata": {},
   "source": [
    "#### Compute Attention Scores - 2nd input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fee95f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2956967., 16597455., 29439276., 30712224., 23737832.,  2581577.,\n",
      "        23769278., 12535636.])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # Takes 'journey' as the query.\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "  attention_scores_2[i] = torch.dot(x_i, query) \n",
    "\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002ab0",
   "metadata": {},
   "source": [
    "#### Compute Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "88a70b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.0208, 0.1166, 0.2068, 0.2158, 0.1668, 0.0181, 0.1670, 0.0881])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attention_weights_2_tmp)\n",
    "print(\"Sum: \", attention_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ccb429bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Sum: tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following is a naive Softmax Implementation. However, this implementation has weaknesses when dealing with very small or large values.\n",
    "Therefore it is recommended to simply use PyTorch's implmentation of Softmax.\n",
    "''' \n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attention_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2a347d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Softmax Function Implementation, results in same value of previous softmax function.\n",
    "attn_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1446cd",
   "metadata": {},
   "source": [
    "#### Compute Context Vectors\n",
    "The context vector z is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "becf5848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6203.2407, 3242.1980,  940.7390, 1740.2675])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attention_weights_2_tmp[i]*x_i #x_i is simply the input vector, multiplying with its corresponding attention weights.\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3122ce",
   "metadata": {},
   "source": [
    "#### A Generalised Approach\n",
    "So far, for the attention mechanism built above, we have implemented attention with respect to the embeddings vector of the second input. Now we will create a generalised method that can be applied to all embeddings vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa276d",
   "metadata": {},
   "source": [
    "#### Generalized Process\n",
    "We perform the same 3 steps performed prior, with a few modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c3f5b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ddcacc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n!\\nThe issue with the above implementation is that cause of the nested for loop\\nit is slow and computationally expensive, a better approach can be done with linear algebra.\\n'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, j_i in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, j_i)\n",
    "\n",
    "print(attention_scores) # Unnormalized output\n",
    "\n",
    "\"\"\" \n",
    "!\n",
    "The issue with the above implementation is that cause of the nested for loop\n",
    "it is slow and computationally expensive, a better approach can be done with linear algebra.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9700d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following method is more efficient, not because the time complexity is any less,\n",
    "but the implementation time is far less, the time it takes to perform each multiplication, the '@' symbol represents matrix multiplication.\n",
    "You get the exact same answer as the previous method, but much faster.\n",
    "\"\"\"\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83715b4",
   "metadata": {},
   "source": [
    "#### Calculating the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d530f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "----------------------------------------------------------------\n",
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Declaring the dimensions to be -1 will normally apply the normalization to the last dimension, however, for a 2 dimensional tensor,\n",
    "the dim = -1 will apply the Softmax into all columns.\n",
    "'''\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) #\n",
    "print(attention_weights)\n",
    "\n",
    "# To verify that all rows sum up to 1\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attention_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641172",
   "metadata": {},
   "source": [
    "#### Computing the Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "200027bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Simply a matrix multiplication is performed. Concluding the generalised method of calculating context vectors.\n",
    "all_context_vec = attention_weights @ inputs\n",
    "print(all_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5497f",
   "metadata": {},
   "source": [
    "### Second Implementation: **Self-Attention**\n",
    "This self-attention mechanism is also called *scaled dot-product attention*. \n",
    "<br>We will be building on top of our last model, this time introducing **Trainable Weight Matrices**. This can help the model to produce better context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a933d6",
   "metadata": {},
   "source": [
    "For illustration purposes, we will be computing for only one context vector, *z(2)*.\n",
    "- Note that in GPT-like models, the input and output dimensions are usually the same,\n",
    " but to better follow the computation, weâ€™ll use different input (d_in=3) and output\n",
    " (d_out=2) dimensions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a9e377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables dealing with the second input\n",
    "x_2 = inputs[1]    \n",
    "d_in = inputs.shape[1]     \n",
    "d_out = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abb67c",
   "metadata": {},
   "source": [
    " Initialize the query, key, and value matrices of the second input \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "ef594936",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "52e655dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7b100eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac2bf",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Input 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "aa4d33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]            \n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d2d81",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "26b54dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2) # Notice the attention score matches the previously calculated attention score for the second input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e1344",
   "metadata": {},
   "source": [
    "#### Calculate the Attention Weights of the entire Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "cdbcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247b93b",
   "metadata": {},
   "source": [
    "#### Calculate the Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d624f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0915d",
   "metadata": {},
   "source": [
    "#### Implementing a compact self-attention Python class\n",
    "* In practice, with the LLM implementation that'll be performed later in mind, it is helpful to organize this code into a Python class, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b4abb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attention_scores = queries @ keys.T # omega\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "3819522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "11b141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention Class version 2\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c55b9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\\n they use different initial weights for the weight matrices since nn.Linear uses a more\\n sophisticated weight initialization scheme.\\n'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "'''\n",
    " Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    " they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    " sophisticated weight initialization scheme.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de148abf",
   "metadata": {},
   "source": [
    "### Third Implementation: **Causal Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ddc795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)    \n",
    "keys = sa_v2.W_key(inputs) \n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56313a",
   "metadata": {},
   "source": [
    "#### Step 1: Initialise the masked matrix with the \"*tril*\" function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "7526eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c81c",
   "metadata": {},
   "source": [
    "#### Step 2: Multiply the attention weights matrix with the masked matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "55398fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attention_weights * mask_simple\n",
    "print(masked_simple) # prints non-normalised values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586bc4",
   "metadata": {},
   "source": [
    "#### Step 3: Renormalize each row to sum up to 1 using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "4e128baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd130fef",
   "metadata": {},
   "source": [
    "#### Second More Efficient Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "458ff6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ffa990d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "84c6f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995c95",
   "metadata": {},
   "source": [
    "#### Using *Dropout*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a8f89d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)   \n",
    "example = torch.ones(6, 6)     \n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e7b47d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying dropout to the attention weights\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f172bc",
   "metadata": {},
   "source": [
    "#### Implementing a Causal Attention Class\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented <br>\n",
    "earlier, except that we added the dropout and causal mask components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d6696b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)           \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                  \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2)   \n",
    "        attention_scores.masked_fill_(                   \n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "015435da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0f197cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa70423",
   "metadata": {},
   "source": [
    "### Final Implementation: **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8a206a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "        [CausalAttention(\n",
    "         d_in, d_out, context_length, dropout, qkv_bias\n",
    "     ) \n",
    "     for _ in range(num_heads)]\n",
    " )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "39937465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    "    )\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35d32",
   "metadata": {},
   "source": [
    "#### Efficient Multi-Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "f1d5e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "         context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                        diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)   \n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim    \n",
    "        )\n",
    "\n",
    "        keys = keys.transpose(1, 2)   \n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] \n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec)   \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688d0e3",
   "metadata": {},
   "source": [
    "## PART 3 - *LLM ARCHITECTURE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "16afc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e852edb",
   "metadata": {},
   "source": [
    "### Placeholder GPT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f88db552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])      # Creates the 768 dimensional word embedding.\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])  # Creates the positional embeddings.\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])                        # Creates a random dropout embedding of 10% of the total embeddings. \n",
    "        self.trf_blocks = nn.Sequential(                                    # Creates an \"n_layers\" amount of transformer blocks.\n",
    "            *[DummyTransformerBlock(cfg)              \n",
    "            for _ in range(cfg[\"n_layers\"])]        \n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                    # Final normalization layer\n",
    "        self.out_head = nn.Linear(                                          # Projects the final hidden states to vocabulary size to get the logits for predicting the next token.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):                                              # in_idx param is a tensor of shape (batch_size, seq_len) containing token indices.\n",
    "        batch_size, seq_len = in_idx.shape                                  # Splitting the in_idx tensor into 2 seperate vectors.\n",
    "        tok_embeds = self.tok_emb(in_idx)                                   # Converts input tokens into vectors.\n",
    "        pos_embeds = self.pos_emb(                                          # Creates the (absolute) position vectors from the sequence length of each input.\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds                                         # Adds the token embeddings with the (absolute) positional embeddings, resulting into our prepared input.\n",
    "        x = self.drop_emb(x)                                                # We drop the randomly chosen matrix values (dropout regularization).\n",
    "        x = self.trf_blocks(x)                                              # Runs the input through each transformer block in sequence.\n",
    "        x = self.final_norm(x)                                              # Final normalization\n",
    "        logits = self.out_head(x)                                           # The output logits will contain the probability values needed for prediction.\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):                                     # Simple placeholder class that'll be replaced by a real Transformer Block later.\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):    \n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):                                            # Simple placeholder class that'll be replaced by a real LayerNorm later.\n",
    "    def __init__(self, normalized_shape, eps=1e-5):   \n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "ccb7f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")           # Instantiate the GPT-2 tokenizer.\n",
    "batch = []                                          # Initialize an empty list\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))  # First encodes the text using GPT-2 tokenizer -> converted into a tensor -> is appended into the batch list.\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)                   # Takes in a list of tensors and \"stacks\" (concatenates) them into a 2D tensor.\n",
    "print(batch)                                        # Print the resulting 2D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "26e03364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe output tensor has two rows corresponding to the two text samples. \\nEach text sample consists of four tokens.\\n'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)                      # Passes the previously defined batch into our DummyGPTModel.\n",
    "print(\"Output shape:\", logits.shape)       # The standard shape distribution: (batch_size, seq_len, vocab_size).\n",
    "print(logits)                              # Outputs the 2 batches, each containing 4 sequences(inputs) and their embeddings relative to a vocab of 50257.\n",
    "\n",
    "\"\"\"\n",
    "The output tensor has two rows corresponding to the two text samples. \n",
    "Each text sample consists of four tokens.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "abdd6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)                   # Creates 2 batches of random numbers, each containing 5 values. \n",
    "layer = nn.Sequential(                              # Constructs a sequential neural network module.\n",
    "    nn.Linear(5, 6),                                # Creates a fully connected linear layer, taking in 5 input vectors and outputing 6.\n",
    "    nn.ReLU()                                       # Applies nonlinear activation function.\n",
    ")       \n",
    "out = layer(batch_example)                          # Feeds the nn layer with the batch example\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b5adaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)   # Keepdim ensures the dimensions stay the same, and doesn't combine the dimensions of the first and second input together.\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "347ca1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)          # Disables Scientific Notation \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78df5c",
   "metadata": {},
   "source": [
    "#### GELU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "efc64996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x  * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0cbdf",
   "metadata": {},
   "source": [
    "#### Layer Normalization Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "88b6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8adcd",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "74bc04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(                            # Simply an implemtation of a 3 layered forward neural network.\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),      # This linear portion \"expands\" the input embedding dimension into the number of nodes of the following hidden layer, in our case it'll 4 times the amount of the emb dim.\n",
    "            GELU(),                                             # Calls the GELU function on the nodes.\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])       # Does the opposite of the first linear layer, \"compressing\" from 4 times the size back to the original size of the input/\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8ec4a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)  # Intance\n",
    "x = torch.rand(2, 3, 768)           # 2 batches, each batch has 3 tokens, and each token will have an embedding size of 768.\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037276cc",
   "metadata": {},
   "source": [
    "### Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "9bd77bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([                                   # Implement 5 layers.\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \n",
    "                  GELU())\n",
    " ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)        \n",
    "            if self.use_shortcut and x.shape == layer_output.shape:   \n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a4f39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)                           \n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "37e56d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)               # Forward Pass.\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)     # Calculates loss based on how close the target and outputs are.\n",
    "    \n",
    "    loss.backward()                 # Backward pass to calculate the gradients.\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "08db27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8432b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    " )\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a08917f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  \n",
    "   \n",
    "        shortcut = x        \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5812e",
   "metadata": {},
   "source": [
    "#### Instantiating a Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d3e4e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                  \n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be38b3",
   "metadata": {},
   "source": [
    "### GPT FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "550129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):        #initializes the token and positional embedding layers using the configurations passed in via cfg.\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(                # Creates transformer blocks equal to that in specified in cfg.\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]    \n",
    "            )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])     # Layer normalization is applied.\n",
    "        self.out_head = nn.Linear(  # linear output head without bias is defined, which projects the transformerâ€™s output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f76141",
   "metadata": {},
   "source": [
    "#### Instantiating our GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "90a92ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)     # Shape: [2, 4, 50257]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2cba46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5e328bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f434dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    " )\n",
    "\n",
    "print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e422a",
   "metadata": {},
   "source": [
    "#### Calculate the total size needed for the 163 million parameters in our GPTModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "673d4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4      \n",
    "total_size_mb = total_size_bytes / (1024 * 1024)    \n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "208052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,                \n",
    "                 max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]   \n",
    "        with torch.no_grad():\n",
    "           logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]                   \n",
    "        probas = torch.softmax(logits, dim=-1)          \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7f44fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "baa9627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()                 \n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2b5236fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d9c1e",
   "metadata": {},
   "source": [
    "# **Stage Two - Foundational Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b901e7",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42bce8",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0e399602",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,   #! Context length dropped from 1024 -> 256.\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "fa5be60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)                 # Unsqueeze adds a batch dimension. Shape becomes [1, seq_length]\n",
    "    return encoded_tensor\n",
    " \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)                 # Removes batch dimension, resulting in a 1D tensor.\n",
    "    return tokenizer.decode(flat.tolist())      # First the tensor is converted to a list of integers and then decoded to human readable text.\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(               # Calls and passes parameter values into the generate_text_simple function.\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),    # Is the starting prompt.\n",
    "    max_new_tokens=10,                                  # Specifies that 10 new tokens should be generated after the first prompt\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]      # Determines the maximum token size to be considered at once.\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))       # Calls token ids to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "16937771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs.long())\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "7533de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "28125199",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "               [        40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e9b34",
   "metadata": {},
   "source": [
    "#### Calculating Logits and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "3eea9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "937701dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)  # Since we have 2 batches, each containing 3 tokens, we received the highest probability value for each token in each batch.\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "75bd1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957afd64",
   "metadata": {},
   "source": [
    "### Text Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f199b",
   "metadata": {},
   "source": [
    "#### Calculating Target Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "57543b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0000,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Printing the initial Softmax probability scores\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409c271",
   "metadata": {},
   "source": [
    "#### Calculating the Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "aadc810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20d5f",
   "metadata": {},
   "source": [
    "#### Average Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2cc451d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8765)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed6e58",
   "metadata": {},
   "source": [
    "#### Negative Average Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "99a093be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1   # Simply multiply the average by -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbfa57",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "57be226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# It's important to keep track of the dimensions in order to perform cross entropy.\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e83bc9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "For the cross_entropy loss function in PyTorch, we want to flatten these tensors\n",
    "by combining them over the batch dimension:\n",
    "'''\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c86fce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b42c4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52918.7773)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Perplexity\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22782",
   "metadata": {},
   "source": [
    "### Training & Validation Losses\n",
    "We will use the text file of \"the verdict\" once more to demonstrate training our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e4ccaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "31df5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)  # We will only be working with 5145 tokens for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56f834",
   "metadata": {},
   "source": [
    "#### Train and Testing Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "7901e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bca0de",
   "metadata": {},
   "source": [
    "#### Calling the Dataloader from Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "ad18c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader is called for both training and validation sets.\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    " )\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4a1006d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Iterating through both data loaders\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")   \n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e11a",
   "metadata": {},
   "source": [
    "#### Cross entropy lost among Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6ef8e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)        \n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d37b8",
   "metadata": {},
   "source": [
    "#### Cross entropy lost over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7b45c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))  #  Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()   # Sums the loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches     # Averages the loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "9352e13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training loss: 10.98850186665853\n",
      "Validation loss: 10.990342140197754\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)  \n",
    "with torch.no_grad():                                       \n",
    "    train_loss = calc_loss_loader(train_loader, model, device)   \n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cd656",
   "metadata": {},
   "source": [
    "### The main function for pretraining LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "51f47fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model and generate_and_print_sample functions are not defined yet. \n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "               optimizer, device, num_epochs,\n",
    "               eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []   \n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                    \n",
    "            optimizer.step()                   \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:   \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(                     \n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b2de5",
   "metadata": {},
   "source": [
    "#### Evaluate model function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2389b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()            # Dropout is disabled \n",
    "    with torch.no_grad():   # Disables gradient tracking\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3693a8",
   "metadata": {},
   "source": [
    "#### Generate and print sample function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4d946563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1508a",
   "metadata": {},
   "source": [
    "#### Training a GPTModel for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f48aca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.823, Val loss 9.932\n",
      "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.336\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.621, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.043, Val loss 6.599\n",
      "Every effort moves you, and,, and,, and,,,, and, and,,,,,,,,, and,,,, the,,,, and,, and,,, the, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.547, Val loss 6.485\n",
      "Ep 3 (Step 000025): Train loss 5.450, Val loss 6.397\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.982, Val loss 6.301\n",
      "Ep 4 (Step 000035): Train loss 4.755, Val loss 6.296\n",
      "Every effort moves you, and I had been the of the picture to the picture.                                     \n",
      "Ep 5 (Step 000040): Train loss 4.162, Val loss 6.182\n",
      "Every effort moves you know the                                                \n",
      "Ep 6 (Step 000045): Train loss 3.742, Val loss 6.175\n",
      "Ep 6 (Step 000050): Train loss 3.213, Val loss 6.189\n",
      "Every effort moves you know the fact, and I felt--I had the fact a little of a little to my work, and in fact, and in the picture.      \"--and it, and, and down, and he was his\n",
      "Ep 7 (Step 000055): Train loss 3.143, Val loss 6.172\n",
      "Ep 7 (Step 000060): Train loss 2.422, Val loss 6.136\n",
      "Every effort moves you know the picture.  I glanced after him, and I was one of the house.\"   \"I didn't you know. I was his pictures.  \"--and I was a little a little the room, I was\n",
      "Ep 8 (Step 000065): Train loss 1.963, Val loss 6.178\n",
      "Ep 8 (Step 000070): Train loss 1.639, Val loss 6.230\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"Once, when I looked up, I had been. Gisburn--as Jack himself, as once one had been the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.286, Val loss 6.227\n",
      "Ep 9 (Step 000080): Train loss 0.987, Val loss 6.249\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs. \"--and I was a year after Jack's resolve had been his eyes; then I looked at the donkey--and I saw that, and down the room, I had\n",
      "Ep 10 (Step 000085): Train loss 0.727, Val loss 6.356\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),          \n",
    "    lr=0.0004, weight_decay=0.1\n",
    " )\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448655fb",
   "metadata": {},
   "source": [
    "#### Visualizing the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "185b08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    \n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                  \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)) \n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f63439",
   "metadata": {},
   "source": [
    "#### 5. Text Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ea21979b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5ffc6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a69b8",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "67b38a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the next-token generation process, let's use a small vocab.\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a5ba265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    " )\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5a6c4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# To implement a probabilistic sampling process, we can now replace argmax with the multinomial function in PyTorch.\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "\"\"\"\n",
    "As a result, \"forward\" will still be printed as it is still the most likely next word. \n",
    "However, this time, it will not be the case ALL the time. Let's test this out.\n",
    "\"\"\"\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    \n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa329c4",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "9b06e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e180614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the diversity of outputs depending on the temperature value.\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "        for T in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "           bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4d9fe",
   "metadata": {},
   "source": [
    "#### Top-K Sampling\n",
    "Selects the top 'k' probability values of the logits then sets all other values to `-inf` where the softmax function is applied and the remaining probabilities sum up to 1 and the `-inf` values become equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "872276f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Applying in code: Step 1 - Select the top 3 highest probability logits (k being 3 in this example)\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "\n",
    "# Step 2 - Assigning -inf for all non-selected logits.\n",
    "new_logits = torch.where(                               \n",
    "    condition=next_token_logits < top_logits[-1],       # Identifies logits less than the minimum in the top 3\n",
    "    input=torch.tensor(float('-inf')),                  # Assigns â€“inf to these lower logits\n",
    "    other=next_token_logits                             #  Retains the original logits for all other tokens\n",
    ")\n",
    "print(new_logits)\n",
    "\n",
    "# Step 3 - Applying the Softmax function\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1a394983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "     temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):           # The for loop is the same as before: gets logits and only focuses on the last time step\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:               # Filters logits with top_k sampling\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        if temperature > 0.0:               #  Applies temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:                               #  Carries out greedy next token selection as before when temperature scaling is disabled\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:             #  Stops generating early if end-of-sequence token is encountered\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "7da3274e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you not,\" she down surprise. You. GisitelyI quote by by\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=55,\n",
    "    temperature=1.6\n",
    "    )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0b149",
   "metadata": {},
   "source": [
    "### Loading and Saving Model Weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3058998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "be25a395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()    # switches the model to evaluation mode for inference, disabling the dropout layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "960a616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({    # Using torch.save, we can save both the model and optimizer state_dict contents\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c8a8b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3477371f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x199ae16d550>)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download GPT2 Weights\n",
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    "    )\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "dcb1ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8ee84465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f946afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "91d1bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5c0e0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting which model variant to load\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "# Since we used a 256 token length earlier, meanwhile, the original model uses a token length of 1,024 we must update the model accordingly\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "\n",
    "# Bias vectors are no longer in use, however they we're for creating GPT-2\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "4f93bfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating new gpt instance\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "9c0bbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                          \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "c5b59e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the function that'll load the weights into our gpt instance\n",
    "def load_weights_into_gpt(gpt, params):       #  Sets the modelâ€™s positional and token embedding weights to those specified in params\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):    # Iterates over each transformer block\n",
    "        q_w, k_w, v_w = np.split(             #  The np.split function is used to divide the attention and bias weights into three equal parts for the query, key, and value components.\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    " \n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "        \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "eaebf544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the weights\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "c8edeb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    " )\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92621977",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 1 - CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacbf93",
   "metadata": {},
   "source": [
    "## Mobile Spam Text Classifier\n",
    "We will use a dataset to help us classify whether a text is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1673c7",
   "metadata": {},
   "source": [
    "### Importing Libraries and Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "507d3831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'sms_spam_collection\\\\SMSSpamCollection' -> 'sms_spam_collection\\\\SMSSpamCollection.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[424]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m         os.rename(original_file_path, data_file_path)              \n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile downloaded and saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mdownload_and_unzip_spam_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[424]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mdownload_and_unzip_spam_data\u001b[39m\u001b[34m(url, zip_path, extracted_path, data_file_path)\u001b[39m\n\u001b[32m     17\u001b[39m     zip_ref.extractall(extracted_path)\n\u001b[32m     19\u001b[39m original_file_path = Path(extracted_path) / \u001b[33m\"\u001b[39m\u001b[33mSMSSpamCollection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m              \n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile downloaded and saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: 'sms_spam_collection\\\\SMSSpamCollection' -> 'sms_spam_collection\\\\SMSSpamCollection.tsv'"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\" \n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(\n",
    "    url, zip_path, extracted_path, data_file_path):\n",
    "        if data_file_path.exists():\n",
    "            print(f\"{data_file_path} already exists. Skipping download \"\n",
    "                                    \"and extraction.\"\n",
    "            )\n",
    "        \n",
    "        with urllib.request.urlopen(url) as response:   \n",
    "            with open(zip_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:   \n",
    "            zip_ref.extractall(extracted_path)\n",
    "\n",
    "        original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "        os.rename(original_file_path, data_file_path)              \n",
    "        print(f\"File downloaded and saved as {data_file_path}\")\n",
    "    \n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "b6288536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3517e1df-3d5b-45f4-83cb-56403d3e0a61",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thatÂ’s the way u feel. ThatÂ’s the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "Iâ€˜m going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So Ã¼ pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged Â£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg Ã¼ again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will Ã¼ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
    " )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e6b691b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "55e64fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]    \n",
    "\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
    "        num_spam, random_state=123\n",
    "    )                                        \n",
    "\n",
    "    balanced_df = pd.concat([\n",
    "        ham_subset, df[df[\"Label\"] == \"spam\"]\n",
    "    ])                              \n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "5b124a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping ham to 1, and spam to 0\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "b468433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "\n",
    "    df = df.sample(\n",
    "        frac=1, random_state=123\n",
    "    ).reset_index(drop=True)                                    # Shuffles entire dataframe\n",
    "\n",
    "    train_end = int(len(df) * train_frac)                       # Calculates split indices\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)  # Test size is implied to be 0.2 as the remainder is 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "27afe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the seperated CSVs for later use. Training, Validation and Test sets.\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9478cdf",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "02acf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * \n",
    "            (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd55d65",
   "metadata": {},
   "source": [
    "#### Instantiating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "1e50733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length) # Prints longest sub-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "001d9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    " )\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f2563",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "c710486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0     \n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "3f733cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n",
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "# To test if the data loaders are working\n",
    "\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "# Dataset Size\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371345cb",
   "metadata": {},
   "source": [
    "### Preparing For Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "524ee7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,         \n",
    "    \"context_length\": 1024,      \n",
    "    \"drop_rate\": 0.0,            \n",
    "    \"qkv_bias\": True             \n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    " }\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "61024c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    " )\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "249436c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "78b690c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAs you'll see, the model struggles with following the instructions as we only pre-trained it \\nwithout instruction fine-tuning the model\\n\""
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our model can generate coherent text, let's see if it is capable of classifying spam already or not.\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "'''\n",
    "As you'll see, the model struggles with following the instructions as we only pre-trained it \n",
    "without instruction fine-tuning the model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e01f8",
   "metadata": {},
   "source": [
    "### Mapping the input layer into 2 final output layers (spam or not spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "daea8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "5171f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we freeze the model, meaning we make all the layers nontrainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "6b804406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis new model.out_head layer will have it's 'requires_grad' attribute set to True by default.\\nTherefor, it is the only layer that'll be updated during training.\\n\""
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"], \n",
    "    out_features=num_classes\n",
    ")\n",
    "'''\n",
    "This new model.out_head layer will have it's 'requires_grad' attribute set to True by default.\n",
    "Therefor, it is the only layer that'll be updated during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "8a8ed867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the final Normalization layer and transformer block trainable.\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "0b3dae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test using the model\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "2c6e0c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "f26e4bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "When determining a classification problem, we don't need to analyze every token, in fact, we just need to analyze\n",
    "the very last token since it contains information from all the past tokens. \n",
    "Therefore we look into the last row representing the output token.\n",
    "'''\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319f0fc",
   "metadata": {},
   "source": [
    "### Implement Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "8c982c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n",
      "Class label: 1\n",
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Considering the last token output and determine the class.\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())\n",
    "\n",
    "# Softmax isn't necessary since the higher value simply correlates to its specified meaning and therefore we can further simply our code into:\n",
    "\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30296dde",
   "metadata": {},
   "source": [
    "#### Calculating Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "83c89ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]    \n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += ((predicted_labels == target_batch).sum().item()\n",
    "                                    )\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "b4d4dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "602d056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]   # Logits of the last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "12ac9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:                                       \n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "171f9e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():                \n",
    "   train_loss = calc_loss_loader(\n",
    "   train_loader, model, device, num_batches=5\n",
    ")\n",
    "val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbad34f",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model to classify spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "b21ecb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  \n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()                     \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                         \n",
    "            optimizer.step()                         \n",
    "            examples_seen += input_batch.shape[0]   \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "            \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    \n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "72985244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "c0e06fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[455]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=\u001b[32m5e-5\u001b[39m, weight_decay=\u001b[32m0.1\u001b[39m)\n\u001b[32m      4\u001b[39m num_epochs = \u001b[32m5\u001b[39m\n\u001b[32m      5\u001b[39m train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mtrain_classifier_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m end_time = time.time()\n\u001b[32m     13\u001b[39m execution_time_minutes = (end_time - start_time) / \u001b[32m60\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[453]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_classifier_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     11\u001b[39m     optimizer.zero_grad()                     \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     loss.backward()                         \n\u001b[32m     16\u001b[39m     optimizer.step()                         \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[450]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     input_batch = \u001b[43minput_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     target_batch = target_batch.to(device)\n\u001b[32m      4\u001b[39m     logits = model(input_batch)[:, -\u001b[32m1\u001b[39m, :]   \u001b[38;5;66;03m# Logits of the last output token\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50,\n",
    "        eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546a3e1",
   "metadata": {},
   "source": [
    "#### Plotting the Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "5086725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_values(\n",
    " epochs_seen, examples_seen, train_values, val_values,\n",
    " label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    \n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)   \n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    fig.tight_layout()            \n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "aec587b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Classification Accuracies\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(\n",
    "    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
    "    label=\"accuracy\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdceff",
   "metadata": {},
   "source": [
    "#### Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da695b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[420]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_accuracy = \u001b[43mcalc_accuracy_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m val_accuracy = calc_accuracy_loader(val_loader, model, device)\n\u001b[32m      3\u001b[39m test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[387]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcalc_accuracy_loader\u001b[39m\u001b[34m(data_loader, model, device, num_batches)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_accuracy_loader\u001b[39m(data_loader, model, device, num_batches=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m()\n\u001b[32m      3\u001b[39m     correct_predictions, num_examples = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m num_batches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579035f",
   "metadata": {},
   "source": [
    "### Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text)         \n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    input_ids = input_ids[:min(             \n",
    "        max_length, supported_context_length\n",
    "    )]\n",
    "\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))   \n",
    "    input_tensor = torch.tensor(\n",
    "        input_ids, device=device\n",
    "    ).unsqueeze(0)             \n",
    "    \n",
    "    with torch.no_grad():                               \n",
    "        logits = model(input_tensor)[:, -1, :]    \n",
    "    \n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd8433",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[423]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m text_1 = (\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou are a winner you have been specially\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m selected to receive $1000 cash or a $2000 award.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m  )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassify_review\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[421]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mclassify_review\u001b[39m\u001b[34m(text, model, tokenizer, device, max_length, pad_token_id)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclassify_review\u001b[39m(text, model, tokenizer, device, max_length=\u001b[38;5;28;01mNone\u001b[39;00m, pad_token_id=\u001b[32m50256\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m()\n\u001b[32m      3\u001b[39m     input_ids = tokenizer.encode(text)         \n\u001b[32m      4\u001b[39m     supported_context_length = model.pos_emb.weight.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    " )\n",
    " \n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9d74c",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fced3a3",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e68b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[419]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m      3\u001b[39m model.eval() \n\u001b[32m      4\u001b[39m model = torch.load(\u001b[33m\"\u001b[39m\u001b[33mreview_classifier.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device)\n",
      "\u001b[31mAttributeError\u001b[39m: 'collections.OrderedDict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() \n",
    "model = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "\n",
    "\n",
    "def classify_message(message):\n",
    "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2239, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1746, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3214034251.py\", line 4, in classify_message\n",
      "    return classify_review(message, model, tokenizer, device, max_length=train_dataset.max_length)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Mohammed Fadel\\AppData\\Local\\Temp\\ipykernel_19040\\3480187256.py\", line 2, in classify_review\n",
      "    model.eval()\n",
      "    ^^^^^^^^^^\n",
      "AttributeError: 'collections.OrderedDict' object has no attribute 'eval'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=classify_message,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your message here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Spam Classifier\",\n",
    "    description=\"Enter a message to determine if it's spam or not.\"\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b9ac0",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 2 - PERSONAL ASSISTANT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b3746",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
