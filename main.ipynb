{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee66e88",
   "metadata": {},
   "source": [
    "# **Building a Small Language Model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5fe9d",
   "metadata": {},
   "source": [
    "# **Stage One - Building the LLM**\n",
    "This Stage Consists of 3 Parts:\n",
    "1. Data Preparation and Sampling\n",
    "2. Attention Mechanisms\n",
    "3. LLM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f70983",
   "metadata": {},
   "source": [
    "## Part 1 - *Data Preparation & Sampling*\n",
    "Formatting and preparing the data into the necessary form to be processed and understood by the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578b9cc",
   "metadata": {},
   "source": [
    "### All Imports Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "import urllib.request\n",
    "import zipfile \n",
    "import os\n",
    "from pathlib import Path\n",
    "from gpt_download import download_and_load_gpt2\n",
    "import time\n",
    "\n",
    "import json\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee58bf5",
   "metadata": {},
   "source": [
    "#### Importing the Dataset Example\n",
    "In our case, we will be using the pdf form of the book \"The Verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a75eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x212b174d850>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b5ceb",
   "metadata": {},
   "source": [
    "### Creating the Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a534b0",
   "metadata": {},
   "source": [
    "#### Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43373940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ']\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to create tokens.\n",
    "# We want to filter out whiite spaces and special characters.\n",
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(preprocessed[:50])\n",
    "\n",
    "# Will filter out any whitespaces and only return the characters.\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b258f4",
   "metadata": {},
   "source": [
    "#### Step 2: Determining the Vocabulary and mapping them to their token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "# Creating our Vocabulary of unique tokens (Alphabetically Arranged)\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "# Print first 50 vocab elements\n",
    "for i, item in enumerate(vocab.items()):\n",
    "  print(item)\n",
    "  if i >= 50:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff2239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6af922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SimpleTokenizerV1 - A simple tokenizer that can perform encoding and decoding.\n",
    "\n",
    "SimpleTokenizerV2 - Replaces Uknown words with the special character <|unk|> and\n",
    "unrelated pieces of texts with <|endoftext|>\n",
    "\n",
    "\"\"\"\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "      self.str_to_int = vocab\n",
    "      self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "      preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "      preprocessed = [\n",
    "      item.strip() for item in preprocessed if item.strip()\n",
    "      ]\n",
    "      ids = [self.str_to_int[s] for s in preprocessed]\n",
    "      return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "      text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "      return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5986e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "  def __init__(self, vocab):\n",
    "    self.str_to_int = vocab\n",
    "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "\n",
    "  def encode(self, text):\n",
    "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "    preprocessed = [\n",
    "    item.strip() for item in preprocessed if item.strip()\n",
    "    ]\n",
    "\n",
    "    preprocessed = [item if item in self.str_to_int\n",
    "                else \"<|unk|>\" for item in preprocessed]\n",
    "    ids = [self.str_to_int[s] for s in preprocessed]\n",
    "    return ids\n",
    "\n",
    "  def decode(self, ids):\n",
    "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc647e92",
   "metadata": {},
   "source": [
    "##### Create Byte-Pair Encoder\n",
    "- Is a subword tokenization algorithm. The most common pair of consecutive bytes of data is replaced with a byte that does not occur in data.\n",
    "\n",
    "Advantages:\n",
    "- Byte-pair encoding can reduce the size of the vocabulary significantly.\n",
    "- The BPE tokenizer can handle any unknown word without needing the `<|unk|>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a498ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text)) # Prints the new number of tokens using the GPT2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "z:            [2241, 287, 257, 4489]\n",
      "------------------------------------\n",
      "[290] ----> 4920\n",
      " and ---->  established\n",
      "\n",
      "[290, 4920] ----> 2241\n",
      " and established ---->  himself\n",
      "\n",
      "[290, 4920, 2241] ----> 287\n",
      " and established himself ---->  in\n",
      "\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      " and established himself in ---->  a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To better visualize what's being done\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "z = enc_sample[2:context_size+2]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "print(f\"z:            {z}\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Representation: left side = input, right side = target\n",
    "for i in range(1, context_size+1):\n",
    "  context = enc_sample[:i]\n",
    "  desired = enc_sample[i]\n",
    "  print(context, \"---->\", desired)\n",
    "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired])) # Text equivalent\n",
    "  print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f21822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Implementation\n",
    "class GPTDatasetV1(Dataset):\n",
    "  def __init__(self, txt, tokenizer, max_length, stride):\n",
    "    self.input_ids = []\n",
    "    self.target_ids = []\n",
    "\n",
    "    # Tokenizes the entire text\n",
    "    token_ids = tokenizer.encode(txt)\n",
    "\n",
    "    # Uses a sliding window approach to chunk the book into overlapping sequences.\n",
    "    for i in range(0, len(token_ids) - max_length, stride):\n",
    "      input_chunk = token_ids[i:i + max_length]\n",
    "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "      self.input_ids.append(torch.tensor(input_chunk))\n",
    "      self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "# Dataloader Implementation\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                 stride=128, shuffle=True, drop_last=True,\n",
    "                 num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") # Instantiates the gpt2 tokenizer\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  # Initialize the Dataset class created earlier\n",
    "\n",
    "    # Intantiates and provides parameters for the DataLoader python class provided by PyTorch.\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549478f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464,  1807,  3619],\n",
      "        [  402,   271, 10899,  2138,   257,  7026]]), tensor([[  367,  2885,  1464,  1807,  3619,   402],\n",
      "        [  271, 10899,  2138,   257,  7026, 15632]])]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    dataloader = create_dataloader_v1(\n",
    "      raw_text, batch_size=2, max_length=6, stride=6, shuffle=False)\n",
    "    data_iter = iter(dataloader)  # Creates an Iterator\n",
    "    first_batch = next(data_iter) # Gets the next batch from the data Iterator, the first_batch will be assigned a tuple of tensors.\n",
    "    print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d30748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[15632,   438,  2016,   257,   922,  5891],\n",
      "        [ 1576,   438,   568,   340,   373,   645]]), tensor([[ 438, 2016,  257,  922, 5891, 1576],\n",
      "        [ 438,  568,  340,  373,  645, 1049]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a32d20",
   "metadata": {},
   "source": [
    "#### Step 3: Mapping Token Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8fe982",
   "metadata": {},
   "source": [
    "##### Positional Embedding Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the inputs that we'll be using, with their embeddings, given that each word has 3 dimensions. \n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ee5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 256])\n",
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Embedding function only requires the vocab_size\n",
    "as it'll provide random embeddings originally\n",
    "and those embeddings will be adjusted over training.\n",
    "\"\"\"\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs.long())\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Instantiating the data loader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "   stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24bc33",
   "metadata": {},
   "source": [
    "## Part 2 - *ATTENTION MECHANISM*\n",
    "There are 4 kinds of attention mechanisms and we will be implementing each one until we arrive to the original </br>\n",
    "transformer's **\"Multi-Head Self-Attention\"** mechanism, building on top of the previous implementation and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fabb8",
   "metadata": {},
   "source": [
    "### First Implementation: **Simplified Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a22ef6",
   "metadata": {},
   "source": [
    "#### Compute Attention Scores - 2nd input example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee95f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2956967., 16597455., 29439276., 30712224., 23737832.,  2581577.,\n",
      "        23769278., 12535636.])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # Takes 'journey' as the query.\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "  attention_scores_2[i] = torch.dot(x_i, query) \n",
    "\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002ab0",
   "metadata": {},
   "source": [
    "#### Compute Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a70b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.0208, 0.1166, 0.2068, 0.2158, 0.1668, 0.0181, 0.1670, 0.0881])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attention_weights_2_tmp)\n",
    "print(\"Sum: \", attention_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb429bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Sum: tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following is a naive Softmax Implementation. However, this implementation has weaknesses when dealing with very small or large values.\n",
    "Therefore it is recommended to simply use PyTorch's implmentation of Softmax.\n",
    "''' \n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attention_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a347d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Softmax Function Implementation, results in same value of previous softmax function.\n",
    "attn_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1446cd",
   "metadata": {},
   "source": [
    "#### Compute Context Vectors\n",
    "The context vector z is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf5848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6203.2407, 3242.1980,  940.7390, 1740.2675])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attention_weights_2_tmp[i]*x_i #x_i is simply the input vector, multiplying with its corresponding attention weights.\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3122ce",
   "metadata": {},
   "source": [
    "#### A Generalised Approach\n",
    "So far, for the attention mechanism built above, we have implemented attention with respect to the embeddings vector of the second input. Now we will create a generalised method that can be applied to all embeddings vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa276d",
   "metadata": {},
   "source": [
    "#### Generalized Process\n",
    "We perform the same 3 steps performed prior, with a few modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f5b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your    \n",
    "   [0.55, 0.87, 0.66], # journey \n",
    "   [0.57, 0.85, 0.64], # starts  \n",
    "   [0.22, 0.58, 0.33], # with    \n",
    "   [0.77, 0.25, 0.10], # one     \n",
    "   [0.05, 0.80, 0.55]] # step    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcacc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nThe issue with the above implementation is that cause of the nested for loop\\nit is slow and computationally expensive, a better approach can be done with linear algebra.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, j_i in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, j_i)\n",
    "\n",
    "print(attention_scores) # Unnormalized output\n",
    "\n",
    "\"\"\" \n",
    "The issue with the above implementation is that cause of the nested for loop\n",
    "it is slow and computationally expensive, a better approach can be done with linear algebra.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following method is more efficient, not because the time complexity is any less,\n",
    "but the implementation time is far less, the time it takes to perform each multiplication, the '@' symbol represents matrix multiplication.\n",
    "You get the exact same answer as the previous method, but much faster.\n",
    "\"\"\"\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83715b4",
   "metadata": {},
   "source": [
    "#### Calculating the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "----------------------------------------------------------------\n",
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Declaring the dimensions to be -1 will normally apply the normalization to the last dimension, however, for a 2 dimensional tensor,\n",
    "the dim = -1 will apply the Softmax into all columns.\n",
    "'''\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights)\n",
    "\n",
    "# To verify that all rows sum up to 1\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attention_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641172",
   "metadata": {},
   "source": [
    "#### Computing the Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200027bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Simply a matrix multiplication is performed. Concluding the generalised method of calculating context vectors.\n",
    "all_context_vec = attention_weights @ inputs\n",
    "print(all_context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5497f",
   "metadata": {},
   "source": [
    "### Second Implementation: **Self-Attention**\n",
    "This self-attention mechanism is also called *scaled dot-product attention*. \n",
    "<br>We will be building on top of our last model, this time introducing **Trainable Weight Matrices**. This can help the model to produce better context vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a933d6",
   "metadata": {},
   "source": [
    "For illustration purposes, we will be computing for only one context vector, *z(2)*.\n",
    "- Note that in GPT-like models, the input and output dimensions are usually the same,\n",
    " but to better follow the computation, weâ€™ll use different input (d_in=3) and output\n",
    " (d_out=2) dimensions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables dealing with the second input\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abb67c",
   "metadata": {},
   "source": [
    " Initialize the query, key, and value matrices of the second input \"journey\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef594936",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e655dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b100eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cac2bf",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Input 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d33a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]            \n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77628b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f50d2d81",
   "metadata": {},
   "source": [
    "#### Calculate Attention Score of Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b54dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2) # Notice the attention score matches the previously calculated attention score for the second input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e1344",
   "metadata": {},
   "source": [
    "#### Calculate the Attention Weights of the entire Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcd287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k ** 0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "print(d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247b93b",
   "metadata": {},
   "source": [
    "#### Calculate the Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0915d",
   "metadata": {},
   "source": [
    "#### Implementing a compact self-attention Python class\n",
    "* In practice, with the LLM implementation that'll be performed later in mind, it is helpful to organize this code into a Python class, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attention_scores = queries @ keys.T # omega\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b141a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention Class version 2\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\\n they use different initial weights for the weight matrices since nn.Linear uses a more\\n sophisticated weight initialization scheme.\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n",
    "\n",
    "'''\n",
    " Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because\n",
    " they use different initial weights for the weight matrices since nn.Linear uses a more\n",
    " sophisticated weight initialization scheme.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de148abf",
   "metadata": {},
   "source": [
    "### Third Implementation: **Causal Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc795a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)    \n",
    "keys = sa_v2.W_key(inputs) \n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56313a",
   "metadata": {},
   "source": [
    "#### Step 1: Initialise the masked matrix with the \"*tril*\" function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924c81c",
   "metadata": {},
   "source": [
    "#### Step 2: Multiply the attention weights matrix with the masked matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55398fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attention_weights * mask_simple\n",
    "print(masked_simple) # prints non-normalised values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e586bc4",
   "metadata": {},
   "source": [
    "#### Step 3: Renormalize each row to sum up to 1 using the softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e128baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd130fef",
   "metadata": {},
   "source": [
    "#### Second More Efficient Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ff6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(context_length, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa990d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6f16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04995c95",
   "metadata": {},
   "source": [
    "#### Using *Dropout*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f89d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)   \n",
    "example = torch.ones(6, 6)     \n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b47d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Applying dropout to the attention weights\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attention_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f172bc",
   "metadata": {},
   "source": [
    "#### Implementing a Causal Attention Class\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented <br>\n",
    "earlier, except that we added the dropout and causal mask components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6696b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)           \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "            diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                  \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.transpose(1, 2)   \n",
    "        attention_scores.masked_fill_(                   \n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attention_weights = torch.softmax(\n",
    "        attention_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015435da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f197cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa70423",
   "metadata": {},
   "source": [
    "### Final Implementation: **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a206a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper class to implement multi-head attention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "        [CausalAttention(\n",
    "         d_in, d_out, context_length, dropout, qkv_bias\n",
    "     ) \n",
    "     for _ in range(num_heads)]\n",
    " )\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39937465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    "    )\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc35d32",
   "metadata": {},
   "source": [
    "#### Efficient Multi-Head Attention Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, \n",
    "         context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "        \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads   \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)   \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                        diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)   \n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)      \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)  \n",
    "        queries = queries.view(                                             \n",
    "            b, num_tokens, self.num_heads, self.head_dim    \n",
    "        )\n",
    "\n",
    "        keys = keys.transpose(1, 2)   \n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] \n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        context_vec = context_vec.contiguous().view(\n",
    "            b, num_tokens, self.d_out\n",
    "        )\n",
    "        \n",
    "        context_vec = self.out_proj(context_vec)   \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d688d0e3",
   "metadata": {},
   "source": [
    "## PART 3 - *LLM ARCHITECTURE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16afc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e852edb",
   "metadata": {},
   "source": [
    "### Placeholder GPT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88db552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])      # Creates the 768 dimensional word embedding.\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])  # Creates the positional embeddings.\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])                        # Creates a random dropout embedding of 10% of the total embeddings. \n",
    "        self.trf_blocks = nn.Sequential(                                    # Creates an \"n_layers\" amount of transformer blocks.\n",
    "            *[DummyTransformerBlock(cfg)              \n",
    "            for _ in range(cfg[\"n_layers\"])]        \n",
    "        )\n",
    "\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                    # Final normalization layer\n",
    "        self.out_head = nn.Linear(                                          # Projects the final hidden states to vocabulary size to get the logits for predicting the next token.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):                                              # in_idx param is a tensor of shape (batch_size, seq_len) containing token indices.\n",
    "        batch_size, seq_len = in_idx.shape                                  # Splitting the in_idx tensor into 2 seperate vectors.\n",
    "        tok_embeds = self.tok_emb(in_idx)                                   # Converts input tokens into vectors.\n",
    "        pos_embeds = self.pos_emb(                                          # Creates the (absolute) position vectors from the sequence length of each input.\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds                                         # Adds the token embeddings with the (absolute) positional embeddings, resulting into our prepared input.\n",
    "        x = self.drop_emb(x)                                                # We drop the randomly chosen matrix values (dropout regularization).\n",
    "        x = self.trf_blocks(x)                                              # Runs the input through each transformer block in sequence.\n",
    "        x = self.final_norm(x)                                              # Final normalization\n",
    "        logits = self.out_head(x)                                           # The output logits will contain the probability values needed for prediction.\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):                                     # Simple placeholder class that'll be replaced by a real Transformer Block later.\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):    \n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):                                            # Simple placeholder class that'll be replaced by a real LayerNorm later.\n",
    "    def __init__(self, normalized_shape, eps=1e-5):   \n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")           # Instantiate the GPT-2 tokenizer.\n",
    "batch = []                                          # Initialize an empty list\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))  # First encodes the text using GPT-2 tokenizer -> converted into a tensor -> is appended into the batch list.\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)                   # Takes in a list of tensors and \"stacks\" (concatenates) them into a 2D tensor.\n",
    "print(batch)                                        # Print the resulting 2D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e03364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe output tensor has two rows corresponding to the two text samples. \\nEach text sample consists of four tokens.\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)                      # Passes the previously defined batch into our DummyGPTModel.\n",
    "print(\"Output shape:\", logits.shape)       # The standard shape distribution: (batch_size, seq_len, vocab_size).\n",
    "print(logits)                              # Outputs the 2 batches, each containing 4 sequences(inputs) and their embeddings relative to a vocab of 50257.\n",
    "\n",
    "\"\"\"\n",
    "The output tensor has two rows corresponding to the two text samples. \n",
    "Each text sample consists of four tokens.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)                   # Creates 2 batches of random numbers, each containing 5 values. \n",
    "layer = nn.Sequential(                              # Constructs a sequential neural network module.\n",
    "    nn.Linear(5, 6),                                # Creates a fully connected linear layer, taking in 5 input vectors and outputing 6.\n",
    "    nn.ReLU()                                       # Applies nonlinear activation function.\n",
    ")       \n",
    "out = layer(batch_example)                          # Feeds the nn layer with the batch example\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adaf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)   # Keepdim ensures the dimensions stay the same, and doesn't combine the dimensions of the first and second input together.\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ca1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [5.9605e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)          # Disables Scientific Notation \n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78df5c",
   "metadata": {},
   "source": [
    "#### GELU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc64996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x  * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0cbdf",
   "metadata": {},
   "source": [
    "#### Layer Normalization Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8adcd",
   "metadata": {},
   "source": [
    "#### Feed Forward Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bc04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(                            # Simply an implemtation of a 3 layered forward neural network.\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),      # This linear portion \"expands\" the input embedding dimension into the number of nodes of the following hidden layer, in our case it'll 4 times the amount of the emb dim.\n",
    "            GELU(),                                             # Calls the GELU function on the nodes.\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])       # Does the opposite of the first linear layer, \"compressing\" from 4 times the size back to the original size of the input/\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)  # Intance\n",
    "x = torch.rand(2, 3, 768)           # 2 batches, each batch has 3 tokens, and each token will have an embedding size of 768.\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037276cc",
   "metadata": {},
   "source": [
    "### Shortcut Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd77bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([                                   # Implement 5 layers.\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), \n",
    "                  GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), \n",
    "                  GELU())\n",
    " ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)        \n",
    "            if self.use_shortcut and x.shape == layer_output.shape:   \n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f39719",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)                           \n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e56d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)               # Forward Pass.\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)     # Calculates loss based on how close the target and outputs are.\n",
    "    \n",
    "    loss.backward()                 # Backward pass to calculate the gradients.\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db27c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173590746708214\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152042235247791\n",
      "layers.3.0.weight has gradient mean of 0.0013988739810883999\n",
      "layers.4.0.weight has gradient mean of 0.00504964729771018\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8432b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694102346897125\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    " )\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08917f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  \n",
    "   \n",
    "        shortcut = x        \n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5812e",
   "metadata": {},
   "source": [
    "#### Instantiating a Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4e8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                  \n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be38b3",
   "metadata": {},
   "source": [
    "### GPT FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550129a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):        #initializes the token and positional embedding layers using the configurations passed in via cfg.\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(                # Creates transformer blocks equal to that in specified in cfg.\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]    \n",
    "            )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])     # Layer normalization is applied.\n",
    "        self.out_head = nn.Linear(  # linear output head without bias is defined, which projects the transformerâ€™s output into the vocabulary space of the tokenizer to generate logits for each token in the vocabulary.\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f76141",
   "metadata": {},
   "source": [
    "#### Instantiating our GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a92ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)     # Shape: [2, 4, 50257]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba46df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e328bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434dff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    " )\n",
    "\n",
    "print(f\"Number of trainable parameters \"\n",
    "      f\"considering weight tying: {total_params_gpt2:,}\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e422a",
   "metadata": {},
   "source": [
    "#### Calculate the total size needed for the 163 million parameters in our GPTModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d4e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4      \n",
    "total_size_mb = total_size_bytes / (1024 * 1024)    \n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx,                \n",
    "                 max_new_tokens, context_size): \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]   \n",
    "        with torch.no_grad():\n",
    "           logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]                   \n",
    "        probas = torch.softmax(logits, dim=-1)         \n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)   \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)   \n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval()                 \n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5236fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d9c1e",
   "metadata": {},
   "source": [
    "# **Stage Two - Foundational Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b901e7",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42bce8",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e399602",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,   #! Context length dropped from 1024 -> 256.\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5be60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)                 # Unsqueeze adds a batch dimension. Shape becomes [1, seq_length]\n",
    "    return encoded_tensor\n",
    " \n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)                 # Removes batch dimension, resulting in a 1D tensor.\n",
    "    return tokenizer.decode(flat.tolist())      # First the tensor is converted to a list of integers and then decoded to human readable text.\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(               # Calls and passes parameter values into the generate_text_simple function.\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),    # Is the starting prompt.\n",
    "    max_new_tokens=10,                                  # Specifies that 10 new tokens should be generated after the first prompt\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]      # Determines the maximum token size to be considered at once.\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))       # Calls token ids to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16937771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs.long())\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533de8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]],\n",
      "\n",
      "        [[27515],\n",
      "         [29578],\n",
      "         [    9]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28125199",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "               [        40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e9b34",
   "metadata": {},
   "source": [
    "#### Calculating Logits and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eea9f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "tensor([[[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]],\n",
      "\n",
      "        [[    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000],\n",
      "         [    0.0000,     0.0000,     0.0000,  ...,     0.0000,\n",
      "              0.0000,     0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)    \n",
    "print(probas.shape)\n",
    "print(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937701dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)  # Since we have 2 batches, each containing 3 tokens, we received the highest probability value for each token in each batch.\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: Gathering SerbianFriday\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\"{token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957afd64",
   "metadata": {},
   "source": [
    "### Text Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f199b",
   "metadata": {},
   "source": [
    "#### Calculating Target Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57543b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([    0.0000,     0.0000,     0.0000])\n",
      "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Printing the initial Softmax probability scores\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409c271",
   "metadata": {},
   "source": [
    "#### Calculating the Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc810d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20d5f",
   "metadata": {},
   "source": [
    "#### Average Log Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc451d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.8765)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed6e58",
   "metadata": {},
   "source": [
    "#### Negative Average Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a093be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1   # Simply multiply the average by -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afbfa57",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be226f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# It's important to keep track of the dimensions in order to perform cross entropy.\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bc9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "For the cross_entropy loss function in PyTorch, we want to flatten these tensors\n",
    "by combining them over the batch dimension:\n",
    "'''\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86fce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8765)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52918.7773)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Perplexity\n",
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22782",
   "metadata": {},
   "source": [
    "### Training & Validation Losses\n",
    "We will use the text file of \"the verdict\" once more to demonstrate training our LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)  # We will only be working with 5145 tokens for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56f834",
   "metadata": {},
   "source": [
    "#### Train and Testing Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7901e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bca0de",
   "metadata": {},
   "source": [
    "#### Calling the Dataloader from Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader is called for both training and validation sets.\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    " )\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1006d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Iterating through both data loaders\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")   \n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e11a",
   "metadata": {},
   "source": [
    "#### Cross entropy lost among Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)        \n",
    "    target_batch = target_batch.to(device)      \n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d37b8",
   "metadata": {},
   "source": [
    "#### Cross entropy lost over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))  #  Reduces the number of batches to match the total number of batches in the data loader if num_batches exceeds the number of batches in the data loader.\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()   # Sums the loss for each batch\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches     # Averages the loss over all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352e13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98850186665853\n",
      "Validation loss: 10.990342140197754\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)  \n",
    "with torch.no_grad():                                       \n",
    "    train_loss = calc_loss_loader(train_loader, model, device)   \n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cd656",
   "metadata": {},
   "source": [
    "### The main function for pretraining LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f47fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_model and generate_and_print_sample functions are not defined yet. \n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "               optimizer, device, num_epochs,\n",
    "               eval_freq, eval_iter, start_context, tokenizer):\n",
    "    \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []   \n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                    \n",
    "            optimizer.step()                   \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:   \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(                     \n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b2de5",
   "metadata": {},
   "source": [
    "#### Evaluate model function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()            # Dropout is disabled \n",
    "    with torch.no_grad():   # Disables gradient tracking\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3693a8",
   "metadata": {},
   "source": [
    "#### Generate and print sample function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d946563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    \n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "        \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1508a",
   "metadata": {},
   "source": [
    "#### Training a GPTModel for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48aca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.823, Val loss 9.932\n",
      "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.336\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.621, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.043, Val loss 6.599\n",
      "Every effort moves you, and,, and,, and,,,, and, and,,,,,,,,, and,,,, the,,,, and,, and,,, the, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.547, Val loss 6.485\n",
      "Ep 3 (Step 000025): Train loss 5.450, Val loss 6.397\n",
      "Every effort moves you, and to the to the of the to the, and I had. Gis, and, and, and, and, and, and I had, and, and, and, and, and, and, and, and, and,\n",
      "Ep 4 (Step 000030): Train loss 4.982, Val loss 6.301\n",
      "Ep 4 (Step 000035): Train loss 4.755, Val loss 6.296\n",
      "Every effort moves you, and I had been the of the picture to the picture.                                     \n",
      "Ep 5 (Step 000040): Train loss 4.162, Val loss 6.182\n",
      "Every effort moves you know the                                                \n",
      "Ep 6 (Step 000045): Train loss 3.742, Val loss 6.175\n",
      "Ep 6 (Step 000050): Train loss 3.213, Val loss 6.189\n",
      "Every effort moves you know the fact, and I felt--I had the fact a little of a little to my work, and in fact, and in the picture.      \"--and it, and, and down, and he was his\n",
      "Ep 7 (Step 000055): Train loss 3.143, Val loss 6.172\n",
      "Ep 7 (Step 000060): Train loss 2.422, Val loss 6.136\n",
      "Every effort moves you know the picture.  I glanced after him, and I was one of the house.\"   \"I didn't you know. I was his pictures.  \"--and I was a little a little the room, I was\n",
      "Ep 8 (Step 000065): Train loss 1.963, Val loss 6.178\n",
      "Ep 8 (Step 000070): Train loss 1.639, Val loss 6.230\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and uncertain.  \"Once, when I looked up, I had been. Gisburn--as Jack himself, as once one had been the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.286, Val loss 6.227\n",
      "Ep 9 (Step 000080): Train loss 0.987, Val loss 6.249\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs. \"--and I was a year after Jack's resolve had been his eyes; then I looked at the donkey--and I saw that, and down the room, I had\n",
      "Ep 10 (Step 000085): Train loss 0.727, Val loss 6.356\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),          \n",
    "    lr=0.0004, weight_decay=0.1\n",
    " )\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448655fb",
   "metadata": {},
   "source": [
    "#### Visualizing the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b08f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    \n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()                  \n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)    \n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)) \n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f63439",
   "metadata": {},
   "source": [
    "#### 5. Text Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc6521",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m token_ids = \u001b[43mgenerate_text_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvery effort moves you\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[123]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mgenerate_text_simple\u001b[39m\u001b[34m(model, idx, max_new_tokens, context_size)\u001b[39m\n\u001b[32m      4\u001b[39m idx_cond = idx[:, -context_size:]   \n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m    logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]                   \n\u001b[32m      9\u001b[39m probas = torch.softmax(logits, dim=-\u001b[32m1\u001b[39m)         \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[32m     18\u001b[39m     batch_size, seq_len = in_idx.shape\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     tok_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     pos_embeds = \u001b[38;5;28mself\u001b[39m.pos_emb(\n\u001b[32m     22\u001b[39m         torch.arange(seq_len, device=in_idx.device)\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     25\u001b[39m     x = tok_embeds + pos_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    " )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a69b8",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b38a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the next-token generation process, let's use a small vocab.\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    " )\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "582 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "343 x toward\n"
     ]
    }
   ],
   "source": [
    "# To implement a probabilistic sampling process, we can now replace argmax with the multinomial function in PyTorch.\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "\"\"\"\n",
    "As a result, \"forward\" will still be printed as it is still the most likely next word. \n",
    "However, this time, it will not be the case ALL the time. Let's test this out.\n",
    "\"\"\"\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    \n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa329c4",
   "metadata": {},
   "source": [
    "#### Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise the diversity of outputs depending on the temperature value.\n",
    "temperatures = [1, 0.1, 5]\n",
    "\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "        for T in temperatures]\n",
    "\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], \n",
    "           bar_width, label=f'Temperature = {T}')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4d9fe",
   "metadata": {},
   "source": [
    "#### Top-K Sampling\n",
    "Selects the top 'k' probability values of the logits then sets all other values to `-inf` where the softmax function is applied and the remaining probabilities sum up to 1 and the `-inf` values become equal to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872276f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n",
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# Applying in code: Step 1 - Select the top 3 highest probability logits (k being 3 in this example)\n",
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n",
    "\n",
    "# Step 2 - Assigning -inf for all non-selected logits.\n",
    "new_logits = torch.where(                               \n",
    "    condition=next_token_logits < top_logits[-1],       # Identifies logits less than the minimum in the top 3\n",
    "    input=torch.tensor(float('-inf')),                  # Assigns â€“inf to these lower logits\n",
    "    other=next_token_logits                             #  Retains the original logits for all other tokens\n",
    ")\n",
    "print(new_logits)\n",
    "\n",
    "# Step 3 - Applying the Softmax function\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a394983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size,\n",
    "     temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):           # The for loop is the same as before: gets logits and only focuses on the last time step\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:               # Filters logits with top_k sampling\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        if temperature > 0.0:               #  Applies temperature scaling\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:                               #  Carries out greedy next token selection as before when temperature scaling is disabled\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:             #  Stops generating early if end-of-sequence token is encountered\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3274e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m token_ids = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvery effort moves you\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m55\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.6\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id)\u001b[39m\n\u001b[32m      4\u001b[39m idx_cond = idx[:, -context_size:]\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m logits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m top_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:               \u001b[38;5;66;03m# Filters logits with top_k sampling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_idx):\n\u001b[32m     18\u001b[39m     batch_size, seq_len = in_idx.shape\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     tok_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtok_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     pos_embeds = \u001b[38;5;28mself\u001b[39m.pos_emb(\n\u001b[32m     22\u001b[39m         torch.arange(seq_len, device=in_idx.device)\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     25\u001b[39m     x = tok_embeds + pos_embeds\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mohammed Fadel\\Documents\\GitHub\\Building-a-Small-Language-Model\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=55,\n",
    "    temperature=1.6\n",
    "    )\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df0b149",
   "metadata": {},
   "source": [
    "### Loading and Saving Model Weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25a395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()    # switches the model to evaluation mode for inference, disabling the dropout layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({    # Using torch.save, we can save both the model and optimizer state_dict contents\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477371f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt_download.py', <http.client.HTTPMessage at 0x198b270a540>)"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download GPT2 Weights\n",
    "import urllib.request\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/\"\n",
    "    \"LLMs-from-scratch/main/ch05/\"\n",
    "    \"01_main-chapter-code/gpt_download.py\"\n",
    "    )\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=\"124M\", models_dir=\"gpt2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee84465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting which model variant to load\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "\n",
    "# Since we used a 256 token length earlier, meanwhile, the original model uses a token length of 1,024 we must update the model accordingly\n",
    "NEW_CONFIG.update({\"context_length\": 1024})\n",
    "\n",
    "# Bias vectors are no longer in use, however they we're for creating GPT-2\n",
    "NEW_CONFIG.update({\"qkv_bias\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f93bfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating new gpt instance\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
    "                          \"Right: {right.shape}\"\n",
    "        )\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b59e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the function that'll load the weights into our gpt instance\n",
    "def load_weights_into_gpt(gpt, params):       #  Sets the modelâ€™s positional and token embedding weights to those specified in params\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):    # Iterates over each transformer block\n",
    "        q_w, k_w, v_w = np.split(             #  The np.split function is used to divide the attention and bias weights into three equal parts for the query, key, and value components.\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        \n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        \n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    " \n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    " \n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "        \n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebf544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the weights\n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8edeb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as the hand can go until the end of your turn unless something happens\n",
      "\n",
      "This would remove you from a battle\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    " )\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92621977",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 1 - CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacbf93",
   "metadata": {},
   "source": [
    "## Mobile Spam Text Classifier\n",
    "We will use a dataset to help us classify whether a text is spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1673c7",
   "metadata": {},
   "source": [
    "### Importing Libraries and Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d3831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'sms_spam_collection\\\\SMSSpamCollection' -> 'sms_spam_collection\\\\SMSSpamCollection.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileExistsError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[616]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m         os.rename(original_file_path, data_file_path)              \n\u001b[32m     21\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile downloaded and saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mdownload_and_unzip_spam_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextracted_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[616]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mdownload_and_unzip_spam_data\u001b[39m\u001b[34m(url, zip_path, extracted_path, data_file_path)\u001b[39m\n\u001b[32m     17\u001b[39m     zip_ref.extractall(extracted_path)\n\u001b[32m     19\u001b[39m original_file_path = Path(extracted_path) / \u001b[33m\"\u001b[39m\u001b[33mSMSSpamCollection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m)\u001b[49m              \n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile downloaded and saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileExistsError\u001b[39m: [WinError 183] Cannot create a file when that file already exists: 'sms_spam_collection\\\\SMSSpamCollection' -> 'sms_spam_collection\\\\SMSSpamCollection.tsv'"
     ]
    }
   ],
   "source": [
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\" \n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(\n",
    "    url, zip_path, extracted_path, data_file_path):\n",
    "        if data_file_path.exists():\n",
    "            print(f\"{data_file_path} already exists. Skipping download \"\n",
    "                                    \"and extraction.\"\n",
    "            )\n",
    "        \n",
    "        with urllib.request.urlopen(url) as response:   \n",
    "            with open(zip_path, \"wb\") as out_file:\n",
    "                out_file.write(response.read())\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:   \n",
    "            zip_ref.extractall(extracted_path)\n",
    "\n",
    "        original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "        os.rename(original_file_path, data_file_path)              \n",
    "        print(f\"File downloaded and saved as {data_file_path}\")\n",
    "    \n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6288536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "91d88f0b-e4d4-4037-96ab-999b1385b310",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ],
        [
         "5",
         "spam",
         "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv"
        ],
        [
         "6",
         "ham",
         "Even my brother is not like to speak with me. They treat me like aids patent."
        ],
        [
         "7",
         "ham",
         "As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune"
        ],
        [
         "8",
         "spam",
         "WINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only."
        ],
        [
         "9",
         "spam",
         "Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030"
        ],
        [
         "10",
         "ham",
         "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today."
        ],
        [
         "11",
         "spam",
         "SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info"
        ],
        [
         "12",
         "spam",
         "URGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18"
        ],
        [
         "13",
         "ham",
         "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times."
        ],
        [
         "14",
         "ham",
         "I HAVE A DATE ON SUNDAY WITH WILL!!"
        ],
        [
         "15",
         "spam",
         "XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL"
        ],
        [
         "16",
         "ham",
         "Oh k...i'm watching here:)"
        ],
        [
         "17",
         "ham",
         "Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet."
        ],
        [
         "18",
         "ham",
         "Fine if thatÂ’s the way u feel. ThatÂ’s the way its gota b"
        ],
        [
         "19",
         "spam",
         "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOXox36504W45WQ 16+"
        ],
        [
         "20",
         "ham",
         "Is that seriously how you spell his name?"
        ],
        [
         "21",
         "ham",
         "Iâ€˜m going to try for 2 months ha ha only joking"
        ],
        [
         "22",
         "ham",
         "So Ã¼ pay first lar... Then when is da stock comin..."
        ],
        [
         "23",
         "ham",
         "Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?"
        ],
        [
         "24",
         "ham",
         "Ffffffffff. Alright no way I can meet up with you sooner?"
        ],
        [
         "25",
         "ham",
         "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol"
        ],
        [
         "26",
         "ham",
         "Lol your always so convincing."
        ],
        [
         "27",
         "ham",
         "Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?"
        ],
        [
         "28",
         "ham",
         "I'm back &amp; we're packing the car now, I'll let you know if there's room"
        ],
        [
         "29",
         "ham",
         "Ahhh. Work. I vaguely remember that! What does it feel like? Lol"
        ],
        [
         "30",
         "ham",
         "Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us"
        ],
        [
         "31",
         "ham",
         "Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You? "
        ],
        [
         "32",
         "ham",
         "K tell me anything about you."
        ],
        [
         "33",
         "ham",
         "For fear of fainting with the of all that housework you just did? Quick have a cuppa"
        ],
        [
         "34",
         "spam",
         "Thanks for your subscription to Ringtone UK your mobile will be charged Â£5/month Please confirm by replying YES or NO. If you reply NO you will not be charged"
        ],
        [
         "35",
         "ham",
         "Yup... Ok i go home look at the timings then i msg Ã¼ again... Xuhui going to learn on 2nd may too but her lesson is at 8am"
        ],
        [
         "36",
         "ham",
         "Oops, I'll let you know when my roommate's done"
        ],
        [
         "37",
         "ham",
         "I see the letter B on my car"
        ],
        [
         "38",
         "ham",
         "Anything lor... U decide..."
        ],
        [
         "39",
         "ham",
         "Hello! How's you and how did saturday go? I was just texting to see if you'd decided to do anything tomo. Not that i'm trying to invite myself or anything!"
        ],
        [
         "40",
         "ham",
         "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola"
        ],
        [
         "41",
         "ham",
         "Did I forget to tell you ? I want you , I need you, I crave you ... But most of all ... I love you my sweet Arabian steed ... Mmmmmm ... Yummy"
        ],
        [
         "42",
         "spam",
         "07732584351 - Rodger Burns - MSG = We tried to call you re your reply to our sms for a free nokia mobile + free camcorder. Please call now 08000930705 for delivery tomorrow"
        ],
        [
         "43",
         "ham",
         "WHO ARE YOU SEEING?"
        ],
        [
         "44",
         "ham",
         "Great! I hope you like your man well endowed. I am  &lt;#&gt;  inches..."
        ],
        [
         "45",
         "ham",
         "No calls..messages..missed calls"
        ],
        [
         "46",
         "ham",
         "Didn't you get hep b immunisation in nigeria."
        ],
        [
         "47",
         "ham",
         "Fair enough, anything going on?"
        ],
        [
         "48",
         "ham",
         "Yeah hopefully, if tyler can't do it I could maybe ask around a bit"
        ],
        [
         "49",
         "ham",
         "U don't know how stubborn I am. I didn't even want to go to the hospital. I kept telling Mark I'm not a weak sucker. Hospitals are for weak suckers."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5572
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will Ã¼ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]\n",
    " )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b691b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e64fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]    \n",
    "\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(\n",
    "        num_spam, random_state=123\n",
    "    )                                        \n",
    "\n",
    "    balanced_df = pd.concat([\n",
    "        ham_subset, df[df[\"Label\"] == \"spam\"]\n",
    "    ])                              \n",
    "\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b124a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping ham to 1, and spam to 0\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "\n",
    "    df = df.sample(\n",
    "        frac=1, random_state=123\n",
    "    ).reset_index(drop=True)                                    # Shuffles entire dataframe\n",
    "\n",
    "    train_end = int(len(df) * train_frac)                       # Calculates split indices\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)  # Test size is implied to be 0.2 as the remainder is 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27afe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the seperated CSVs for later use. Training, Validation and Test sets.\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9478cdf",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            \n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * \n",
    "            (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd55d65",
   "metadata": {},
   "source": [
    "#### Instantiating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length) # Prints longest sub-sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    " )\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f2563",
   "metadata": {},
   "source": [
    "#### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0     \n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f733cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n",
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "# To test if the data loaders are working\n",
    "\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "# Dataset Size\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371345cb",
   "metadata": {},
   "source": [
    "### Preparing For Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ee7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,         \n",
    "    \"context_length\": 1024,      \n",
    "    \"drop_rate\": 0.0,            \n",
    "    \"qkv_bias\": True             \n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    " }\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61024c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n",
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, models_dir=\"gpt2\"\n",
    " )\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249436c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b690c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAs you'll see, the model struggles with following the instructions as we only pre-trained it \\nwithout instruction fine-tuning the model\\n\""
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our model can generate coherent text, let's see if it is capable of classifying spam already or not.\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "'''\n",
    "As you'll see, the model struggles with following the instructions as we only pre-trained it \n",
    "without instruction fine-tuning the model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e01f8",
   "metadata": {},
   "source": [
    "### Mapping the input layer into 2 final output layers (spam or not spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea8950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we freeze the model, meaning we make all the layers nontrainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b804406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis new model.out_head layer will have it's 'requires_grad' attribute set to True by default.\\nTherefor, it is the only layer that'll be updated during training.\\n\""
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"], \n",
    "    out_features=num_classes\n",
    ")\n",
    "'''\n",
    "This new model.out_head layer will have it's 'requires_grad' attribute set to True by default.\n",
    "Therefor, it is the only layer that'll be updated during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ed867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the final Normalization layer and transformer block trainable.\n",
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3dae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Test using the model\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e0c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e4bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "When determining a classification problem, we don't need to analyze every token, in fact, we just need to analyze\n",
    "the very last token since it contains information from all the past tokens. \n",
    "Therefore we look into the last row representing the output token.\n",
    "'''\n",
    "\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319f0fc",
   "metadata": {},
   "source": [
    "### Implement Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c982c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n",
      "Class label: 1\n",
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "# Considering the last token output and determine the class.\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())\n",
    "\n",
    "# Softmax isn't necessary since the higher value simply correlates to its specified meaning and therefore we can further simply our code into:\n",
    "\n",
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30296dde",
   "metadata": {},
   "source": [
    "#### Calculating Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c89ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_batch)[:, -1, :]    \n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += ((predicted_labels == target_batch).sum().item()\n",
    "                                    )\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, model, device, num_batches=10\n",
    ")\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :]   # Logits of the last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:                                       \n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f9e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():                \n",
    "   train_loss = calc_loss_loader(\n",
    "   train_loader, model, device, num_batches=5\n",
    ")\n",
    "val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbad34f",
   "metadata": {},
   "source": [
    "#### Fine-tuning the model to classify spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ecb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs, eval_freq, eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []  \n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):   \n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()                     \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()                         \n",
    "            optimizer.step()                         \n",
    "            examples_seen += input_batch.shape[0]   \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "            \n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss:.3f}, \"\n",
    "                    f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "    \n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72985244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e06fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.635, Val loss 0.610\n",
      "Ep 1 (Step 000050): Train loss 0.447, Val loss 0.503\n",
      "Ep 1 (Step 000100): Train loss 0.346, Val loss 0.430\n",
      "Training accuracy: 85.00% | Validation accuracy: 90.00%\n",
      "Ep 2 (Step 000150): Train loss 0.643, Val loss 0.421\n",
      "Ep 2 (Step 000200): Train loss 0.370, Val loss 0.338\n",
      "Ep 2 (Step 000250): Train loss 0.468, Val loss 0.305\n",
      "Training accuracy: 82.50% | Validation accuracy: 82.50%\n",
      "Ep 3 (Step 000300): Train loss 0.271, Val loss 0.259\n",
      "Ep 3 (Step 000350): Train loss 0.260, Val loss 0.127\n",
      "Training accuracy: 95.00% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.050, Val loss 0.116\n",
      "Ep 4 (Step 000450): Train loss 0.093, Val loss 0.098\n",
      "Ep 4 (Step 000500): Train loss 0.192, Val loss 0.107\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.182, Val loss 0.123\n",
      "Ep 5 (Step 000600): Train loss 0.046, Val loss 0.059\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 9.72 minutes.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = \\\n",
    "    train_classifier_simple(\n",
    "        model, train_loader, val_loader, optimizer, device,\n",
    "        num_epochs=num_epochs, eval_freq=50,\n",
    "        eval_iter=5\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546a3e1",
   "metadata": {},
   "source": [
    "#### Plotting the Classification Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086725e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_values(\n",
    " epochs_seen, examples_seen, train_values, val_values,\n",
    " label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    \n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)   \n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    fig.tight_layout()            \n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec587b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Classification Accuracies\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(\n",
    "    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
    "    label=\"accuracy\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdceff",
   "metadata": {},
   "source": [
    "#### Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da695b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[652]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_accuracy = \u001b[43mcalc_accuracy_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m val_accuracy = calc_accuracy_loader(val_loader, model, device)\n\u001b[32m      3\u001b[39m test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[640]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcalc_accuracy_loader\u001b[39m\u001b[34m(data_loader, model, device, num_batches)\u001b[39m\n\u001b[32m     14\u001b[39m     predicted_labels = torch.argmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m     num_examples += predicted_labels.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     correct_predictions += (\u001b[43m(\u001b[49m\u001b[43mpredicted_labels\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m                             )\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579035f",
   "metadata": {},
   "source": [
    "### Use the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    input_ids = input_ids[:min(             \n",
    "        max_length, supported_context_length\n",
    "    )]\n",
    "\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))   \n",
    "    input_tensor = torch.tensor(\n",
    "        input_ids, device=device\n",
    "    ).unsqueeze(0)             \n",
    "    \n",
    "    with torch.no_grad():                               \n",
    "        logits = model(input_tensor)[:, -1, :]    \n",
    "    \n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd8433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    " )\n",
    " \n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463c3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hi\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9d74c",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_state_dict = torch.load(\"review_classifier.pth\", map_location=device)\n",
    "# model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fced3a3",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_message(message):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    model = GPTModel(BASE_CONFIG)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return classify_review(message, model, tokenizer, device, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d39c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cudssa\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    return classify_review(text, model, tokenizer, device, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7874\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs= gr.Textbox(lines=5, placeholder=\"Type or paste text here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Spam Classifier\",\n",
    "    description=\"Enter a message and the model will predict whether it's spam or not.\"\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b9ac0",
   "metadata": {},
   "source": [
    "# **STAGE THREE PART 2 - PERSONAL ASSISTANT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b3746",
   "metadata": {},
   "source": [
    "## Downloading the dataset needed\n",
    "Contains 1,100 instruction-response pairs, in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc24284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:                                               \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    " \n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    " \n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab8a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# Let's print one entry to see how each entry is structured\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd505ebb",
   "metadata": {},
   "source": [
    "### Prompt Formatting\n",
    "A crucial step in defining the structure of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589109ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes a dictionary entry as input and constructs a formatted string.\n",
    "'''\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2461d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "# Test on the data[50] entry.\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43325b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# Testing for empty input fields\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)                                   # Will completely skip over the input portion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0afca",
   "metadata": {},
   "source": [
    "### **Dividing the Dataset**- *Training, Testing, and Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)                           # 85% of the data is used for training\n",
    "test_portion = int(len(data) * 0.1)                             # 10% of the data is used for testing\n",
    "val_portion = len(data) - train_portion - test_portion          # 5% of the data is used for validating\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4230395",
   "metadata": {},
   "source": [
    "### Batching the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd4303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:        \n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e071036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02ee43",
   "metadata": {},
   "source": [
    "#### Creating a custom collate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc76a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates batches from a list of inputs\n",
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)  \n",
    "    inputs_lst = []\n",
    "    for item in batch:    \n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])   \n",
    "        inputs_lst.append(inputs)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)    \n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e080d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nThe 50256th token represents the '<|endoftext|>' token and will be used as a padding until the length of the longest element.\\n\""
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))\n",
    "\n",
    "'''\n",
    "The 50256th token represents the '<|endoftext|>' token and will be used as a padding until the length of the longest element.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7971a1",
   "metadata": {},
   "source": [
    "#### Create target token IDs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e802d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "We will implement the custom collate function again but with the addition of the ability\n",
    "to generate the target IDs from the input token IDs.\n",
    "'''\n",
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])    \n",
    "        targets = torch.tensor(padded[1:])   \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print()\n",
    "print(targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532224d2",
   "metadata": {},
   "source": [
    "#### Replace the padding tokens with placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will replace all but the first instance of the end-of-text (padding) token with -100.\n",
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (                              \n",
    "            new_item + [pad_token_id] *         \n",
    "            (batch_max_length - len(new_item))  \n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])     \n",
    "        targets = torch.tensor(padded[1:])    \n",
    "        \n",
    "        mask = targets == pad_token_id             \n",
    "        indices = torch.nonzero(mask).squeeze()    \n",
    "        if indices.numel() > 1:                    \n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]      \n",
    "            targets = targets[:allowed_max_length]    \n",
    "            \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Testing the Collate Function\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print()\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fe078",
   "metadata": {},
   "source": [
    "#### Calculating cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],    \n",
    "     [-0.5, 1.5]]     \n",
    ")\n",
    "\n",
    "targets_1 = torch.tensor([0, 1]) # Correct token indices to generate\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)\n",
    "\n",
    "# Adding another token ID and it's effect on the loss calculation:\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]     \n",
    ")\n",
    "\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e3a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3: tensor(True)\n"
     ]
    }
   ],
   "source": [
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e0031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
